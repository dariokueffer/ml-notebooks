{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 3e-4\n",
    "D_MODEL = 256  # Embedding dimension\n",
    "N_HEADS = 8    # Number of attention heads\n",
    "N_LAYERS = 2   # Number of transformer layers\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=MAX_LENGTH,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, D_MODEL)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=D_MODEL,\n",
    "            nhead=N_HEADS,\n",
    "            dim_feedforward=D_MODEL * 4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=N_LAYERS)\n",
    "        self.classifier = nn.Linear(D_MODEL, 2)  # Binary classification\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.embedding(input_ids)\n",
    "        \n",
    "        # Create padding mask for transformer\n",
    "        mask = attention_mask.bool()\n",
    "        \n",
    "        # Pass through transformer\n",
    "        x = self.transformer(x, src_key_padding_mask=~mask)\n",
    "        \n",
    "        # Pool the output (use [CLS] token or mean pooling)\n",
    "        x = torch.mean(x * attention_mask.unsqueeze(-1), dim=1)\n",
    "        \n",
    "        # Classify\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0, bias=True, batch_first=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Basic checks\n",
    "        if embed_dim <= 0 or num_heads <= 0:\n",
    "            raise ValueError(f\"embed_dim ({embed_dim}) and num_heads ({num_heads}) must be greater than 0\")\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\")\n",
    "            \n",
    "        # Save parameters\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.batch_first = batch_first\n",
    "        self.scaling = float(self.head_dim) ** -0.5\n",
    "        \n",
    "        # Projection layers\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.xavier_uniform_(self.q_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.k_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.v_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
    "        if bias:\n",
    "            nn.init.constant_(self.q_proj.bias, 0.)\n",
    "            nn.init.constant_(self.k_proj.bias, 0.)\n",
    "            nn.init.constant_(self.v_proj.bias, 0.)\n",
    "            nn.init.constant_(self.out_proj.bias, 0.)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None, attn_mask=None, \n",
    "               need_weights=True, average_attn_weights=True, is_causal=False):\n",
    "        # Check if inputs are batched and handle batch_first\n",
    "        is_batched = query.dim() == 3\n",
    "        if self.batch_first and is_batched:\n",
    "            query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\n",
    "        \n",
    "        # Get sizes\n",
    "        tgt_len, bsz, embed_dim = query.shape\n",
    "        src_len = key.shape[0]\n",
    "        \n",
    "        # 1. Project inputs\n",
    "        q = self.q_proj(query)\n",
    "        k = self.k_proj(key)\n",
    "        v = self.v_proj(value)\n",
    "        \n",
    "        # 2. Reshape for multi-head attention\n",
    "        q = q.contiguous().view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        k = k.contiguous().view(src_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        v = v.contiguous().view(src_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        \n",
    "        # 3. Calculate attention scores\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scaling\n",
    "        \n",
    "        # 4. Apply masks if provided\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dim() == 2:\n",
    "                attn_mask = attn_mask.unsqueeze(0)\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_weights.masked_fill_(attn_mask, float('-inf'))\n",
    "            else:\n",
    "                attn_weights += attn_mask\n",
    "            \n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = key_padding_mask.to(torch.bool)  # Convert to boolean\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights.masked_fill(\n",
    "                key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "                float('-inf'),\n",
    "            )\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        if is_causal:\n",
    "            causal_mask = torch.triu(\n",
    "                torch.ones(tgt_len, src_len, dtype=torch.bool, device=query.device), \n",
    "                diagonal=1\n",
    "            )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights.masked_fill(\n",
    "                causal_mask.unsqueeze(0).unsqueeze(0),\n",
    "                float('-inf')\n",
    "            )\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "        \n",
    "        # 5. Apply softmax and dropout\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # 6. Apply attention to values\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # 7. Reshape and apply output projection\n",
    "        attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        \n",
    "        # Handle batch_first for output\n",
    "        if self.batch_first and is_batched:\n",
    "            attn_output = attn_output.transpose(1, 0)\n",
    "        \n",
    "        if need_weights:\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            if average_attn_weights:\n",
    "                attn_weights = attn_weights.mean(dim=1)\n",
    "            return attn_output, attn_weights\n",
    "        else:\n",
    "            return attn_output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, batch_first=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        # self.self_attn = nn.MultiheadAttention(\n",
    "        #     embed_dim=d_model,\n",
    "        #     num_heads=nhead,\n",
    "        #     dropout=dropout,\n",
    "        #     batch_first=batch_first\n",
    "        # )\n",
    "\n",
    "        self.self_attn = CustomMultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=nhead,\n",
    "            dropout=dropout,\n",
    "            batch_first=batch_first\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, src, src_mask=None, is_causal=None, src_key_padding_mask=None):  # Added is_causal parameter\n",
    "        # Self-attention block\n",
    "        attn_output, _ = self.self_attn(src, src, src,\n",
    "                                      attn_mask=src_mask,\n",
    "                                      key_padding_mask=src_key_padding_mask,\n",
    "                                      is_causal=is_causal)  # Added is_causal parameter\n",
    "        \n",
    "        src = src + self.dropout(attn_output)  # Residual connection\n",
    "        src = self.norm1(src)  # Layer normalization\n",
    "        \n",
    "        # Feedforward block\n",
    "        ff_output = self.feedforward(src)\n",
    "        src = src + self.dropout(ff_output)  # Residual connection\n",
    "        src = self.norm2(src)  # Layer normalization\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSpamTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, D_MODEL)\n",
    "        encoder_layer = CustomTransformerEncoderLayer(\n",
    "            d_model=D_MODEL,\n",
    "            nhead=N_HEADS,\n",
    "            dim_feedforward=D_MODEL * 4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=N_LAYERS)\n",
    "        self.classifier = nn.Linear(D_MODEL, 2)  # Binary classification\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.embedding(input_ids)\n",
    "        \n",
    "        # Create padding mask for transformer\n",
    "        mask = attention_mask.bool()\n",
    "        \n",
    "        # Pass through transformer\n",
    "        x = self.transformer(x, src_key_padding_mask=~mask)\n",
    "        \n",
    "        # Pool the output (use [CLS] token or mean pooling)\n",
    "        x = torch.mean(x * attention_mask.unsqueeze(-1), dim=1)\n",
    "        \n",
    "        # Classify\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianSpamTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Create GaussianBlock\n",
    "        norm_axes = [1] * n_layers  # 1 for sequence dimension\n",
    "        num_heads = [n_heads] * n_layers\n",
    "        num_gaussians = [8] * n_layers  # You can experiment with this\n",
    "        \n",
    "        self.gaussian_block = GaussianBlock(\n",
    "            norm_axes=norm_axes,\n",
    "            num_heads=num_heads,\n",
    "            num_gaussians=num_gaussians,\n",
    "            num_layers=n_layers,\n",
    "            padding_value=0  # Your padding token value\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.gaussian_block(x)  # Just use the block directly\n",
    "        x = torch.mean(x * attention_mask.unsqueeze(-1), dim=1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, patience=5):\n",
    "    model.train()\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "    wait = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['label'].to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(DEVICE)\n",
    "                attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "                labels = batch['label'].to(DEVICE)\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_accuracy = 100 * correct / total\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        print(f'Epoch {epoch+1}:')\n",
    "        print(f'Training Loss: {total_loss/len(train_loader):.4f}')\n",
    "        print(f'Training Accuracy: {100 * correct/total:.2f}%')\n",
    "        print(f'Validation Accuracy: {100 * val_correct/val_total:.2f}%')\n",
    "        print('-' * 50)\n",
    "\n",
    "        # Early Stopping Logic\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model_state = model.state_dict()  # Save the best model\n",
    "            wait = 0  # Reset patience counter\n",
    "        else:\n",
    "            wait += 1  # Increment patience counter\n",
    "        \n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}. Best Validation Accuracy: {best_val_accuracy:.2f}%\")\n",
    "            break\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_enron_data(spam_path, ham_path):\n",
    "    \"\"\"Load and preprocess the Enron spam/ham dataset.\"\"\"\n",
    "    spam_emails = []\n",
    "    ham_emails = []\n",
    "    \n",
    "    # Load spam emails\n",
    "    for filename in os.listdir(spam_path):\n",
    "        with open(os.path.join(spam_path, filename), 'r', encoding='latin1') as f:\n",
    "            try:\n",
    "                content = f.read()\n",
    "                spam_emails.append(content)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "    \n",
    "    # Load ham emails\n",
    "    for filename in os.listdir(ham_path):\n",
    "        with open(os.path.join(ham_path, filename), 'r', encoding='latin1') as f:\n",
    "            try:\n",
    "                content = f.read()\n",
    "                ham_emails.append(content)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "    \n",
    "    # Create labels (1 for spam, 0 for ham)\n",
    "    emails = spam_emails + ham_emails\n",
    "    labels = [1] * len(spam_emails) + [0] * len(ham_emails)\n",
    "    \n",
    "    # Clean the text\n",
    "    cleaned_emails = []\n",
    "    for email in emails:\n",
    "        # Remove email headers\n",
    "        try:\n",
    "            content = email.split('\\n\\n', 1)[1]\n",
    "        except IndexError:\n",
    "            content = email\n",
    "            \n",
    "        # Remove special characters and extra whitespace\n",
    "        content = re.sub(r'[^\\w\\s]', ' ', content)\n",
    "        content = ' '.join(content.split())\n",
    "        cleaned_emails.append(content)\n",
    "    \n",
    "    return cleaned_emails, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 7):\n",
    "    # !wget --no-check-certificate -P data https://www2.aueb.gr/users/ion/data/enron-spam/preprocessed/enron{i}.tar.gz\n",
    "    !tar -xzf data/enron{i}.tar.gz -C data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data_from_directories(max_items=None):\n",
    "    spam_emails = []\n",
    "    ham_emails = []\n",
    "\n",
    "    for i in range(1, 2):\n",
    "        spam_path = f'data/enron{i}/spam'\n",
    "        ham_path = f'data/enron{i}/ham'\n",
    "        \n",
    "        # Load spam and ham emails\n",
    "        spam, ham = load_enron_data(spam_path, ham_path)\n",
    "        spam_emails.extend(spam)\n",
    "        ham_emails.extend(ham)\n",
    "\n",
    "    # Combine corresponding spam and ham emails into pairs\n",
    "    email_pairs = list(zip(spam_emails, ham_emails))\n",
    "\n",
    "    # Shuffle email pairs\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(email_pairs)\n",
    "\n",
    "    if max_items is not None:\n",
    "        email_pairs = email_pairs[:max_items]\n",
    "\n",
    "    # Separate spam and ham emails after shuffling\n",
    "    spam_emails, ham_emails = zip(*email_pairs)\n",
    "\n",
    "    return list(spam_emails), list(ham_emails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Enron dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Enron dataset...\")\n",
    "texts, labels = load_data_from_directories()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    texts, labels, test_size=0.3, random_state=42  # 70% train, 30% for val+test\n",
    ")\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, test_size=0.5, random_state=42  # Split temp 50-50\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_dataset = SpamDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = SpamDataset(val_texts, val_labels, tokenizer)\n",
    "test_dataset = SpamDataset(test_texts, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GaussianAdaptiveAttention(nn.Module):\n",
    "    def __init__(self, norm_axis, num_heads, num_gaussians, padding_value, mean_offset_init=0, eps=1e-8):\n",
    "        super().__init__()\n",
    "        if not isinstance(norm_axis, int):\n",
    "            raise ValueError(\"norm_axis must be an integer.\")\n",
    "        if num_heads <= 0 or not isinstance(num_heads, int):\n",
    "            raise ValueError(\"num_heads must be a positive integer.\")\n",
    "        if num_gaussians <= 0 or not isinstance(num_gaussians, int):\n",
    "            raise ValueError(\"num_gaussians must be a positive integer.\")\n",
    "\n",
    "        self.norm_axis = norm_axis\n",
    "        self.eps = eps\n",
    "        self.num_heads = num_heads\n",
    "        self.padding_value = padding_value\n",
    "        self.num_gaussians = num_gaussians\n",
    "\n",
    "        self.mean_offsets = nn.Parameter(torch.zeros(num_gaussians, dtype=torch.float))\n",
    "        self.c = nn.Parameter(torch.exp(torch.randn(num_gaussians, dtype=torch.float)))\n",
    "\n",
    "    def forward(self, x, return_attention_details=False):\n",
    "        if x.dim() < 2:\n",
    "            raise ValueError(f\"Input tensor must have at least 2 dimensions, got {x.dim()}.\")\n",
    "        if self.norm_axis >= x.dim() or self.norm_axis < -x.dim():\n",
    "            raise ValueError(f\"norm_axis {self.norm_axis} is out of bounds for input tensor with {x.dim()} dimensions.\")\n",
    "\n",
    "        mask = x != self.padding_value if self.padding_value is not None else None\n",
    "        x_masked = torch.where(mask, x, torch.zeros_like(x)) if mask is not None else x\n",
    "\n",
    "        mean = x_masked.mean(dim=self.norm_axis, keepdim=True)\n",
    "        var = x_masked.var(dim=self.norm_axis, keepdim=True) + self.eps\n",
    "\n",
    "        mixture = 1\n",
    "        for i in range(self.num_gaussians):\n",
    "            adjusted_mean = mean + self.mean_offsets[i]\n",
    "            y_norm = (x - adjusted_mean) / torch.sqrt(var)\n",
    "            gaussian = torch.exp(-((y_norm ** 2) / (2.0 * (self.c[i] ** 2)))) / torch.sqrt(2 * torch.pi * (self.c[i] ** 2))\n",
    "            mixture *= gaussian\n",
    "\n",
    "        mixture /= mixture.sum(dim=self.norm_axis, keepdim=True).clamp(min=self.eps)\n",
    "\n",
    "        if return_attention_details:\n",
    "            return torch.where(mask, x * mixture, x) if mask is not None else x * mixture, mixture.detach()\n",
    "        else:\n",
    "            return torch.where(mask, x * mixture, x) if mask is not None else x * mixture\n",
    "            \n",
    "            \n",
    "class MultiHeadGaussianAdaptiveAttention(nn.Module):\n",
    "    def __init__(self, norm_axis, num_heads, num_gaussians, padding_value=None, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.norm_axis = norm_axis\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_heads = nn.ModuleList([\n",
    "            GaussianAdaptiveAttention(norm_axis, num_heads, num_gaussians, padding_value, eps)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, return_attention_details=False):\n",
    "        chunk_size = x.shape[self.norm_axis] // self.num_heads\n",
    "        if chunk_size == 0:\n",
    "            raise ValueError(f\"Input tensor size along norm_axis ({self.norm_axis}) must be larger than the number of heads ({self.num_heads}).\")\n",
    "\n",
    "        outputs, attention_details_ = [], []\n",
    "        for i in range(self.num_heads):\n",
    "            start_index = i * chunk_size\n",
    "            end_index = start_index + chunk_size if i < self.num_heads - 1 else x.shape[self.norm_axis]\n",
    "            chunk = x.narrow(self.norm_axis, start_index, end_index - start_index)\n",
    "            if return_attention_details:\n",
    "                out, mixture = self.attention_heads[i](chunk, return_attention_details=True)\n",
    "                outputs.append(out)\n",
    "                attention_details_.append(mixture)\n",
    "            else:\n",
    "                outputs.append(self.attention_heads[i](chunk))\n",
    "\n",
    "        if return_attention_details:\n",
    "            return torch.cat(outputs, dim=self.norm_axis), torch.cat(attention_details_, dim=self.norm_axis)\n",
    "        else:\n",
    "            return torch.cat(outputs, dim=self.norm_axis)\n",
    "            \n",
    "            \n",
    "\n",
    "class GaussianBlock(nn.Module):\n",
    "    def __init__(self, norm_axes, num_heads, num_gaussians, num_layers, padding_value=None, eps=1e-8):\n",
    "        super().__init__()\n",
    "        if len(norm_axes) != num_layers or len(num_heads) != num_layers or len(num_gaussians) != num_layers:\n",
    "            raise ValueError(\"Lengths of norm_axes, num_heads, and num_gaussians must match num_layers.\")\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            MultiHeadGaussianAdaptiveAttention(norm_axes[i], num_heads[i], num_gaussians[i], padding_value, eps)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, return_attention_details=False):\n",
    "        attention_details_ = {}\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            if return_attention_details:\n",
    "                x_, attention_details = layer(x, return_attention_details=True)\n",
    "                attention_details_['layer_'+str(idx)] = attention_details\n",
    "                x = x_ + x\n",
    "            else:\n",
    "                x = layer(x) + x\n",
    "\n",
    "        if return_attention_details:\n",
    "            return x, attention_details_\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Initialize model\n",
    "model = SpamTransformer(tokenizer.vocab_size).to(DEVICE)\n",
    "# model = CustomSpamTransformer(tokenizer.vocab_size).to(DEVICE)\n",
    "\n",
    "# Initialize loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Training Loss: 0.1056\n",
      "Training Accuracy: 96.66%\n",
      "Validation Accuracy: 94.97%\n",
      "--------------------------------------------------\n",
      "Epoch 2:\n",
      "Training Loss: 0.0639\n",
      "Training Accuracy: 98.26%\n",
      "Validation Accuracy: 94.97%\n",
      "--------------------------------------------------\n",
      "Epoch 3:\n",
      "Training Loss: 0.0441\n",
      "Training Accuracy: 98.84%\n",
      "Validation Accuracy: 96.13%\n",
      "--------------------------------------------------\n",
      "Epoch 4:\n",
      "Training Loss: 0.0304\n",
      "Training Accuracy: 99.23%\n",
      "Validation Accuracy: 96.01%\n",
      "--------------------------------------------------\n",
      "Epoch 5:\n",
      "Training Loss: 0.0172\n",
      "Training Accuracy: 99.70%\n",
      "Validation Accuracy: 95.23%\n",
      "--------------------------------------------------\n",
      "Epoch 6:\n",
      "Training Loss: 0.0134\n",
      "Training Accuracy: 99.83%\n",
      "Validation Accuracy: 96.26%\n",
      "--------------------------------------------------\n",
      "Epoch 7:\n",
      "Training Loss: 0.0228\n",
      "Training Accuracy: 99.45%\n",
      "Validation Accuracy: 96.01%\n",
      "--------------------------------------------------\n",
      "Epoch 8:\n",
      "Training Loss: 0.0264\n",
      "Training Accuracy: 99.45%\n",
      "Validation Accuracy: 96.52%\n",
      "--------------------------------------------------\n",
      "Epoch 9:\n",
      "Training Loss: 0.0143\n",
      "Training Accuracy: 99.81%\n",
      "Validation Accuracy: 96.13%\n",
      "--------------------------------------------------\n",
      "Epoch 10:\n",
      "Training Loss: 0.0090\n",
      "Training Accuracy: 99.83%\n",
      "Validation Accuracy: 96.52%\n",
      "--------------------------------------------------\n",
      "Epoch 11:\n",
      "Training Loss: 0.0066\n",
      "Training Accuracy: 99.94%\n",
      "Validation Accuracy: 96.78%\n",
      "--------------------------------------------------\n",
      "Epoch 12:\n",
      "Training Loss: 0.0056\n",
      "Training Accuracy: 99.94%\n",
      "Validation Accuracy: 96.78%\n",
      "--------------------------------------------------\n",
      "Epoch 13:\n",
      "Training Loss: 0.0051\n",
      "Training Accuracy: 99.94%\n",
      "Validation Accuracy: 96.39%\n",
      "--------------------------------------------------\n",
      "Epoch 14:\n",
      "Training Loss: 0.0045\n",
      "Training Accuracy: 99.97%\n",
      "Validation Accuracy: 96.65%\n",
      "--------------------------------------------------\n",
      "Epoch 15:\n",
      "Training Loss: 0.0041\n",
      "Training Accuracy: 99.97%\n",
      "Validation Accuracy: 96.65%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['label'].to(DEVICE)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "    \n",
    "    print(\"\\nTest Set Evaluation:\")\n",
    "    print(f\"Average Loss: {total_loss/len(test_loader):.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation:\n",
      "Average Loss: 0.2596\n",
      "Accuracy: 0.9652\n",
      "Precision: 0.9425\n",
      "Recall: 0.9383\n",
      "F1 Score: 0.9404\n",
      "\n",
      "Confusion Matrix:\n",
      "[[536  13]\n",
      " [ 14 213]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9652061855670103,\n",
       " 'precision': 0.9424778761061947,\n",
       " 'recall': 0.9383259911894273,\n",
       " 'f1': 0.9403973509933775,\n",
       " 'confusion_matrix': array([[536,  13],\n",
       "        [ 14, 213]])}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianSpamTransformer(tokenizer.vocab_size, D_MODEL, N_HEADS, N_LAYERS).to(DEVICE)\n",
    "\n",
    "# Initialize loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Training Loss: 0.6050\n",
      "Training Accuracy: 72.54%\n",
      "Validation Accuracy: 66.62%\n",
      "--------------------------------------------------\n",
      "Epoch 2:\n",
      "Training Loss: 0.4878\n",
      "Training Accuracy: 73.29%\n",
      "Validation Accuracy: 69.07%\n",
      "--------------------------------------------------\n",
      "Epoch 3:\n",
      "Training Loss: 0.4024\n",
      "Training Accuracy: 78.65%\n",
      "Validation Accuracy: 77.06%\n",
      "--------------------------------------------------\n",
      "Epoch 4:\n",
      "Training Loss: 0.3292\n",
      "Training Accuracy: 86.41%\n",
      "Validation Accuracy: 86.98%\n",
      "--------------------------------------------------\n",
      "Epoch 5:\n",
      "Training Loss: 0.2669\n",
      "Training Accuracy: 92.71%\n",
      "Validation Accuracy: 91.11%\n",
      "--------------------------------------------------\n",
      "Epoch 6:\n",
      "Training Loss: 0.2192\n",
      "Training Accuracy: 95.06%\n",
      "Validation Accuracy: 92.91%\n",
      "--------------------------------------------------\n",
      "Epoch 7:\n",
      "Training Loss: 0.1844\n",
      "Training Accuracy: 96.27%\n",
      "Validation Accuracy: 94.07%\n",
      "--------------------------------------------------\n",
      "Epoch 8:\n",
      "Training Loss: 0.1556\n",
      "Training Accuracy: 97.10%\n",
      "Validation Accuracy: 94.46%\n",
      "--------------------------------------------------\n",
      "Epoch 9:\n",
      "Training Loss: 0.1340\n",
      "Training Accuracy: 97.40%\n",
      "Validation Accuracy: 94.85%\n",
      "--------------------------------------------------\n",
      "Epoch 10:\n",
      "Training Loss: 0.1182\n",
      "Training Accuracy: 97.82%\n",
      "Validation Accuracy: 95.10%\n",
      "--------------------------------------------------\n",
      "Epoch 11:\n",
      "Training Loss: 0.1061\n",
      "Training Accuracy: 98.23%\n",
      "Validation Accuracy: 95.23%\n",
      "--------------------------------------------------\n",
      "Epoch 12:\n",
      "Training Loss: 0.0933\n",
      "Training Accuracy: 98.48%\n",
      "Validation Accuracy: 95.36%\n",
      "--------------------------------------------------\n",
      "Epoch 13:\n",
      "Training Loss: 0.0837\n",
      "Training Accuracy: 98.73%\n",
      "Validation Accuracy: 95.62%\n",
      "--------------------------------------------------\n",
      "Epoch 14:\n",
      "Training Loss: 0.0759\n",
      "Training Accuracy: 98.90%\n",
      "Validation Accuracy: 96.13%\n",
      "--------------------------------------------------\n",
      "Epoch 15:\n",
      "Training Loss: 0.0693\n",
      "Training Accuracy: 98.98%\n",
      "Validation Accuracy: 96.26%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation:\n",
      "Average Loss: 0.1094\n",
      "Accuracy: 0.9729\n",
      "Precision: 0.9682\n",
      "Recall: 0.9383\n",
      "F1 Score: 0.9530\n",
      "\n",
      "Confusion Matrix:\n",
      "[[542   7]\n",
      " [ 14 213]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9729381443298969,\n",
       " 'precision': 0.9681818181818181,\n",
       " 'recall': 0.9383259911894273,\n",
       " 'f1': 0.9530201342281879,\n",
       " 'confusion_matrix': array([[542,   7],\n",
       "        [ 14, 213]])}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, test_loader, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
