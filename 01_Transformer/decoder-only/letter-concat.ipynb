{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.6)\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/emilianosandri/usnames?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.47M/6.47M [00:02<00:00, 2.77MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting model files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/dariokuffer/.cache/kagglehub/datasets/emilianosandri/usnames/versions/1\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Destination path './1' already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath to dataset files:\u001b[39m\u001b[38;5;124m\"\u001b[39m, path)\n\u001b[1;32m     10\u001b[0m destination \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Current directory\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset moved to current directory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/AML/lib/python3.8/shutil.py:789\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    787\u001b[0m     real_dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, _basename(src))\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(real_dst):\n\u001b[0;32m--> 789\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Error(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDestination path \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m real_dst)\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m     os\u001b[38;5;241m.\u001b[39mrename(src, real_dst)\n",
      "\u001b[0;31mError\u001b[0m: Destination path './1' already exists"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"emilianosandri/usnames\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "\n",
    "destination = \".\"  # Current directory\n",
    "shutil.move(path, destination)\n",
    "print(\"Dataset moved to current directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "firstnames_path = './1/firstnames_f.json'\n",
    "surnames_path = './1/surnames.json'\n",
    "\n",
    "firstnames_df = pd.read_json(firstnames_path)\n",
    "surnames_df = pd.read_json(surnames_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surname</th>\n",
       "      <th>firstname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Smith</td>\n",
       "      <td>Elleana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Johnson</td>\n",
       "      <td>Analis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Williams</td>\n",
       "      <td>Clarinda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brown</td>\n",
       "      <td>Morena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jones</td>\n",
       "      <td>Myhanh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    surname firstname\n",
       "0     Smith   Elleana\n",
       "1   Johnson    Analis\n",
       "2  Williams  Clarinda\n",
       "3     Brown    Morena\n",
       "4     Jones    Myhanh"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "combined_names_df = pd.DataFrame()\n",
    "combined_names_df['surname'] = surnames_df\n",
    "\n",
    "combined_names_df['firstname'] = np.random.choice(firstnames_df[0].values, len(surnames_df), replace=True)\n",
    "combined_names_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column concat with firstname and surname\n",
    "combined_names_df['name'] = combined_names_df['firstname'] + ' ' + combined_names_df['surname']\n",
    "\n",
    "# add a column target with first letter of firstname and surname\n",
    "combined_names_df['target'] = combined_names_df['firstname'].str[0] + combined_names_df['surname'].str[0]\n",
    "combined_names_df.head()\n",
    "\n",
    "# save the dataset\n",
    "combined_names_df.to_csv('combined_names.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class NameDataset(Dataset):\n",
    "    def __init__(self, names, targets):\n",
    "        self.names = names\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'name': self.names[idx],\n",
    "            'target': self.targets[idx]\n",
    "        }\n",
    "\n",
    "def collate_fn(batch, tokenizer, max_length=30):\n",
    "    names = [item['name'] for item in batch]\n",
    "    targets = [item['target'] for item in batch]\n",
    "\n",
    "    text_inputs = [f\"{name} -> {target}{tokenizer.eos_token}\" for name, target in zip(names, targets)]\n",
    "\n",
    "    encoded = tokenizer(\n",
    "        text_inputs,\n",
    "        padding=True,  # Dynamic padding\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'input_ids': encoded['input_ids'],\n",
    "        'attention_mask': encoded['attention_mask'],\n",
    "        'labels': encoded['input_ids']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomDecoderModel(nn.Module):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__()\n",
    "        config = GPT2Config(\n",
    "            vocab_size=len(tokenizer), \n",
    "            n_embd=128, \n",
    "            n_layer=4, \n",
    "            n_head=4\n",
    "        )\n",
    "        self.model = GPT2LMHeadModel(config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs  # Includes logits needed for generate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M', 'a', 'x', ' ', 'M', 'u', 's', 't', 'e', 'r', ' ', '->']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\")})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['->']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': [' ']})\n",
    "tokenizer.add_special_tokens({'eos_token': '<EOS>'})\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "print(tokenizer.tokenize(\"Max Muster ->\"))  # ['H', 'e', 'l', 'l', 'o', 'W', 'o', 'r', 'l', 'd', '!']\n",
    "\n",
    "\n",
    "\n",
    "file_path = 'combined_names.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "train_df = df[:10000]\n",
    "\n",
    "\n",
    "dataset = NameDataset(\n",
    "    train_df['name'].tolist(),\n",
    "    train_df['target'].tolist()\n",
    ")\n",
    "# Assume dataset is already prepared\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
    "\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
    "\n",
    "# Training loop example\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "class CustomDecoderModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        config = GPT2Config(\n",
    "            vocab_size=len(tokenizer), \n",
    "            n_embd=128, \n",
    "            n_layer=6, \n",
    "            n_head=8\n",
    "        )\n",
    "        self.model = GPT2LMHeadModel(config)  # Includes output head\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits  # Returns predicted token logits directly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "class CustomVaswaniDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, n_layers=2, n_heads=4, dim_feedforward=512, max_seq_len=100, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Learned positional embedding\n",
    "        self.positional_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        decoder_layer = TransformerDecoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Ensures input has shape (batch_size, seq_len, features)\n",
    "        )\n",
    "\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
    "        self.output_head = nn.Linear(d_model, vocab_size)  # Final projection to vocabulary size\n",
    "\n",
    "    def forward(self, input_ids, tgt_mask=None, tgt_key_padding_mask=None):\n",
    "        seq_len = input_ids.size(1)\n",
    "\n",
    "        # Convert token IDs to embeddings\n",
    "        tgt_embeddings = self.embedding(input_ids)\n",
    "\n",
    "        # Add learned positional embeddings\n",
    "        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand_as(input_ids)\n",
    "        pos_embeddings = self.positional_embedding(position_ids)\n",
    "\n",
    "        # Combine token and positional embeddings\n",
    "        tgt_embeddings = tgt_embeddings + pos_embeddings\n",
    "\n",
    "        # Generate causal mask if not provided (prevents future token information leakage)\n",
    "        if tgt_mask is None:\n",
    "            tgt_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(input_ids.device)\n",
    "\n",
    "        # Pass through decoder\n",
    "        decoder_output = self.decoder(\n",
    "            tgt=tgt_embeddings, \n",
    "            memory=tgt_embeddings,  # Self-attention, no separate encoder memory\n",
    "            tgt_mask=tgt_mask, \n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "\n",
    "        logits = self.output_head(decoder_output)\n",
    "        return logits  # Return logits for prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, tokenizer, dataloader, num_epochs=5, learning_rate=1e-4, max_length=30):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            batch = {key: val.to(device) for key, val in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids']\n",
    "\n",
    "            # Generate causal mask for decoder (future tokens are masked)\n",
    "            tgt_mask = torch.triu(torch.ones(input_ids.size(1), input_ids.size(1)), diagonal=1).bool().to(device)\n",
    "\n",
    "            # Convert attention_mask to tgt_key_padding_mask if available\n",
    "            tgt_key_padding_mask = batch['attention_mask'] == 0 if 'attention_mask' in batch else None\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(input_ids=input_ids, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "\n",
    "            # Shift logits and labels for causal language modeling\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = batch['labels'][:, 1:].contiguous()\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomVaswaniDecoder(\n",
       "  (embedding): Embedding(50260, 128)\n",
       "  (positional_embedding): Embedding(100, 128)\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (dropout3): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_head): Linear(in_features=128, out_features=50260, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# model = CustomDecoderModel(tokenizer=tokenizer)\n",
    "# model = CustomDecoderModel()\n",
    "model = CustomVaswaniDecoder(len(tokenizer))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 4.7733\n",
      "Epoch 2/5, Loss: 1.0433\n",
      "Epoch 3/5, Loss: 0.1903\n",
      "Epoch 4/5, Loss: 0.0650\n",
      "Epoch 5/5, Loss: 0.0321\n",
      "Training complete!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomVaswaniDecoder(\n",
       "  (embedding): Embedding(50260, 128)\n",
       "  (positional_embedding): Embedding(100, 128)\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (dropout3): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_head): Linear(in_features=128, out_features=50260, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trained_model = train_model(model, tokenizer, dataloader, num_epochs=100)\n",
    "train_model(model, tokenizer, dataloader, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_initials(model, tokenizer, name, max_length=20):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare input\n",
    "    input_text = f\"{name} -> \"\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    input_ids = inputs['input_ids']\n",
    "    print(\"Input Text:\", tokenizer.decode(input_ids[0], skip_special_tokens=False))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Create the sequence mask\n",
    "        seq_len = input_ids.size(1)\n",
    "        tgt_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(device)\n",
    "        \n",
    "        # Get initial prediction\n",
    "        outputs = model(input_ids=input_ids, tgt_mask=tgt_mask)\n",
    "        next_token_logits = outputs[:, -1, :]\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "        generated = torch.cat([input_ids, next_token], dim=1)\n",
    "        \n",
    "        # Generate remaining tokens\n",
    "        for _ in range(max_length - 1):\n",
    "            seq_len = generated.size(1)\n",
    "            tgt_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(device)\n",
    "            \n",
    "            outputs = model(input_ids=generated, tgt_mask=tgt_mask)\n",
    "            next_token_logits = outputs[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "            \n",
    "            # Stop if we predict EOS\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "                \n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "    \n",
    "    # Decode and print the final output\n",
    "    predicted_text = tokenizer.decode(generated[0], skip_special_tokens=False)\n",
    "    print(\"Generated Output:\", predicted_text)\n",
    "    \n",
    "    # Extract just the initials\n",
    "    initials = predicted_text.split(\"->\")[-1].strip()\n",
    "    initials = initials.split(\"<\")[0].strip()  # Remove EOS token if present\n",
    "    \n",
    "    return initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Dario Mustermann -> \n",
      "Generated Output: Dario Mustermann -> DM\n",
      "Predicted initials for 'Dario Mustermann': DM\n"
     ]
    }
   ],
   "source": [
    "# Sample input\n",
    "name = \"Dario Mustermann\"\n",
    "\n",
    "# Generate initials\n",
    "initials = generate_initials(model, tokenizer, name)\n",
    "print(f\"Predicted initials for '{name}': {initials}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Tnya Mogrovejo -> \n",
      "Generated Output: Tnya Mogrovejo -> TM\n",
      "Predicted initials for 'Tnya Mogrovejo': TM\n",
      "Input Text: Kerilyn Mollin -> \n",
      "Generated Output: Kerilyn Mollin -> KM\n",
      "Predicted initials for 'Kerilyn Mollin': KM\n",
      "Input Text: Elin Mollitor -> \n",
      "Generated Output: Elin Mollitor -> EE\n",
      "Predicted initials for 'Elin Mollitor': EE\n",
      "Input Text: Alice Mooris -> \n",
      "Generated Output: Alice Mooris -> AM\n",
      "Predicted initials for 'Alice Mooris': AM\n",
      "Input Text: Cheron Moraida -> \n",
      "Generated Output: Cheron Moraida -> CM\n",
      "Predicted initials for 'Cheron Moraida': CM\n",
      "Input Text: Shanetria Motheral ->\n",
      "Generated Output: Shanetria Motheral -> SM\n",
      "Predicted initials for 'Shanetria Motheral': SM\n",
      "Input Text: Sue Mottaz -> \n",
      "Generated Output: Sue Mottaz -> SttM\n",
      "Predicted initials for 'Sue Mottaz': SttM\n",
      "Input Text: Velva Moucha -> \n",
      "Generated Output: Velva Moucha -> VM\n",
      "Predicted initials for 'Velva Moucha': VM\n",
      "Input Text: Grissel Muia -> \n",
      "Generated Output: Grissel Muia -> GM\n",
      "Predicted initials for 'Grissel Muia': GM\n",
      "Input Text: Lasondra Mulrine -> \n",
      "Generated Output: Lasondra Mulrine -> LM\n",
      "Predicted initials for 'Lasondra Mulrine': LM\n",
      "Input Text: July Mummery -> \n",
      "Generated Output: July Mummery -> JM\n",
      "Predicted initials for 'July Mummery': JM\n",
      "Input Text: Caitlin Musheyev -> \n",
      "Generated Output: Caitlin Musheyev -> CM\n",
      "Predicted initials for 'Caitlin Musheyev': CM\n",
      "Input Text: Lequisha Mustion -> \n",
      "Generated Output: Lequisha Mustion -> LM\n",
      "Predicted initials for 'Lequisha Mustion': LM\n",
      "Input Text: Dietra Myvett -> \n",
      "Generated Output: Dietra Myvett -> DM\n",
      "Predicted initials for 'Dietra Myvett': DM\n",
      "Input Text: Leon Nagan -> \n",
      "Generated Output: Leon Nagan -> LN\n",
      "Predicted initials for 'Leon Nagan': LN\n",
      "Input Text: Soyla Nakaya -> \n",
      "Generated Output: Soyla Nakaya -> SN\n",
      "Predicted initials for 'Soyla Nakaya': SN\n",
      "Input Text: Leroy Navitsky -> \n",
      "Generated Output: Leroy Navitsky -> LN\n",
      "Predicted initials for 'Leroy Navitsky': LN\n",
      "Input Text: Jacinda Neckar -> \n",
      "Generated Output: Jacinda Neckar -> JN\n",
      "Predicted initials for 'Jacinda Neckar': JN\n",
      "Input Text: Shania Nelams -> \n",
      "Generated Output: Shania Nelams -> SN\n",
      "Predicted initials for 'Shania Nelams': SN\n",
      "Input Text: Lorea Nietert -> \n",
      "Generated Output: Lorea Nietert -> LN\n",
      "Predicted initials for 'Lorea Nietert': LN\n",
      "Input Text: Jarita Nimick -> \n",
      "Generated Output: Jarita Nimick -> JN\n",
      "Predicted initials for 'Jarita Nimick': JN\n",
      "Input Text: Layali Nittoli -> \n",
      "Generated Output: Layali Nittoli -> LN\n",
      "Predicted initials for 'Layali Nittoli': LN\n",
      "Input Text: Alexah Noling -> \n",
      "Generated Output: Alexah Noling -> AN\n",
      "Predicted initials for 'Alexah Noling': AN\n",
      "Input Text: Arien Norrgard -> \n",
      "Generated Output: Arien Norrgard -> AN\n",
      "Predicted initials for 'Arien Norrgard': AN\n",
      "Input Text: Arvilla Nors -> \n",
      "Generated Output: Arvilla Nors -> AN\n",
      "Predicted initials for 'Arvilla Nors': AN\n",
      "Input Text: Natanya Nuebel -> \n",
      "Generated Output: Natanya Nuebel -> NN\n",
      "Predicted initials for 'Natanya Nuebel': NN\n",
      "Input Text: Ranyla Obasi -> \n",
      "Generated Output: Ranyla Obasi -> RO\n",
      "Predicted initials for 'Ranyla Obasi': RO\n",
      "Input Text: Randall Obermier -> \n",
      "Generated Output: Randall Obermier -> RO\n",
      "Predicted initials for 'Randall Obermier': RO\n",
      "Input Text: Topacio Ogi -> \n",
      "Generated Output: Topacio Ogi -> TO\n",
      "Predicted initials for 'Topacio Ogi': TO\n",
      "Input Text: Johnelle Ogundipe -> \n",
      "Generated Output: Johnelle Ogundipe -> JO\n",
      "Predicted initials for 'Johnelle Ogundipe': JO\n",
      "Input Text: Takira Okuhara -> \n",
      "Generated Output: Takira Okuhara -> TO\n",
      "Predicted initials for 'Takira Okuhara': TO\n",
      "Input Text: Shareka Olejar -> \n",
      "Generated Output: Shareka Olejar -> SO\n",
      "Predicted initials for 'Shareka Olejar': SO\n",
      "Input Text: Marilou Olien -> \n",
      "Generated Output: Marilou Olien -> MO\n",
      "Predicted initials for 'Marilou Olien': MO\n",
      "Input Text: Laurita Olier -> \n",
      "Generated Output: Laurita Olier -> LO\n",
      "Predicted initials for 'Laurita Olier': LO\n",
      "Input Text: Sharlet Ondish -> \n",
      "Generated Output: Sharlet Ondish -> SO\n",
      "Predicted initials for 'Sharlet Ondish': SO\n",
      "Input Text: Maricella Ornales -> \n",
      "Generated Output: Maricella Ornales -> MO\n",
      "Predicted initials for 'Maricella Ornales': MO\n",
      "Input Text: Agata Otta -> \n",
      "Generated Output: Agata Otta -> At\n",
      "Predicted initials for 'Agata Otta': At\n",
      "Input Text: Severina Owl -> \n",
      "Generated Output: Severina Owl -> SO\n",
      "Predicted initials for 'Severina Owl': SO\n",
      "Input Text: Leana Pagen -> \n",
      "Generated Output: Leana Pagen -> LP\n",
      "Predicted initials for 'Leana Pagen': LP\n",
      "Input Text: Donia Pagliuso -> \n",
      "Generated Output: Donia Pagliuso -> DP\n",
      "Predicted initials for 'Donia Pagliuso': DP\n",
      "Input Text: Cheryll Paliga -> \n",
      "Generated Output: Cheryll Paliga -> CP\n",
      "Predicted initials for 'Cheryll Paliga': CP\n",
      "Input Text: Cristina Paumier -> \n",
      "Generated Output: Cristina Paumier -> CP\n",
      "Predicted initials for 'Cristina Paumier': CP\n",
      "Input Text: Deana Pehle -> \n",
      "Generated Output: Deana Pehle -> DP\n",
      "Predicted initials for 'Deana Pehle': DP\n",
      "Input Text: Cordelia Perng -> \n",
      "Generated Output: Cordelia Perng -> CP\n",
      "Predicted initials for 'Cordelia Perng': CP\n",
      "Input Text: Glenese Perruzzi -> \n",
      "Generated Output: Glenese Perruzzi -> GP\n",
      "Predicted initials for 'Glenese Perruzzi': GP\n",
      "Input Text: Jen Pertl -> \n",
      "Generated Output: Jen Pertl -> JP\n",
      "Predicted initials for 'Jen Pertl': JP\n",
      "Input Text: Clydie Pervin -> \n",
      "Generated Output: Clydie Pervin -> CP\n",
      "Predicted initials for 'Clydie Pervin': CP\n",
      "Input Text: Shannon Petka -> \n",
      "Generated Output: Shannon Petka -> SP\n",
      "Predicted initials for 'Shannon Petka': SP\n",
      "Input Text: Maydelle Pfantz -> \n",
      "Generated Output: Maydelle Pfantz -> MP\n",
      "Predicted initials for 'Maydelle Pfantz': MP\n",
      "Input Text: Tannie Phills -> \n",
      "Generated Output: Tannie Phills -> TP\n",
      "Predicted initials for 'Tannie Phills': TP\n",
      "Input Text: Genowefa Pilny -> \n",
      "Generated Output: Genowefa Pilny -> GP\n",
      "Predicted initials for 'Genowefa Pilny': GP\n",
      "Input Text: Mitchell Plance -> \n",
      "Generated Output: Mitchell Plance -> MP\n",
      "Predicted initials for 'Mitchell Plance': MP\n",
      "Input Text: Rosalind Plasecki -> \n",
      "Generated Output: Rosalind Plasecki -> RP\n",
      "Predicted initials for 'Rosalind Plasecki': RP\n",
      "Input Text: Dora Plathe -> \n",
      "Generated Output: Dora Plathe -> DP\n",
      "Predicted initials for 'Dora Plathe': DP\n",
      "Input Text: Grayson Plizga -> \n",
      "Generated Output: Grayson Plizga -> GP\n",
      "Predicted initials for 'Grayson Plizga': GP\n",
      "Input Text: Cindy Polchinski -> \n",
      "Generated Output: Cindy Polchinski -> CP\n",
      "Predicted initials for 'Cindy Polchinski': CP\n",
      "Input Text: Chasta Polz -> \n",
      "Generated Output: Chasta Polz -> CP\n",
      "Predicted initials for 'Chasta Polz': CP\n",
      "Input Text: Tikisha Popiolek -> \n",
      "Generated Output: Tikisha Popiolek -> TP\n",
      "Predicted initials for 'Tikisha Popiolek': TP\n",
      "Input Text: Olean Pratl -> \n",
      "Generated Output: Olean Pratl -> OP\n",
      "Predicted initials for 'Olean Pratl': OP\n",
      "Input Text: Shawntavia Preap -> \n",
      "Generated Output: Shawntavia Preap -> SP\n",
      "Predicted initials for 'Shawntavia Preap': SP\n",
      "Input Text: Shoshana Proietto -> \n",
      "Generated Output: Shoshana Proietto -> SP\n",
      "Predicted initials for 'Shoshana Proietto': SP\n",
      "Input Text: Yosselyn Proudman -> \n",
      "Generated Output: Yosselyn Proudman -> YP\n",
      "Predicted initials for 'Yosselyn Proudman': YP\n",
      "Input Text: Jeana Pruszynski -> \n",
      "Generated Output: Jeana Pruszynski -> JP\n",
      "Predicted initials for 'Jeana Pruszynski': JP\n",
      "Input Text: Kennadi Qaqish -> \n",
      "Generated Output: Kennadi Qaqish -> KQ\n",
      "Predicted initials for 'Kennadi Qaqish': KQ\n",
      "Input Text: Aleksandra Quagliato\n",
      "Generated Output: Aleksandra Quagliatoaa Aa\n",
      "Predicted initials for 'Aleksandra Quagliato': Aleksandra Quagliatoaa Aa\n",
      "Input Text: Duaa Raedel -> \n",
      "Generated Output: Duaa Raedel -> DR\n",
      "Predicted initials for 'Duaa Raedel': DR\n",
      "Input Text: Memory Raghunathan ->\n",
      "Generated Output: Memory Raghunathan -> MR\n",
      "Predicted initials for 'Memory Raghunathan': MR\n",
      "Input Text: Kalana Raymaker -> \n",
      "Generated Output: Kalana Raymaker -> KR\n",
      "Predicted initials for 'Kalana Raymaker': KR\n",
      "Input Text: Florie Redhair -> \n",
      "Generated Output: Florie Redhair -> FR\n",
      "Predicted initials for 'Florie Redhair': FR\n",
      "Input Text: Johnetta Reillo -> \n",
      "Generated Output: Johnetta Reillo -> JR\n",
      "Predicted initials for 'Johnetta Reillo': JR\n",
      "Input Text: Evelyn Reimund -> \n",
      "Generated Output: Evelyn Reimund -> ER\n",
      "Predicted initials for 'Evelyn Reimund': ER\n",
      "Input Text: Kadeisha Renkel -> \n",
      "Generated Output: Kadeisha Renkel -> KR\n",
      "Predicted initials for 'Kadeisha Renkel': KR\n",
      "Input Text: Aalaysia Rezaei -> \n",
      "Generated Output: Aalaysia Rezaei -> AR\n",
      "Predicted initials for 'Aalaysia Rezaei': AR\n",
      "Input Text: Martesha Ribblett -> \n",
      "Generated Output: Martesha Ribblett -> MR\n",
      "Predicted initials for 'Martesha Ribblett': MR\n",
      "Input Text: Corrinne Rieckmann ->\n",
      "Generated Output: Corrinne Rieckmann -> CR\n",
      "Predicted initials for 'Corrinne Rieckmann': CR\n",
      "Input Text: Lorian Ritsch -> \n",
      "Generated Output: Lorian Ritsch -> LR\n",
      "Predicted initials for 'Lorian Ritsch': LR\n",
      "Input Text: Cierria Riveiro -> \n",
      "Generated Output: Cierria Riveiro -> CR\n",
      "Predicted initials for 'Cierria Riveiro': CR\n",
      "Input Text: Jatara Roba -> \n",
      "Generated Output: Jatara Roba -> JR\n",
      "Predicted initials for 'Jatara Roba': JR\n",
      "Input Text: Malikah Rokus -> \n",
      "Generated Output: Malikah Rokus -> MR\n",
      "Predicted initials for 'Malikah Rokus': MR\n",
      "Input Text: Desta Rosfeld -> \n",
      "Generated Output: Desta Rosfeld -> DR\n",
      "Predicted initials for 'Desta Rosfeld': DR\n",
      "Input Text: Foster Rospert -> \n",
      "Generated Output: Foster Rospert -> FR\n",
      "Predicted initials for 'Foster Rospert': FR\n",
      "Input Text: Karinna Rotell -> \n",
      "Generated Output: Karinna Rotell -> KR\n",
      "Predicted initials for 'Karinna Rotell': KR\n",
      "Input Text: Deaisha Rozic -> \n",
      "Generated Output: Deaisha Rozic -> DR\n",
      "Predicted initials for 'Deaisha Rozic': DR\n",
      "Input Text: Patricia Rumfola -> \n",
      "Generated Output: Patricia Rumfola -> PR\n",
      "Predicted initials for 'Patricia Rumfola': PR\n",
      "Input Text: Shanekia Ruonavaara \n",
      "Generated Output: Shanekia Ruonavaara S\n",
      "Predicted initials for 'Shanekia Ruonavaara': Shanekia Ruonavaara S\n",
      "Input Text: Clelia Rutkin -> \n",
      "Generated Output: Clelia Rutkin -> CR\n",
      "Predicted initials for 'Clelia Rutkin': CR\n",
      "Input Text: Faustine Sagner -> \n",
      "Generated Output: Faustine Sagner -> FS\n",
      "Predicted initials for 'Faustine Sagner': FS\n",
      "Input Text: Timika Sailing -> \n",
      "Generated Output: Timika Sailing -> TS\n",
      "Predicted initials for 'Timika Sailing': TS\n",
      "Input Text: Allysa Sallam -> \n",
      "Generated Output: Allysa Sallam -> AS\n",
      "Predicted initials for 'Allysa Sallam': AS\n",
      "Input Text: Lillian Salsberg -> \n",
      "Generated Output: Lillian Salsberg -> LS\n",
      "Predicted initials for 'Lillian Salsberg': LS\n",
      "Input Text: Nadeen Saluga -> \n",
      "Generated Output: Nadeen Saluga -> NS\n",
      "Predicted initials for 'Nadeen Saluga': NS\n",
      "Input Text: Maryhelen Sarat -> \n",
      "Generated Output: Maryhelen Sarat -> MS\n",
      "Predicted initials for 'Maryhelen Sarat': MS\n",
      "Input Text: Dixie Sardegna -> \n",
      "Generated Output: Dixie Sardegna -> DS\n",
      "Predicted initials for 'Dixie Sardegna': DS\n",
      "Input Text: Undrea Savea -> \n",
      "Generated Output: Undrea Savea -> DS\n",
      "Predicted initials for 'Undrea Savea': DS\n",
      "Input Text: Tajae Sayasane -> \n",
      "Generated Output: Tajae Sayasane -> TS\n",
      "Predicted initials for 'Tajae Sayasane': TS\n",
      "Input Text: Jaslin Schattschneid\n",
      "Generated Output: Jaslin Schattschneideshssnsnnnntnsssssns\n",
      "Predicted initials for 'Jaslin Schattschneider': Jaslin Schattschneideshssnsnnnntnsssssns\n",
      "Input Text: Jakara Schie -> \n",
      "Generated Output: Jakara Schie -> JS\n",
      "Predicted initials for 'Jakara Schie': JS\n",
      "Input Text: Mikalyn Schoenmann ->\n",
      "Generated Output: Mikalyn Schoenmann -> MS\n",
      "Predicted initials for 'Mikalyn Schoenmann': MS\n",
      "Input Text: Gwendolyn Schreider \n",
      "Generated Output: Gwendolyn Schreider GS\n",
      "Predicted initials for 'Gwendolyn Schreider': Gwendolyn Schreider GS\n",
      "Input Text: Josee Schwerman -> \n",
      "Generated Output: Josee Schwerman -> JS\n",
      "Predicted initials for 'Josee Schwerman': JS\n",
      "\n",
      "Predictions: ['TM', 'KM', 'EE', 'AM', 'CM', 'SM', 'SttM', 'VM', 'GM', 'LM', 'JM', 'CM', 'LM', 'DM', 'LN', 'SN', 'LN', 'JN', 'SN', 'LN', 'JN', 'LN', 'AN', 'AN', 'AN', 'NN', 'RO', 'RO', 'TO', 'JO', 'TO', 'SO', 'MO', 'LO', 'SO', 'MO', 'At', 'SO', 'LP', 'DP', 'CP', 'CP', 'DP', 'CP', 'GP', 'JP', 'CP', 'SP', 'MP', 'TP', 'GP', 'MP', 'RP', 'DP', 'GP', 'CP', 'CP', 'TP', 'OP', 'SP', 'SP', 'YP', 'JP', 'KQ', 'Aleksandra Quagliatoaa Aa', 'DR', 'MR', 'KR', 'FR', 'JR', 'ER', 'KR', 'AR', 'MR', 'CR', 'LR', 'CR', 'JR', 'MR', 'DR', 'FR', 'KR', 'DR', 'PR', 'Shanekia Ruonavaara S', 'CR', 'FS', 'TS', 'AS', 'LS', 'NS', 'MS', 'DS', 'DS', 'TS', 'Jaslin Schattschneideshssnsnnnntnsssssns', 'JS', 'MS', 'Gwendolyn Schreider GS', 'JS']\n",
      "Targets: ['TM', 'KM', 'EM', 'AM', 'CM', 'SM', 'SM', 'VM', 'GM', 'LM', 'JM', 'CM', 'LM', 'DM', 'LN', 'SN', 'LN', 'JN', 'SN', 'LN', 'JN', 'LN', 'AN', 'AN', 'AN', 'NN', 'RO', 'RO', 'TO', 'JO', 'TO', 'SO', 'MO', 'LO', 'SO', 'MO', 'AO', 'SO', 'LP', 'DP', 'CP', 'CP', 'DP', 'CP', 'GP', 'JP', 'CP', 'SP', 'MP', 'TP', 'GP', 'MP', 'RP', 'DP', 'GP', 'CP', 'CP', 'TP', 'OP', 'SP', 'SP', 'YP', 'JP', 'KQ', 'AQ', 'DR', 'MR', 'KR', 'FR', 'JR', 'ER', 'KR', 'AR', 'MR', 'CR', 'LR', 'CR', 'JR', 'MR', 'DR', 'FR', 'KR', 'DR', 'PR', 'SR', 'CR', 'FS', 'TS', 'AS', 'LS', 'NS', 'MS', 'DS', 'US', 'TS', 'JS', 'JS', 'MS', 'GS', 'JS']\n",
      "Accuracy: 92.00%\n"
     ]
    }
   ],
   "source": [
    "# use df for testing\n",
    "\n",
    "df = pd.read_csv('combined_names.csv')\n",
    "start = 100000\n",
    "number_of_items = 100\n",
    "df = df[start:start+number_of_items]\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    name = row['name']\n",
    "    initials = generate_initials(model, tokenizer, name)\n",
    "    target = row['target']\n",
    "    targets.append(target)\n",
    "    predictions.append(initials)\n",
    "    print(f\"Predicted initials for '{name}': {initials}\")\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Targets:\", targets)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = sum(1 for pred, tgt in zip(predictions, targets) if pred == tgt)\n",
    "accuracy = correct_predictions / len(targets) * 100  # Percentage accuracy\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
