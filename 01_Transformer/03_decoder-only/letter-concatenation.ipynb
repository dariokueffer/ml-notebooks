{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Letter concatenation with a decoder-only Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (0.3.11)\n",
      "Collecting transformers (from -r requirements.txt (line 2))\n",
      "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from kagglehub->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from kagglehub->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from kagglehub->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from kagglehub->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 2)) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers->-r requirements.txt (line 2))\n",
      "  Downloading huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 2)) (2.2.4)\n",
      "Collecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 2))\n",
      "  Downloading regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers->-r requirements.txt (line 2))\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers->-r requirements.txt (line 2))\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers->-r requirements.txt (line 2)) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers->-r requirements.txt (line 2)) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from requests->kagglehub->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from requests->kagglehub->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from requests->kagglehub->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from requests->kagglehub->-r requirements.txt (line 1)) (2025.1.31)\n",
      "Downloading transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.1-py3-none-any.whl (481 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.30.1 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.50.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "I used a dataset called 'emilianosandri/usnames' from Kaggle, but it is no longer on Kaggle. However, the data used is stored in the data folder. Below, we load the first and last name JSON files and combine them into a dataframe. Additional columns for name, which is the first and last name concatenated, and the target, which is  the initials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "firstnames_path = './data/firstnames_f.json'\n",
    "surnames_path = './data/surnames.json'\n",
    "\n",
    "firstnames_df = pd.read_json(firstnames_path)\n",
    "surnames_df = pd.read_json(surnames_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surname</th>\n",
       "      <th>firstname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Smith</td>\n",
       "      <td>Lewanna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Johnson</td>\n",
       "      <td>Gia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Williams</td>\n",
       "      <td>Maresa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brown</td>\n",
       "      <td>Lorene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jones</td>\n",
       "      <td>Natika</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    surname firstname\n",
       "0     Smith   Lewanna\n",
       "1   Johnson       Gia\n",
       "2  Williams    Maresa\n",
       "3     Brown    Lorene\n",
       "4     Jones    Natika"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "combined_names_df = pd.DataFrame()\n",
    "combined_names_df['surname'] = surnames_df\n",
    "\n",
    "combined_names_df['firstname'] = np.random.choice(firstnames_df[0].values, len(surnames_df), replace=True)\n",
    "combined_names_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surname</th>\n",
       "      <th>firstname</th>\n",
       "      <th>name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Smith</td>\n",
       "      <td>Lewanna</td>\n",
       "      <td>Lewanna Smith</td>\n",
       "      <td>LS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Johnson</td>\n",
       "      <td>Gia</td>\n",
       "      <td>Gia Johnson</td>\n",
       "      <td>GJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Williams</td>\n",
       "      <td>Maresa</td>\n",
       "      <td>Maresa Williams</td>\n",
       "      <td>MW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brown</td>\n",
       "      <td>Lorene</td>\n",
       "      <td>Lorene Brown</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jones</td>\n",
       "      <td>Natika</td>\n",
       "      <td>Natika Jones</td>\n",
       "      <td>NJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    surname firstname             name target\n",
       "0     Smith   Lewanna    Lewanna Smith     LS\n",
       "1   Johnson       Gia      Gia Johnson     GJ\n",
       "2  Williams    Maresa  Maresa Williams     MW\n",
       "3     Brown    Lorene     Lorene Brown     LB\n",
       "4     Jones    Natika     Natika Jones     NJ"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a column concat with firstname and surname\n",
    "combined_names_df['name'] = combined_names_df['firstname'] + ' ' + combined_names_df['surname']\n",
    "\n",
    "# add a column target with first letter of firstname and surname\n",
    "combined_names_df['target'] = combined_names_df['firstname'].str[0] + combined_names_df['surname'].str[0]\n",
    "combined_names_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset to csv\n",
    "# combined_names_df.to_csv('combined_names.csv', index=False)\n",
    "\n",
    "# load from csv\n",
    "combined_names_df = pd.read_csv('combined_names.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class NameDataset(Dataset):\n",
    "    def __init__(self, names, targets):\n",
    "        self.names = names\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'name': self.names[idx],\n",
    "            'target': self.targets[idx]\n",
    "        }\n",
    "\n",
    "def collate_fn(batch, tokenizer, max_length=30):\n",
    "    names = [item['name'] for item in batch]\n",
    "    targets = [item['target'] for item in batch]\n",
    "\n",
    "    text_inputs = [f\"{name} -> {target}{tokenizer.eos_token}\" for name, target in zip(names, targets)]\n",
    "\n",
    "    encoded = tokenizer(\n",
    "        text_inputs,\n",
    "        padding=True,  # Dynamic padding\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'input_ids': encoded['input_ids'],\n",
    "        'attention_mask': encoded['attention_mask'],\n",
    "        'labels': encoded['input_ids']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M', 'a', 'x', ' ', 'M', 'u', 's', 't', 'e', 'r', ' ', '->']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\")})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['->']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': [' ']})\n",
    "tokenizer.add_special_tokens({'eos_token': '<EOS>'})\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "print(tokenizer.tokenize(\"Max Muster ->\"))  # ['H', 'e', 'l', 'l', 'o', 'W', 'o', 'r', 'l', 'd', '!']\n",
    "\n",
    "\n",
    "\n",
    "file_path = 'combined_names.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "train_df = df[:10000]\n",
    "\n",
    "\n",
    "dataset = NameDataset(\n",
    "    train_df['name'].tolist(),\n",
    "    train_df['target'].tolist()\n",
    ")\n",
    "# Assume dataset is already prepared\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=lambda batch: collate_fn(batch, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomDecoderModel(nn.Module):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__()\n",
    "        config = GPT2Config(\n",
    "            vocab_size=len(tokenizer), \n",
    "            n_embd=128, \n",
    "            n_layer=4, \n",
    "            n_head=4\n",
    "        )\n",
    "        self.model = GPT2LMHeadModel(config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "class CustomDecoderModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        config = GPT2Config(\n",
    "            vocab_size=len(tokenizer), \n",
    "            n_embd=128, \n",
    "            n_layer=6, \n",
    "            n_head=8\n",
    "        )\n",
    "        self.model = GPT2LMHeadModel(config)  # Includes output head\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits  # Returns predicted token logits directly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "class CustomVaswaniDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, n_layers=2, n_heads=4, dim_feedforward=512, max_seq_len=100, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Learned positional embedding\n",
    "        self.positional_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        decoder_layer = TransformerDecoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Ensures input has shape (batch_size, seq_len, features)\n",
    "        )\n",
    "\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
    "        self.output_head = nn.Linear(d_model, vocab_size)  # Final projection to vocabulary size\n",
    "\n",
    "    def forward(self, input_ids, tgt_mask=None, tgt_key_padding_mask=None):\n",
    "        seq_len = input_ids.size(1)\n",
    "\n",
    "        # Convert token IDs to embeddings\n",
    "        tgt_embeddings = self.embedding(input_ids)\n",
    "\n",
    "        # Add learned positional embeddings\n",
    "        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand_as(input_ids)\n",
    "        pos_embeddings = self.positional_embedding(position_ids)\n",
    "\n",
    "        # Combine token and positional embeddings\n",
    "        tgt_embeddings = tgt_embeddings + pos_embeddings\n",
    "\n",
    "        # Generate causal mask if not provided (prevents future token information leakage)\n",
    "        if tgt_mask is None:\n",
    "            tgt_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(input_ids.device)\n",
    "\n",
    "        # Pass through decoder\n",
    "        decoder_output = self.decoder(\n",
    "            tgt=tgt_embeddings, \n",
    "            memory=tgt_embeddings,  # Self-attention, no separate encoder memory\n",
    "            tgt_mask=tgt_mask, \n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "\n",
    "        logits = self.output_head(decoder_output)\n",
    "        return logits  # Return logits for prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method to Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, tokenizer, dataloader, num_epochs=5, learning_rate=1e-4, max_length=30):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            batch = {key: val.to(device) for key, val in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids']\n",
    "\n",
    "            # Generate causal mask for decoder (future tokens are masked)\n",
    "            tgt_mask = torch.triu(torch.ones(input_ids.size(1), input_ids.size(1)), diagonal=1).bool().to(device)\n",
    "\n",
    "            # Convert attention_mask to tgt_key_padding_mask if available\n",
    "            tgt_key_padding_mask = batch['attention_mask'] == 0 if 'attention_mask' in batch else None\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(input_ids=input_ids, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "\n",
    "            # Shift logits and labels for causal language modeling\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = batch['labels'][:, 1:].contiguous()\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomVaswaniDecoder(\n",
       "  (embedding): Embedding(50260, 128)\n",
       "  (positional_embedding): Embedding(100, 128)\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (dropout3): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_head): Linear(in_features=128, out_features=50260, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# model = CustomDecoderModel(tokenizer=tokenizer)\n",
    "# model = CustomDecoderModel()\n",
    "model = CustomVaswaniDecoder(len(tokenizer))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 4.7683\n",
      "Epoch 2/5, Loss: 1.1804\n",
      "Epoch 3/5, Loss: 0.2298\n",
      "Epoch 4/5, Loss: 0.0757\n",
      "Epoch 5/5, Loss: 0.0367\n",
      "Training complete!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomVaswaniDecoder(\n",
       "  (embedding): Embedding(50260, 128)\n",
       "  (positional_embedding): Embedding(100, 128)\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (dropout3): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_head): Linear(in_features=128, out_features=50260, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model, tokenizer, dataloader, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Output Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_initials(model, tokenizer, name, max_length=20):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare input\n",
    "    input_text = f\"{name} -> \"\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    input_ids = inputs['input_ids']\n",
    "    print(\"Input Text:\", tokenizer.decode(input_ids[0], skip_special_tokens=False))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Create the sequence mask\n",
    "        seq_len = input_ids.size(1)\n",
    "        tgt_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(device)\n",
    "        \n",
    "        # Get initial prediction\n",
    "        outputs = model(input_ids=input_ids, tgt_mask=tgt_mask)\n",
    "        next_token_logits = outputs[:, -1, :]\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "        generated = torch.cat([input_ids, next_token], dim=1)\n",
    "        \n",
    "        # Generate remaining tokens\n",
    "        for _ in range(max_length - 1):\n",
    "            seq_len = generated.size(1)\n",
    "            tgt_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(device)\n",
    "            \n",
    "            outputs = model(input_ids=generated, tgt_mask=tgt_mask)\n",
    "            next_token_logits = outputs[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "            \n",
    "            # Stop if we predict EOS\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "                \n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "    \n",
    "    # Decode and print the final output\n",
    "    predicted_text = tokenizer.decode(generated[0], skip_special_tokens=False)\n",
    "    print(\"Generated Output:\", predicted_text)\n",
    "    \n",
    "    # Extract just the initials\n",
    "    initials = predicted_text.split(\"->\")[-1].strip()\n",
    "    initials = initials.split(\"<\")[0].strip()  # Remove EOS token if present\n",
    "    \n",
    "    return initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Dario Mustermann -> \n",
      "Generated Output: Dario Mustermann -> DM\n",
      "Predicted initials for 'Dario Mustermann': DM\n"
     ]
    }
   ],
   "source": [
    "# Sample input\n",
    "name = \"Dario Mustermann\"\n",
    "\n",
    "# Generate initials\n",
    "initials = generate_initials(model, tokenizer, name)\n",
    "print(f\"Predicted initials for '{name}': {initials}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Rishita Mogrovejo -> \n",
      "Generated Output: Rishita Mogrovejo -> RM\n",
      "Predicted initials for 'Rishita Mogrovejo': RM\n",
      "Input Text: Inez Mollin -> \n",
      "Generated Output: Inez Mollin -> IM\n",
      "Predicted initials for 'Inez Mollin': IM\n",
      "Input Text: Devony Mollitor -> \n",
      "Generated Output: Devony Mollitor -> DM\n",
      "Predicted initials for 'Devony Mollitor': DM\n",
      "Input Text: Zaidee Mooris -> \n",
      "Generated Output: Zaidee Mooris -> ZM\n",
      "Predicted initials for 'Zaidee Mooris': ZM\n",
      "Input Text: Alma Moraida -> \n",
      "Generated Output: Alma Moraida -> AM\n",
      "Predicted initials for 'Alma Moraida': AM\n",
      "Input Text: Davyn Motheral -> \n",
      "Generated Output: Davyn Motheral -> DM\n",
      "Predicted initials for 'Davyn Motheral': DM\n",
      "Input Text: Doris Mottaz -> \n",
      "Generated Output: Doris Mottaz -> DM\n",
      "Predicted initials for 'Doris Mottaz': DM\n",
      "Input Text: Teonna Moucha -> \n",
      "Generated Output: Teonna Moucha -> TM\n",
      "Predicted initials for 'Teonna Moucha': TM\n",
      "Input Text: Shifra Muia -> \n",
      "Generated Output: Shifra Muia -> SM\n",
      "Predicted initials for 'Shifra Muia': SM\n",
      "Input Text: Nayeli Mulrine -> \n",
      "Generated Output: Nayeli Mulrine -> NM\n",
      "Predicted initials for 'Nayeli Mulrine': NM\n",
      "Input Text: Natoya Mummery -> \n",
      "Generated Output: Natoya Mummery -> NM\n",
      "Predicted initials for 'Natoya Mummery': NM\n",
      "Input Text: Azie Musheyev -> \n",
      "Generated Output: Azie Musheyev -> AM\n",
      "Predicted initials for 'Azie Musheyev': AM\n",
      "Input Text: Shanna Mustion -> \n",
      "Generated Output: Shanna Mustion -> SM\n",
      "Predicted initials for 'Shanna Mustion': SM\n",
      "Input Text: Areonna Myvett -> \n",
      "Generated Output: Areonna Myvett -> AM\n",
      "Predicted initials for 'Areonna Myvett': AM\n",
      "Input Text: Salina Nagan -> \n",
      "Generated Output: Salina Nagan -> SN\n",
      "Predicted initials for 'Salina Nagan': SN\n",
      "Input Text: Lashundra Nakaya -> \n",
      "Generated Output: Lashundra Nakaya -> LN\n",
      "Predicted initials for 'Lashundra Nakaya': LN\n",
      "Input Text: Shauna Navitsky -> \n",
      "Generated Output: Shauna Navitsky -> SN\n",
      "Predicted initials for 'Shauna Navitsky': SN\n",
      "Input Text: Fay Neckar -> \n",
      "Generated Output: Fay Neckar -> FF\n",
      "Predicted initials for 'Fay Neckar': FF\n",
      "Input Text: Shashana Nelams -> \n",
      "Generated Output: Shashana Nelams -> SN\n",
      "Predicted initials for 'Shashana Nelams': SN\n",
      "Input Text: Mylie Nietert -> \n",
      "Generated Output: Mylie Nietert -> MN\n",
      "Predicted initials for 'Mylie Nietert': MN\n",
      "Input Text: Jeneca Nimick -> \n",
      "Generated Output: Jeneca Nimick -> JN\n",
      "Predicted initials for 'Jeneca Nimick': JN\n",
      "Input Text: Alba Nittoli -> \n",
      "Generated Output: Alba Nittoli -> AN\n",
      "Predicted initials for 'Alba Nittoli': AN\n",
      "Input Text: Frona Noling -> \n",
      "Generated Output: Frona Noling -> FN\n",
      "Predicted initials for 'Frona Noling': FN\n",
      "Input Text: Electra Norrgard -> \n",
      "Generated Output: Electra Norrgard -> EN\n",
      "Predicted initials for 'Electra Norrgard': EN\n",
      "Input Text: Mylin Nors -> \n",
      "Generated Output: Mylin Nors -> MN\n",
      "Predicted initials for 'Mylin Nors': MN\n",
      "Input Text: Vinnie Nuebel -> \n",
      "Generated Output: Vinnie Nuebel -> VN\n",
      "Predicted initials for 'Vinnie Nuebel': VN\n",
      "Input Text: Kallen Obasi -> \n",
      "Generated Output: Kallen Obasi -> KO\n",
      "Predicted initials for 'Kallen Obasi': KO\n",
      "Input Text: Jesse Obermier -> \n",
      "Generated Output: Jesse Obermier -> Jb\n",
      "Predicted initials for 'Jesse Obermier': Jb\n",
      "Input Text: Asuncion Ogi -> \n",
      "Generated Output: Asuncion Ogi -> AO\n",
      "Predicted initials for 'Asuncion Ogi': AO\n",
      "Input Text: Devi Ogundipe -> \n",
      "Generated Output: Devi Ogundipe -> DO\n",
      "Predicted initials for 'Devi Ogundipe': DO\n",
      "Input Text: Willma Okuhara -> \n",
      "Generated Output: Willma Okuhara -> WO\n",
      "Predicted initials for 'Willma Okuhara': WO\n",
      "Input Text: Syliva Olejar -> \n",
      "Generated Output: Syliva Olejar -> SO\n",
      "Predicted initials for 'Syliva Olejar': SO\n",
      "Input Text: Nailea Olien -> \n",
      "Generated Output: Nailea Olien -> NO\n",
      "Predicted initials for 'Nailea Olien': NO\n",
      "Input Text: Madlyn Olier -> \n",
      "Generated Output: Madlyn Olier -> MO\n",
      "Predicted initials for 'Madlyn Olier': MO\n",
      "Input Text: Arkia Ondish -> \n",
      "Generated Output: Arkia Ondish -> AO\n",
      "Predicted initials for 'Arkia Ondish': AO\n",
      "Input Text: Sharmine Ornales -> \n",
      "Generated Output: Sharmine Ornales -> SO\n",
      "Predicted initials for 'Sharmine Ornales': SO\n",
      "Input Text: Mataya Otta -> \n",
      "Generated Output: Mataya Otta -> MO\n",
      "Predicted initials for 'Mataya Otta': MO\n",
      "Input Text: Kalyani Owl -> \n",
      "Generated Output: Kalyani Owl -> KO\n",
      "Predicted initials for 'Kalyani Owl': KO\n",
      "Input Text: Nicolasa Pagen -> \n",
      "Generated Output: Nicolasa Pagen -> NP\n",
      "Predicted initials for 'Nicolasa Pagen': NP\n",
      "Input Text: Wilhemina Pagliuso ->\n",
      "Generated Output: Wilhemina Pagliuso -> WP\n",
      "Predicted initials for 'Wilhemina Pagliuso': WP\n",
      "Input Text: Perris Paliga -> \n",
      "Generated Output: Perris Paliga -> PP\n",
      "Predicted initials for 'Perris Paliga': PP\n",
      "Input Text: Lessie Paumier -> \n",
      "Generated Output: Lessie Paumier -> LP\n",
      "Predicted initials for 'Lessie Paumier': LP\n",
      "Input Text: Fredia Pehle -> \n",
      "Generated Output: Fredia Pehle -> FP\n",
      "Predicted initials for 'Fredia Pehle': FP\n",
      "Input Text: Hava Perng -> \n",
      "Generated Output: Hava Perng -> HP\n",
      "Predicted initials for 'Hava Perng': HP\n",
      "Input Text: Debbra Perruzzi -> \n",
      "Generated Output: Debbra Perruzzi -> DP\n",
      "Predicted initials for 'Debbra Perruzzi': DP\n",
      "Input Text: Lilian Pertl -> \n",
      "Generated Output: Lilian Pertl -> LP\n",
      "Predicted initials for 'Lilian Pertl': LP\n",
      "Input Text: Onelia Pervin -> \n",
      "Generated Output: Onelia Pervin -> OP\n",
      "Predicted initials for 'Onelia Pervin': OP\n",
      "Input Text: Andreya Petka -> \n",
      "Generated Output: Andreya Petka -> AP\n",
      "Predicted initials for 'Andreya Petka': AP\n",
      "Input Text: Meisha Pfantz -> \n",
      "Generated Output: Meisha Pfantz -> MP\n",
      "Predicted initials for 'Meisha Pfantz': MP\n",
      "Input Text: Stefany Phills -> \n",
      "Generated Output: Stefany Phills -> SP\n",
      "Predicted initials for 'Stefany Phills': SP\n",
      "Input Text: Kiyah Pilny -> \n",
      "Generated Output: Kiyah Pilny -> KP\n",
      "Predicted initials for 'Kiyah Pilny': KP\n",
      "Input Text: Bleu Plance -> \n",
      "Generated Output: Bleu Plance -> BP\n",
      "Predicted initials for 'Bleu Plance': BP\n",
      "Input Text: Yamel Plasecki -> \n",
      "Generated Output: Yamel Plasecki -> YP\n",
      "Predicted initials for 'Yamel Plasecki': YP\n",
      "Input Text: Deluvina Plathe -> \n",
      "Generated Output: Deluvina Plathe -> DP\n",
      "Predicted initials for 'Deluvina Plathe': DP\n",
      "Input Text: Rashae Plizga -> \n",
      "Generated Output: Rashae Plizga -> RP\n",
      "Predicted initials for 'Rashae Plizga': RP\n",
      "Input Text: Ambree Polchinski -> \n",
      "Generated Output: Ambree Polchinski -> AP\n",
      "Predicted initials for 'Ambree Polchinski': AP\n",
      "Input Text: Tomica Polz -> \n",
      "Generated Output: Tomica Polz -> TP\n",
      "Predicted initials for 'Tomica Polz': TP\n",
      "Input Text: Armelia Popiolek -> \n",
      "Generated Output: Armelia Popiolek -> AP\n",
      "Predicted initials for 'Armelia Popiolek': AP\n",
      "Input Text: Clarrisa Pratl -> \n",
      "Generated Output: Clarrisa Pratl -> CP\n",
      "Predicted initials for 'Clarrisa Pratl': CP\n",
      "Input Text: Theta Preap -> \n",
      "Generated Output: Theta Preap -> TP\n",
      "Predicted initials for 'Theta Preap': TP\n",
      "Input Text: Lesley Proietto -> \n",
      "Generated Output: Lesley Proietto -> LP\n",
      "Predicted initials for 'Lesley Proietto': LP\n",
      "Input Text: Madine Proudman -> \n",
      "Generated Output: Madine Proudman -> MP\n",
      "Predicted initials for 'Madine Proudman': MP\n",
      "Input Text: Leonida Pruszynski ->\n",
      "Generated Output: Leonida Pruszynski -> LP\n",
      "Predicted initials for 'Leonida Pruszynski': LP\n",
      "Input Text: Lylianah Qaqish -> \n",
      "Generated Output: Lylianah Qaqish -> LL\n",
      "Predicted initials for 'Lylianah Qaqish': LL\n",
      "Input Text: Elizah Quagliato -> \n",
      "Generated Output: Elizah Quagliato -> EE\n",
      "Predicted initials for 'Elizah Quagliato': EE\n",
      "Input Text: Katora Raedel -> \n",
      "Generated Output: Katora Raedel -> KR\n",
      "Predicted initials for 'Katora Raedel': KR\n",
      "Input Text: Lonia Raghunathan -> \n",
      "Generated Output: Lonia Raghunathan -> LR\n",
      "Predicted initials for 'Lonia Raghunathan': LR\n",
      "Input Text: Georgia Raymaker -> \n",
      "Generated Output: Georgia Raymaker -> GR\n",
      "Predicted initials for 'Georgia Raymaker': GR\n",
      "Input Text: Talyn Redhair -> \n",
      "Generated Output: Talyn Redhair -> TR\n",
      "Predicted initials for 'Talyn Redhair': TR\n",
      "Input Text: Jaleeyah Reillo -> \n",
      "Generated Output: Jaleeyah Reillo -> JR\n",
      "Predicted initials for 'Jaleeyah Reillo': JR\n",
      "Input Text: Celenia Reimund -> \n",
      "Generated Output: Celenia Reimund -> CR\n",
      "Predicted initials for 'Celenia Reimund': CR\n",
      "Input Text: Elissa Renkel -> \n",
      "Generated Output: Elissa Renkel -> ER\n",
      "Predicted initials for 'Elissa Renkel': ER\n",
      "Input Text: Frank Rezaei -> \n",
      "Generated Output: Frank Rezaei -> FR\n",
      "Predicted initials for 'Frank Rezaei': FR\n",
      "Input Text: Saya Ribblett -> \n",
      "Generated Output: Saya Ribblett -> SR\n",
      "Predicted initials for 'Saya Ribblett': SR\n",
      "Input Text: Thetis Rieckmann -> \n",
      "Generated Output: Thetis Rieckmann -> TR\n",
      "Predicted initials for 'Thetis Rieckmann': TR\n",
      "Input Text: Kristyn Ritsch -> \n",
      "Generated Output: Kristyn Ritsch -> KR\n",
      "Predicted initials for 'Kristyn Ritsch': KR\n",
      "Input Text: Enza Riveiro -> \n",
      "Generated Output: Enza Riveiro -> ER\n",
      "Predicted initials for 'Enza Riveiro': ER\n",
      "Input Text: Rosalie Roba -> \n",
      "Generated Output: Rosalie Roba -> RR\n",
      "Predicted initials for 'Rosalie Roba': RR\n",
      "Input Text: Mishel Rokus -> \n",
      "Generated Output: Mishel Rokus -> MR\n",
      "Predicted initials for 'Mishel Rokus': MR\n",
      "Input Text: Ronda Rosfeld -> \n",
      "Generated Output: Ronda Rosfeld -> RR\n",
      "Predicted initials for 'Ronda Rosfeld': RR\n",
      "Input Text: Karyzma Rospert -> \n",
      "Generated Output: Karyzma Rospert -> KR\n",
      "Predicted initials for 'Karyzma Rospert': KR\n",
      "Input Text: Isabella Rotell -> \n",
      "Generated Output: Isabella Rotell -> IR\n",
      "Predicted initials for 'Isabella Rotell': IR\n",
      "Input Text: Zamiya Rozic -> \n",
      "Generated Output: Zamiya Rozic -> ZR\n",
      "Predicted initials for 'Zamiya Rozic': ZR\n",
      "Input Text: Kamica Rumfola -> \n",
      "Generated Output: Kamica Rumfola -> KR\n",
      "Predicted initials for 'Kamica Rumfola': KR\n",
      "Input Text: Laketra Ruonavaara ->\n",
      "Generated Output: Laketra Ruonavaara -> LR\n",
      "Predicted initials for 'Laketra Ruonavaara': LR\n",
      "Input Text: Shanelle Rutkin -> \n",
      "Generated Output: Shanelle Rutkin -> SR\n",
      "Predicted initials for 'Shanelle Rutkin': SR\n",
      "Input Text: Takoda Sagner -> \n",
      "Generated Output: Takoda Sagner -> TS\n",
      "Predicted initials for 'Takoda Sagner': TS\n",
      "Input Text: Arnecia Sailing -> \n",
      "Generated Output: Arnecia Sailing -> AS\n",
      "Predicted initials for 'Arnecia Sailing': AS\n",
      "Input Text: Luellen Sallam -> \n",
      "Generated Output: Luellen Sallam -> LS\n",
      "Predicted initials for 'Luellen Sallam': LS\n",
      "Input Text: Amalea Salsberg -> \n",
      "Generated Output: Amalea Salsberg -> AS\n",
      "Predicted initials for 'Amalea Salsberg': AS\n",
      "Input Text: Margaretta Saluga -> \n",
      "Generated Output: Margaretta Saluga -> MM\n",
      "Predicted initials for 'Margaretta Saluga': MM\n",
      "Input Text: Hollace Sarat -> \n",
      "Generated Output: Hollace Sarat -> HS\n",
      "Predicted initials for 'Hollace Sarat': HS\n",
      "Input Text: Canei Sardegna -> \n",
      "Generated Output: Canei Sardegna -> CS\n",
      "Predicted initials for 'Canei Sardegna': CS\n",
      "Input Text: Oma Savea -> \n",
      "Generated Output: Oma Savea -> OO\n",
      "Predicted initials for 'Oma Savea': OO\n",
      "Input Text: Chidera Sayasane -> \n",
      "Generated Output: Chidera Sayasane -> CS\n",
      "Predicted initials for 'Chidera Sayasane': CS\n",
      "Input Text: Darin Schattschneide\n",
      "Generated Output: Darin Schattschneidein\n",
      "Predicted initials for 'Darin Schattschneider': Darin Schattschneidein\n",
      "Input Text: Magdeline Schie -> \n",
      "Generated Output: Magdeline Schie -> MS\n",
      "Predicted initials for 'Magdeline Schie': MS\n",
      "Input Text: Zarria Schoenmann -> \n",
      "Generated Output: Zarria Schoenmann -> ZS\n",
      "Predicted initials for 'Zarria Schoenmann': ZS\n",
      "Input Text: Dewanda Schreider -> \n",
      "Generated Output: Dewanda Schreider -> DS\n",
      "Predicted initials for 'Dewanda Schreider': DS\n",
      "Input Text: Vonetta Schwerman -> \n",
      "Generated Output: Vonetta Schwerman -> VS\n",
      "Predicted initials for 'Vonetta Schwerman': VS\n",
      "\n",
      "Predictions: ['RM', 'IM', 'DM', 'ZM', 'AM', 'DM', 'DM', 'TM', 'SM', 'NM', 'NM', 'AM', 'SM', 'AM', 'SN', 'LN', 'SN', 'FF', 'SN', 'MN', 'JN', 'AN', 'FN', 'EN', 'MN', 'VN', 'KO', 'Jb', 'AO', 'DO', 'WO', 'SO', 'NO', 'MO', 'AO', 'SO', 'MO', 'KO', 'NP', 'WP', 'PP', 'LP', 'FP', 'HP', 'DP', 'LP', 'OP', 'AP', 'MP', 'SP', 'KP', 'BP', 'YP', 'DP', 'RP', 'AP', 'TP', 'AP', 'CP', 'TP', 'LP', 'MP', 'LP', 'LL', 'EE', 'KR', 'LR', 'GR', 'TR', 'JR', 'CR', 'ER', 'FR', 'SR', 'TR', 'KR', 'ER', 'RR', 'MR', 'RR', 'KR', 'IR', 'ZR', 'KR', 'LR', 'SR', 'TS', 'AS', 'LS', 'AS', 'MM', 'HS', 'CS', 'OO', 'CS', 'Darin Schattschneidein', 'MS', 'ZS', 'DS', 'VS']\n",
      "Targets: ['RM', 'IM', 'DM', 'ZM', 'AM', 'DM', 'DM', 'TM', 'SM', 'NM', 'NM', 'AM', 'SM', 'AM', 'SN', 'LN', 'SN', 'FN', 'SN', 'MN', 'JN', 'AN', 'FN', 'EN', 'MN', 'VN', 'KO', 'JO', 'AO', 'DO', 'WO', 'SO', 'NO', 'MO', 'AO', 'SO', 'MO', 'KO', 'NP', 'WP', 'PP', 'LP', 'FP', 'HP', 'DP', 'LP', 'OP', 'AP', 'MP', 'SP', 'KP', 'BP', 'YP', 'DP', 'RP', 'AP', 'TP', 'AP', 'CP', 'TP', 'LP', 'MP', 'LP', 'LQ', 'EQ', 'KR', 'LR', 'GR', 'TR', 'JR', 'CR', 'ER', 'FR', 'SR', 'TR', 'KR', 'ER', 'RR', 'MR', 'RR', 'KR', 'IR', 'ZR', 'KR', 'LR', 'SR', 'TS', 'AS', 'LS', 'AS', 'MS', 'HS', 'CS', 'OS', 'CS', 'DS', 'MS', 'ZS', 'DS', 'VS']\n",
      "Accuracy: 93.00%\n"
     ]
    }
   ],
   "source": [
    "# use df for testing\n",
    "\n",
    "df = pd.read_csv('combined_names.csv')\n",
    "start = 100000\n",
    "number_of_items = 100\n",
    "df = df[start:start+number_of_items]\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    name = row['name']\n",
    "    initials = generate_initials(model, tokenizer, name)\n",
    "    target = row['target']\n",
    "    targets.append(target)\n",
    "    predictions.append(initials)\n",
    "    print(f\"Predicted initials for '{name}': {initials}\")\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Targets:\", targets)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = sum(1 for pred, tgt in zip(predictions, targets) if pred == tgt)\n",
    "accuracy = correct_predictions / len(targets) * 100  # Percentage accuracy\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
