{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Letter concatenation with a decoder-only Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (0.3.11)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (4.50.3)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from kagglehub->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from kagglehub->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from kagglehub->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from kagglehub->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 2)) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 2)) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 2)) (2.2.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 2)) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 2)) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers->-r requirements.txt (line 2)) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers->-r requirements.txt (line 2)) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from requests->kagglehub->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from requests->kagglehub->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from requests->kagglehub->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from requests->kagglehub->-r requirements.txt (line 1)) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "I used a dataset called 'emilianosandri/usnames' from Kaggle, but it is no longer on Kaggle. However, the data used is stored in the data folder. Below, we load the first and last name JSON files and combine them into a dataframe. Additional columns for name, which is the first and last name concatenated, and the target, which is  the initials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surname</th>\n",
       "      <th>firstname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Smith</td>\n",
       "      <td>Shonte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Johnson</td>\n",
       "      <td>Sumiko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Williams</td>\n",
       "      <td>Kaylan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brown</td>\n",
       "      <td>Ara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jones</td>\n",
       "      <td>Danaja</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    surname firstname\n",
       "0     Smith    Shonte\n",
       "1   Johnson    Sumiko\n",
       "2  Williams    Kaylan\n",
       "3     Brown       Ara\n",
       "4     Jones    Danaja"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "firstnames_path = './data/firstnames_f.json'\n",
    "surnames_path = './data/surnames.json'\n",
    "\n",
    "firstnames_df = pd.read_json(firstnames_path)\n",
    "surnames_df = pd.read_json(surnames_path)\n",
    "\n",
    "combined_names_df = pd.DataFrame()\n",
    "combined_names_df['surname'] = surnames_df\n",
    "\n",
    "combined_names_df['firstname'] = np.random.choice(firstnames_df[0].values, len(surnames_df), replace=True)\n",
    "combined_names_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surname</th>\n",
       "      <th>firstname</th>\n",
       "      <th>name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Smith</td>\n",
       "      <td>Shonte</td>\n",
       "      <td>Shonte Smith</td>\n",
       "      <td>SS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Johnson</td>\n",
       "      <td>Sumiko</td>\n",
       "      <td>Sumiko Johnson</td>\n",
       "      <td>SJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Williams</td>\n",
       "      <td>Kaylan</td>\n",
       "      <td>Kaylan Williams</td>\n",
       "      <td>KW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brown</td>\n",
       "      <td>Ara</td>\n",
       "      <td>Ara Brown</td>\n",
       "      <td>AB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jones</td>\n",
       "      <td>Danaja</td>\n",
       "      <td>Danaja Jones</td>\n",
       "      <td>DJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    surname firstname             name target\n",
       "0     Smith    Shonte     Shonte Smith     SS\n",
       "1   Johnson    Sumiko   Sumiko Johnson     SJ\n",
       "2  Williams    Kaylan  Kaylan Williams     KW\n",
       "3     Brown       Ara        Ara Brown     AB\n",
       "4     Jones    Danaja     Danaja Jones     DJ"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a column concat with firstname and surname\n",
    "combined_names_df['name'] = combined_names_df['firstname'] + ' ' + combined_names_df['surname']\n",
    "\n",
    "# add a column target with first letter of firstname and surname\n",
    "combined_names_df['target'] = combined_names_df['firstname'].str[0] + combined_names_df['surname'].str[0]\n",
    "combined_names_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset to csv\n",
    "# combined_names_df.to_csv('combined_names.csv', index=False)\n",
    "\n",
    "# load from csv\n",
    "combined_names_df = pd.read_csv('combined_names.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class DatasetWithNames(Dataset):\n",
    "    def __init__(self, names, targets):\n",
    "        self.names = names\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'name': self.names[idx],\n",
    "            'target': self.targets[idx]\n",
    "        }\n",
    "\n",
    "def collate_fn(batch, tokenizer, max_length=30):\n",
    "    names = [item['name'] for item in batch]\n",
    "    targets = [item['target'] for item in batch]\n",
    "\n",
    "    text_inputs = [f\"{name} -> {target}{tokenizer.eos_token}\" for name, target in zip(names, targets)]\n",
    "\n",
    "    encoded = tokenizer.tokenize(\n",
    "        text_inputs,\n",
    "        padding=True,  # Dynamic padding\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'input_ids': encoded['input_ids'],\n",
    "        'attention_mask': encoded['attention_mask'],\n",
    "        'labels': encoded['input_ids'],\n",
    "        'names': names,\n",
    "        'targets': targets\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, tokens=None):\n",
    "        alphabet = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ \")\n",
    "        self.additional_tokens = [\"->\"]\n",
    "        self.special_tokens = [\"<EOS>\", \"[PAD]\", \"[UNK]\"]\n",
    "        self.tokens = tokens if tokens else self.special_tokens + self.additional_tokens + alphabet\n",
    "        self.vocab_size = len(self.tokens)\n",
    "\n",
    "        self.token_to_id = {token: idx for idx, token in enumerate(self.tokens)}\n",
    "        self.id_to_token = {idx: token for token, idx in self.token_to_id.items()}\n",
    "\n",
    "        self.set_special_tokens()\n",
    "\n",
    "    def set_special_tokens(self):\n",
    "        self.eos_token = \"<EOS>\"\n",
    "        self.pad_token = \"[PAD]\"\n",
    "        self.unk_token = \"[UNK]\"\n",
    "        self.pad_token_id = self.token_to_id[self.pad_token]\n",
    "        self.eos_token_id = self.token_to_id[self.eos_token]\n",
    "        self.unk_token_id = self.token_to_id[self.unk_token]\n",
    "\n",
    "    def check_if_special_token(self, text, index):\n",
    "        if text[index:index+5] in self.token_to_id:\n",
    "            return self.token_to_id[text[index:index+5]], 5\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            if text[i:i+2] == \"->\":\n",
    "                tokens.append(self.token_to_id[\"->\"])\n",
    "                i += 2\n",
    "            elif self.check_if_special_token(text, i):\n",
    "                token_id, length = self.check_if_special_token(text, i)\n",
    "                tokens.append(token_id)\n",
    "                i += length\n",
    "            elif text[i] in self.token_to_id:\n",
    "                tokens.append(self.token_to_id[text[i]])\n",
    "                i += 1\n",
    "            else:\n",
    "                tokens.append(self.token_to_id[\"[UNK]\"])\n",
    "                i += 1\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return \"\".join([self.id_to_token.get(idx, \"[UNK]\") for idx in ids])\n",
    "\n",
    "\n",
    "    def tokenize(self, text_inputs, padding=True, truncation=True, max_length=30):\n",
    "        tokenized_texts = [self.encode(text) for text in text_inputs]\n",
    "        max_length = max(len(tokens) for tokens in tokenized_texts)\n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "        for text in text_inputs:\n",
    "            tokens = self.encode(text)\n",
    "            if truncation and len(tokens) > max_length:\n",
    "                tokens = tokens[:max_length]\n",
    "            input_ids.append(tokens)\n",
    "            attention_mask.append([1] * len(tokens))\n",
    "            if padding:\n",
    "                while len(tokens) < max_length:\n",
    "                    tokens.append(self.token_to_id[self.pad_token])\n",
    "                    attention_mask[-1].append(0)\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids),\n",
    "            'attention_mask': torch.tensor(attention_mask),\n",
    "            'labels': torch.tensor(input_ids),\n",
    "            'names': text_inputs,\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Dario Kuffer -> DK<EOS>\n",
      "Encoded: [33, 4, 21, 12, 18, 56, 40, 24, 9, 9, 8, 21, 56, 3, 56, 33, 40, 0]\n",
      "Decoded: Dario Kuffer -> DK<EOS>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Example usage\n",
    "text = \"Dario Kuffer -> DK<EOS>\"\n",
    "encoded = tokenizer.encode(text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "file_path = 'combined_names.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "train_df = df[:100000]\n",
    "\n",
    "\n",
    "dataset = DatasetWithNames(\n",
    "    train_df['name'].tolist(),\n",
    "    train_df['target'].tolist()\n",
    ")\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda batch: collate_fn(batch, tokenizer), drop_last=True)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=lambda batch: collate_fn(batch, tokenizer), drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([33,  8, 17, 12, 22, 56, 32, 12, 21,  8, 15, 15,  4, 56,  3, 56, 33, 32,\n",
      "         0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "Labels: tensor([33,  8, 17, 12, 22, 56, 32, 12, 21,  8, 15, 15,  4, 56,  3, 56, 33, 32,\n",
      "         0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n",
      "Decoded Text: Denis Cirella -> DC<EOS>[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "Original Name: Denis Cirella\n",
      "Original Target: DC\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input IDs: tensor([31, 21,  4, 17,  7,  8, 17, 56, 45, 21, 12, 17,  6,  8, 56,  3, 56, 31,\n",
      "        45,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "Labels: tensor([31, 21,  4, 17,  7,  8, 17, 56, 45, 21, 12, 17,  6,  8, 56,  3, 56, 31,\n",
      "        45,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n",
      "Decoded Text: Branden Prince -> BP<EOS>[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "Original Name: Branden Prince\n",
      "Original Target: BP\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input IDs: tensor([36, 15,  8, 17,  7,  8, 17,  8, 56, 32,  4, 23,  4, 17, 29,  4, 21, 18,\n",
      "        56,  3, 56, 36, 32,  0,  1,  1,  1,  1,  1])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0])\n",
      "Labels: tensor([36, 15,  8, 17,  7,  8, 17,  8, 56, 32,  4, 23,  4, 17, 29,  4, 21, 18,\n",
      "        56,  3, 56, 36, 32,  0,  1,  1,  1,  1,  1])\n",
      "Decoded Text: Glendene Catanzaro -> GC<EOS>[PAD][PAD][PAD][PAD][PAD]\n",
      "Original Name: Glendene Catanzaro\n",
      "Original Target: GC\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input IDs: tensor([40,  4, 28,  6,  8,  8, 56, 51,  8,  7, 21,  4, 15, 56,  3, 56, 40, 51,\n",
      "         0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "Labels: tensor([40,  4, 28,  6,  8,  8, 56, 51,  8,  7, 21,  4, 15, 56,  3, 56, 40, 51,\n",
      "         0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n",
      "Decoded Text: Kaycee Vedral -> KV<EOS>[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "Original Name: Kaycee Vedral\n",
      "Original Target: KV\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "for batch in dataloader:\n",
    "    for index in range(len(batch['input_ids'])):\n",
    "        input_ids = batch['input_ids'][index]\n",
    "        attention_mask = batch['attention_mask'][index]\n",
    "        labels = batch['labels'][index]\n",
    "        print(\"Input IDs:\", input_ids)\n",
    "        print(\"Attention Mask:\", attention_mask)\n",
    "        print(\"Labels:\", labels)\n",
    "        decoded_text = tokenizer.decode(input_ids.tolist())\n",
    "        print(\"Decoded Text:\", decoded_text)\n",
    "        print(\"Original Name:\", batch['names'][index])\n",
    "        print(\"Original Target:\", batch['targets'][index])\n",
    "        print(100*'-')\n",
    "        if index == 3:\n",
    "            break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, n_layers=2, n_heads=4, dim_feedforward=512, max_seq_len=100, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Learned positional embedding\n",
    "        self.positional_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Ensures input has shape (batch_size, seq_len, features)\n",
    "        )\n",
    "\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
    "        self.output_head = nn.Linear(d_model, vocab_size)  # Final projection to vocabulary size\n",
    "\n",
    "\n",
    "    def generate_square_subsequent_mask(self, seq_len, device):\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=device, dtype=torch.bool), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, tgt_mask=None, tgt_key_padding_mask=None):\n",
    "        seq_len = input_ids.size(1)\n",
    "\n",
    "        # Convert token IDs to embeddings\n",
    "        tgt_embeddings = self.embedding(input_ids)\n",
    "\n",
    "        # Add learned positional embeddings\n",
    "        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand_as(input_ids)\n",
    "        pos_embeddings = self.positional_embedding(position_ids)\n",
    "\n",
    "        # Combine token and positional embeddings\n",
    "        tgt_embeddings = tgt_embeddings + pos_embeddings\n",
    "        tgt_embeddings = self.dropout(tgt_embeddings)\n",
    "\n",
    "        # Generate causal mask if not provided (prevents future token information leakage)\n",
    "        if tgt_mask is None:\n",
    "            tgt_mask = self.generate_square_subsequent_mask(seq_len, input_ids.device)\n",
    "\n",
    "        # Pass through decoder\n",
    "        decoder_output = self.decoder(\n",
    "            tgt=tgt_embeddings, \n",
    "            memory=tgt_embeddings,  # Self-attention, no separate encoder memory\n",
    "            tgt_mask=tgt_mask, \n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "\n",
    "        logits = self.output_head(decoder_output)\n",
    "        return logits  # Return logits for prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method to Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "def train_model(model, tokenizer, dataloader, num_epochs=5, learning_rate=1e-4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            batch = {key: val.to(device) for key, val in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids']\n",
    "            seq_len = input_ids.size(1)\n",
    "\n",
    "            tgt_mask = model.generate_square_subsequent_mask(seq_len, input_ids.device)\n",
    "            tgt_key_padding_mask = batch['attention_mask'] == 0 if 'attention_mask' in batch else None\n",
    "\n",
    "            logits = model(input_ids=input_ids, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = batch['labels'][:, 1:].contiguous()\n",
    "\n",
    "            loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.4861\n",
      "Epoch 2/5, Loss: 0.0654\n",
      "Epoch 3/5, Loss: 0.0182\n",
      "Epoch 4/5, Loss: 0.0091\n",
      "Epoch 5/5, Loss: 0.0056\n"
     ]
    }
   ],
   "source": [
    "model = DecoderOnlyTransformer(tokenizer.vocab_size)\n",
    "train_model(model, tokenizer, dataloader, num_epochs=5, learning_rate=2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Output Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(model, tokenizer, name, max_length=10, max_output=None, verbose=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare input\n",
    "    input_text = [f\"{name} -> \"]\n",
    "    inputs = tokenizer.tokenize(\n",
    "        input_text,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=max_length)\n",
    "    \n",
    "    generated = inputs['input_ids']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _ in range(max_length - 1):\n",
    "            seq_len = generated.size(1)\n",
    "            tgt_mask = model.generate_square_subsequent_mask(seq_len, device)\n",
    "            outputs = model(input_ids=generated, tgt_mask=tgt_mask)\n",
    "            next_token_logits = outputs[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "            if next_token[0].item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "                \n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "    \n",
    "\n",
    "    predicted_text = tokenizer.decode(generated[0].tolist())\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Predicted text:\", predicted_text)\n",
    "        \n",
    "    # Extract just the initials\n",
    "    initials = predicted_text.split(\"->\")[-1].strip()\n",
    "    if max_output is not None:\n",
    "        initials = initials[:max_output]\n",
    "    \n",
    "    return initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, tokenizer, name, beam_width=3, max_length=10):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    inputs = tokenizer.tokenize(\n",
    "        [f\"{name} -> \"],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "\n",
    "    beams = [(input_ids, 0)]  # (sequence, score)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            new_beams = []\n",
    "\n",
    "            for beam_input_ids, beam_score in beams:\n",
    "                seq_len = beam_input_ids.size(1)\n",
    "                tgt_mask = model.generate_square_subsequent_mask(seq_len, device)\n",
    "\n",
    "                outputs = model(input_ids=beam_input_ids, tgt_mask=tgt_mask)\n",
    "                next_token_logits = outputs[:, -1, :]\n",
    "                log_probs = torch.log_softmax(next_token_logits, dim=-1)\n",
    "\n",
    "                topk_log_probs, topk_indices = torch.topk(log_probs, beam_width, dim=-1)\n",
    "\n",
    "                for log_prob, token_id in zip(topk_log_probs[0], topk_indices[0]):\n",
    "                    token_id = token_id.view(1, 1)\n",
    "                    token = tokenizer.decode(token_id[0].tolist())\n",
    "                    next_input_ids = torch.cat([beam_input_ids, token_id], dim=1)\n",
    "                    total_score = beam_score + log_prob.item()\n",
    "\n",
    "        \n",
    "                    new_beams.append((next_input_ids, total_score))\n",
    "\n",
    "            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "            completed = [beam for beam in beams if beam[0][0, -1].item() == tokenizer.eos_token_id]\n",
    "            if completed:\n",
    "                beams = completed\n",
    "                break\n",
    "\n",
    "    # print beams\n",
    "    beams = sorted(beams, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    best_beam = beams[0][0].squeeze(0).tolist()\n",
    "    # remove EOS token if present\n",
    "    best_beam = [token for token in best_beam if token != tokenizer.eos_token_id]\n",
    "    predicted_text = tokenizer.decode(best_beam)\n",
    "\n",
    "    initials = predicted_text.split(\"->\")[-1].strip()\n",
    "    return initials\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted initials for 'Salina Nagan': SN\n"
     ]
    }
   ],
   "source": [
    "name = \"Salina Nagan\"\n",
    "\n",
    "initials = beam_search(model, tokenizer, name)\n",
    "print(f\"Predicted initials for '{name}': {initials}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted initials for 'Salina Nagan': SN\n"
     ]
    }
   ],
   "source": [
    "name = \"Salina Nagan\"\n",
    "\n",
    "initials = greedy(model, tokenizer, name)\n",
    "print(f\"Predicted initials for '{name}': {initials}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataloader, fn=greedy, max_items=100):\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    counter = 0\n",
    "    for batch in dataloader:\n",
    "        for index in range(len(batch['input_ids'])):\n",
    "            if counter >= max_items:\n",
    "                break\n",
    "            counter += 1\n",
    "            name = batch['names'][index]\n",
    "            initials = fn(model, tokenizer, name)\n",
    "            predictions.append(initials)\n",
    "            targets.append(batch['targets'][index])\n",
    "\n",
    "    correct_predictions = sum(1 for pred, tgt in zip(predictions, targets) if pred == tgt)\n",
    "    accuracy = correct_predictions / len(targets) * 100  # Percentage accuracy\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.00%\n"
     ]
    }
   ],
   "source": [
    "# Greedy search evaluation\n",
    "evaluate_model(model, tokenizer, validation_dataloader, fn=greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.00%\n"
     ]
    }
   ],
   "source": [
    "# Beam search evaluation\n",
    "evaluate_model(model, tokenizer, validation_dataloader, fn=beam_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.00%\n"
     ]
    }
   ],
   "source": [
    "# Greedy search evaluation with trimmed output\n",
    "# This will limit the output to 2 characters\n",
    "greedy_trimmed_to_two_chars = lambda model, tokenizer, name: greedy(model, tokenizer, name, max_length=10, max_output=2)\n",
    "evaluate_model(model, tokenizer, validation_dataloader, fn=greedy_trimmed_to_two_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
