{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data\n",
    "\n",
    "In the follow method creates a dataset with pairs of numerical numbers and its textual representation. It returns a split of train and test validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from num2words import num2words\n",
    "\n",
    "def create_dataset(start=0, end=1000, train_size=0.8):\n",
    "   numbers = list(range(start, end))\n",
    "   data = [(str(n), num2words(n)) for n in numbers]\n",
    "   df = pd.DataFrame(data, columns=['num', 'word'])\n",
    "   \n",
    "   train_df = df.sample(frac=train_size, random_state=42)\n",
    "   test_df = df.drop(train_df.index)\n",
    "   \n",
    "   return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in val_df:  200\n",
      "Items in train_df:  800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>521</td>\n",
       "      <td>five hundred and twenty-one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>737</td>\n",
       "      <td>seven hundred and thirty-seven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>740</td>\n",
       "      <td>seven hundred and forty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>660</td>\n",
       "      <td>six hundred and sixty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>411</td>\n",
       "      <td>four hundred and eleven</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     num                            word\n",
       "521  521     five hundred and twenty-one\n",
       "737  737  seven hundred and thirty-seven\n",
       "740  740         seven hundred and forty\n",
       "660  660           six hundred and sixty\n",
       "411  411         four hundred and eleven"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = create_dataset(0, 1000, 0.8)\n",
    "print('Items in val_df: ', len(val_df))\n",
    "print('Items in train_df: ', len(train_df))\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "The inputs are tokenized by just using the digits and having the following special tokens:\n",
    "\n",
    "- BOS_TOKEN_ID = 10\n",
    "- EOS_TOKEN_ID = 11\n",
    "- PAD_TOKEN_ID = 12\n",
    "\n",
    "For the outputs we train a sentencepiece tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_BOS_TOKEN_ID = 10\n",
    "INPUT_EOS_TOKEN_ID = 11\n",
    "INPUT_PAD_TOKEN_ID = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: tokenizer/num_to_words\n",
      "  model_type: BPE\n",
      "  vocab_size: 200\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: <PAD>\n",
      "  user_defined_symbols: <BOS>\n",
      "  user_defined_symbols: <EOS>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 800 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <PAD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <BOS>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <EOS>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=20504\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=21\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 800 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 800\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 102\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1442 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=238 size=20 all=108 active=87 piece=ei\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=90 size=40 all=130 active=109 piece=▁fif\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=60 all=118 active=97 piece=eight\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=80 all=114 active=93 piece=▁thirteen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=100 all=98 active=77 piece=le\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=0 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=120 all=78 active=57 piece=and\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=140 all=58 active=37 piece=▁ei\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=160 all=38 active=17 piece=ninet\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: tokenizer/num_to_words.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: tokenizer/num_to_words.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import io\n",
    "\n",
    "text_data = \"\\n\".join(train_df['word'])\n",
    "text_stream = io.StringIO(text_data)\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    sentence_iterator=text_stream,\n",
    "    model_prefix='tokenizer/num_to_words',\n",
    "    vocab_size=200,\n",
    "    model_type='bpe',\n",
    "    character_coverage=1.0,\n",
    "    user_defined_symbols=['<PAD>', '<BOS>', '<EOS>']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['▁one', '▁hundred', '▁and', '▁twenty', '-', 'five']\n",
      "Decoded: one hundred and twenty-five\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('tokenizer/num_to_words.model')\n",
    "\n",
    "text = \"one hundred and twenty-five\"\n",
    "ids = sp.encode(text, out_type=int)\n",
    "tokens = sp.encode(text, out_type=str)\n",
    "decoded = sp.decode(ids)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT_BOS_TOKEN_ID: 4\n",
      "OUTPUT_EOS_TOKEN_ID: 5\n",
      "OUTPUT_PAD_TOKEN_ID: 3\n"
     ]
    }
   ],
   "source": [
    "# get ids of special tokens\n",
    "OUTPUT_BOS_TOKEN_ID = sp.piece_to_id('<BOS>')\n",
    "OUTPUT_EOS_TOKEN_ID = sp.piece_to_id('<EOS>')\n",
    "OUTPUT_PAD_TOKEN_ID = sp.piece_to_id('<PAD>')\n",
    "print('OUTPUT_BOS_TOKEN_ID:', OUTPUT_BOS_TOKEN_ID)\n",
    "print('OUTPUT_EOS_TOKEN_ID:', OUTPUT_EOS_TOKEN_ID)\n",
    "print('OUTPUT_PAD_TOKEN_ID:', OUTPUT_PAD_TOKEN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "def tokenize_textual(df_column):\n",
    "    sp = spm.SentencePieceProcessor(model_file='tokenizer/num_to_words.model')\n",
    "\n",
    "    tokenized_words = df_column.apply(lambda x: [OUTPUT_BOS_TOKEN_ID] + sp.encode(x, out_type=int) + [OUTPUT_EOS_TOKEN_ID])\n",
    "    max_length = tokenized_words.apply(len).max()\n",
    "\n",
    "    padded_tokens = tokenized_words.apply(lambda x: x + [OUTPUT_PAD_TOKEN_ID] * (max_length - len(x)))\n",
    "\n",
    "    tensor_data = torch.tensor(padded_tokens.tolist(), dtype=torch.long)\n",
    "    return tensor_data\n",
    "\n",
    "\n",
    "def tokenize_numeric(df_column):\n",
    "    tokenized_sequences = [\n",
    "        [INPUT_BOS_TOKEN_ID] + [int(d) for d in str(num)] + [INPUT_EOS_TOKEN_ID] for num in df_column\n",
    "    ]\n",
    "\n",
    "    # Ensure consistent padding length across input and target\n",
    "    max_length = max(len(seq) for seq in tokenized_sequences)\n",
    "    \n",
    "    padded_sequences = [\n",
    "        seq + [INPUT_PAD_TOKEN_ID] * (max_length - len(seq)) for seq in tokenized_sequences\n",
    "    ]\n",
    "\n",
    "    tensor_sequences = torch.tensor(padded_sequences, dtype=torch.long)\n",
    "    return tensor_sequences\n",
    "\n",
    "def decode_numeric(sequence):\n",
    "   seq_list = sequence.tolist()\n",
    "   tokens = [\n",
    "       '<BOS>' if x == INPUT_BOS_TOKEN_ID else\n",
    "       '<EOS>' if x == INPUT_EOS_TOKEN_ID else\n",
    "       '<PAD>' if x == INPUT_PAD_TOKEN_ID else\n",
    "       str(x) \n",
    "       for x in seq_list\n",
    "   ]\n",
    "   return \"\".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NumberWordDataset(Dataset):\n",
    "    def __init__(self, input_tensor, target_tensor, pad_token_id=3):\n",
    "        self.input_tensor = input_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.input_tensor[idx]\n",
    "        target_seq = self.target_tensor[idx]\n",
    "\n",
    "        # Create attention masks (1 = actual token, 0 = padding)\n",
    "        input_mask = (input_seq == INPUT_PAD_TOKEN_ID).long()\n",
    "        target_mask = (target_seq == self.pad_token_id).long()\n",
    "        \n",
    "        return {\n",
    "            'input_seq': input_seq,\n",
    "            'target_seq': target_seq,\n",
    "            'input_mask': input_mask.bool(),\n",
    "            'target_mask': target_mask.bool() \n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_textual = tokenize_textual(train_df['word'])\n",
    "tokenized_numeric = tokenize_numeric(train_df['num'])\n",
    "# Create the dataset from tokenized data\n",
    "dataset = NumberWordDataset(tokenized_numeric, tokenized_textual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item: 1\n",
      "tensor([10,  9,  4,  9, 11])\n",
      "Decoded Input: <BOS>949<EOS>\n",
      "Input: tensor([10,  9,  4,  9, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> nine hundred and forty-nine<EOS>\n",
      "Target: tensor([  4,  29,  12,  14,  55, 190,  22,   5])\n",
      "Target Mask: tensor([False, False, False, False, False, False, False, False])\n",
      "\n",
      "Item: 2\n",
      "tensor([10,  1,  3,  6, 11])\n",
      "Decoded Input: <BOS>136<EOS>\n",
      "Input: tensor([10,  1,  3,  6, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> one hundred and thirty-six<EOS>\n",
      "Target: tensor([  4,  47,  12,  14,  52, 190,  66,   5])\n",
      "Target Mask: tensor([False, False, False, False, False, False, False, False])\n",
      "\n",
      "Item: 3\n",
      "tensor([10,  3,  7,  7, 11])\n",
      "Decoded Input: <BOS>377<EOS>\n",
      "Input: tensor([10,  3,  7,  7, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> three hundred and seventy-seven<EOS>\n",
      "Target: tensor([  4,  44,  12,  14,  57, 190,  67,   5])\n",
      "Target Mask: tensor([False, False, False, False, False, False, False, False])\n",
      "\n",
      "Item: 4\n",
      "tensor([10,  7,  8,  5, 11])\n",
      "Decoded Input: <BOS>785<EOS>\n",
      "Input: tensor([10,  7,  8,  5, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> seven hundred and eighty-five<EOS>\n",
      "Target: tensor([  4,  36,  12,  14,  54, 190,  64,   5])\n",
      "Target Mask: tensor([False, False, False, False, False, False, False, False])\n",
      "\n",
      "Item: 5\n",
      "tensor([10,  5,  3,  3, 11])\n",
      "Decoded Input: <BOS>533<EOS>\n",
      "Input: tensor([10,  5,  3,  3, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> five hundred and thirty-three<EOS>\n",
      "Target: tensor([  4,  50,  12,  14,  52, 190,  60,   5])\n",
      "Target Mask: tensor([False, False, False, False, False, False, False, False])\n",
      "\n",
      "Item: 6\n",
      "tensor([10,  6,  3, 11, 12])\n",
      "Decoded Input: <BOS>63<EOS><PAD>\n",
      "Input: tensor([10,  6,  3, 11, 12])\n",
      "Input Mask: tensor([False, False, False, False,  True])\n",
      "Decoded Target: <BOS> sixty-three<EOS><PAD><PAD><PAD>\n",
      "Target: tensor([  4,  58, 190,  60,   5,   3,   3,   3])\n",
      "Target Mask: tensor([False, False, False, False, False,  True,  True,  True])\n",
      "\n",
      "Item: 7\n",
      "tensor([10,  7,  3,  7, 11])\n",
      "Decoded Input: <BOS>737<EOS>\n",
      "Input: tensor([10,  7,  3,  7, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> seven hundred and thirty-seven<EOS>\n",
      "Target: tensor([  4,  36,  12,  14,  52, 190,  67,   5])\n",
      "Target Mask: tensor([False, False, False, False, False, False, False, False])\n",
      "\n",
      "Item: 8\n",
      "tensor([10,  2,  2, 11, 12])\n",
      "Decoded Input: <BOS>22<EOS><PAD>\n",
      "Input: tensor([10,  2,  2, 11, 12])\n",
      "Input Mask: tensor([False, False, False, False,  True])\n",
      "Decoded Target: <BOS> twenty-two<EOS><PAD><PAD><PAD>\n",
      "Target: tensor([  4,  51, 190,  69,   5,   3,   3,   3])\n",
      "Target Mask: tensor([False, False, False, False, False,  True,  True,  True])\n",
      "\n",
      "Item: 9\n",
      "tensor([10,  9,  4,  6, 11])\n",
      "Decoded Input: <BOS>946<EOS>\n",
      "Input: tensor([10,  9,  4,  6, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> nine hundred and forty-six<EOS>\n",
      "Target: tensor([  4,  29,  12,  14,  55, 190,  66,   5])\n",
      "Target Mask: tensor([False, False, False, False, False, False, False, False])\n",
      "\n",
      "Item: 10\n",
      "tensor([10,  4,  5, 11, 12])\n",
      "Decoded Input: <BOS>45<EOS><PAD>\n",
      "Input: tensor([10,  4,  5, 11, 12])\n",
      "Input Mask: tensor([False, False, False, False,  True])\n",
      "Decoded Target: <BOS> forty-five<EOS><PAD><PAD><PAD>\n",
      "Target: tensor([  4,  55, 190,  64,   5,   3,   3,   3])\n",
      "Target Mask: tensor([False, False, False, False, False,  True,  True,  True])\n",
      "\n",
      "Item: 11\n",
      "tensor([10,  6,  6,  9, 11])\n",
      "Decoded Input: <BOS>669<EOS>\n",
      "Input: tensor([10,  6,  6,  9, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> six hundred and sixty-nine<EOS>\n",
      "Target: tensor([  4,  33,  12,  14,  58, 190,  22,   5])\n",
      "Target Mask: tensor([False, False, False, False, False, False, False, False])\n",
      "\n",
      "Item: 12\n",
      "tensor([10,  1,  2,  5, 11])\n",
      "Decoded Input: <BOS>125<EOS>\n",
      "Input: tensor([10,  1,  2,  5, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> one hundred and twenty-five<EOS>\n",
      "Target: tensor([  4,  47,  12,  14,  51, 190,  64,   5])\n",
      "Target Mask: tensor([False, False, False, False, False, False, False, False])\n",
      "\n",
      "Item: 13\n",
      "tensor([10,  4,  3,  3, 11])\n",
      "Decoded Input: <BOS>433<EOS>\n",
      "Input: tensor([10,  4,  3,  3, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> four hundred and thirty-three<EOS>\n",
      "Target: tensor([  4,  43,  12,  14,  52, 190,  60,   5])\n",
      "Target Mask: tensor([False, False, False, False, False, False, False, False])\n",
      "\n",
      "Item: 14\n",
      "tensor([10,  6,  1,  6, 11])\n",
      "Decoded Input: <BOS>616<EOS>\n",
      "Input: tensor([10,  6,  1,  6, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> six hundred and sixteen<EOS><PAD><PAD>\n",
      "Target: tensor([ 4, 33, 12, 14, 76,  5,  3,  3])\n",
      "Target Mask: tensor([False, False, False, False, False, False,  True,  True])\n",
      "\n",
      "Item: 15\n",
      "tensor([10,  5,  0,  3, 11])\n",
      "Decoded Input: <BOS>503<EOS>\n",
      "Input: tensor([10,  5,  0,  3, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> five hundred and three<EOS><PAD><PAD>\n",
      "Target: tensor([ 4, 50, 12, 14, 44,  5,  3,  3])\n",
      "Target Mask: tensor([False, False, False, False, False, False,  True,  True])\n",
      "\n",
      "Item: 16\n",
      "tensor([10,  3,  3,  1, 11])\n",
      "Decoded Input: <BOS>331<EOS>\n",
      "Input: tensor([10,  3,  3,  1, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> three hundred and thirty-one<EOS>\n",
      "Target: tensor([  4,  44,  12,  14,  52, 190,  41,   5])\n",
      "Target Mask: tensor([False, False, False, False, False, False, False, False])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "sp = spm.SentencePieceProcessor(model_file='tokenizer/num_to_words.model')\n",
    "\n",
    "for batch in dataloader:\n",
    "    input_seq = batch['input_seq']\n",
    "    target_seq = batch['target_seq']\n",
    "    input_mask = batch['input_mask']\n",
    "    target_mask = batch['target_mask']\n",
    "\n",
    "    for i in range(input_seq.size(0)):\n",
    "        decoded_input = decode_numeric(input_seq[i])\n",
    "        print('Item:', i + 1)\n",
    "        print(input_seq[i])\n",
    "        print(\"Decoded Input:\", decoded_input)\n",
    "        print(\"Input:\", input_seq[i])\n",
    "        print(\"Input Mask:\", input_mask[i])\n",
    "        decoded_target = sp.decode_ids(target_seq[i].tolist())\n",
    "        print(\"Decoded Target:\", decoded_target)\n",
    "        print(\"Target:\", target_seq[i])\n",
    "        print(\"Target Mask:\", target_mask[i])\n",
    "        print()\n",
    "\n",
    "    break  # Just show the first batch for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_vocab_size, target_vocab_size, d_model=512, nhead=8, num_layers=6, \n",
    "                 d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.pos_decoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        src_emb = self.pos_encoder(self.encoder_embedding(src))\n",
    "        tgt_emb = self.pos_decoder(self.decoder_embedding(tgt))\n",
    "        tgt_seq_len = tgt.size(1)\n",
    "        causal_mask = self.generate_square_subsequent_mask(tgt_seq_len)\n",
    "\n",
    "        output = self.transformer(\n",
    "            src=src_emb,\n",
    "            tgt=tgt_emb,\n",
    "            tgt_mask=causal_mask,\n",
    "            src_key_padding_mask=src_mask,\n",
    "            tgt_key_padding_mask=tgt_mask,\n",
    "            memory_key_padding_mask=src_mask\n",
    "        )\n",
    "        \n",
    "        return self.fc_out(output)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
    "        return mask.to(next(self.parameters()).device)\n",
    "        \n",
    "class PositionalEncoding(nn.Module):\n",
    "   def __init__(self, d_model, dropout=0.1, max_len=100):\n",
    "       super().__init__()\n",
    "       self.dropout = nn.Dropout(p=dropout)\n",
    "       position = torch.arange(max_len).unsqueeze(1)\n",
    "       div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "       pe = torch.zeros(max_len, d_model)\n",
    "       pe[:, 0::2] = torch.sin(position * div_term)\n",
    "       pe[:, 1::2] = torch.cos(position * div_term)\n",
    "       self.register_buffer('pe', pe)\n",
    "\n",
    "   def forward(self, x):\n",
    "       x = x + self.pe[:x.size(1)]\n",
    "       return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_vocab_size, target_vocab_size, d_model=512, nhead=8, num_layers=6,\n",
    "                 d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.pos_decoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Use Pre-LN\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, target_vocab_size)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        src_emb = self.pos_encoder(self.encoder_embedding(src))\n",
    "        tgt_emb = self.pos_decoder(self.decoder_embedding(tgt))\n",
    "\n",
    "        causal_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(src.device)\n",
    "\n",
    "        output = self.transformer(\n",
    "            src=src_emb,\n",
    "            tgt=tgt_emb,\n",
    "            tgt_mask=causal_mask,\n",
    "            src_key_padding_mask=src_mask,\n",
    "            tgt_key_padding_mask=tgt_mask,\n",
    "            memory_key_padding_mask=src_mask\n",
    "        )\n",
    "        return self.fc_out(output)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        # Same as your custom version: fill upper triangle with -inf\n",
    "        return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(1)]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoderLayer\n",
    "import math\n",
    "\n",
    "class CustomMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
    "        batch_size, query_len, _ = query.shape\n",
    "        _, key_len, _ = key.shape\n",
    "\n",
    "        Q = self.q_linear(query).view(batch_size, query_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.k_linear(key).view(batch_size, key_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.v_linear(value).view(batch_size, key_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        scores = scores.clamp(min=-10, max=10)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dim() == 2:\n",
    "                attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)\n",
    "            elif attn_mask.dim() == 3:\n",
    "                attn_mask = attn_mask.unsqueeze(1)\n",
    "            \n",
    "            scores = scores + attn_mask.to(scores.device)\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(key_padding_mask, -1e9)\n",
    "\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, query_len, self.d_model)\n",
    "\n",
    "        return self.out_linear(attn_output)\n",
    "\n",
    "class CustomTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = CustomMultiHeadAttention(d_model, nhead, dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask=None):\n",
    "        src2 = self.norm1(src)\n",
    "        src2 = self.self_attn(src2, src2, src2, key_padding_mask=src_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        \n",
    "        src2 = self.norm2(src)\n",
    "        src2 = self.linear2(self.dropout(F.relu(self.linear1(src2))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        \n",
    "        return src\n",
    "\n",
    "class CustomTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            CustomTransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask=mask)\n",
    "        return self.norm(src)\n",
    "\n",
    "class CustomTransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = CustomMultiHeadAttention(d_model, nhead, dropout)\n",
    "        self.multihead_attn = CustomMultiHeadAttention(d_model, nhead, dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        tgt2 = self.norm1(tgt)\n",
    "        tgt2 = self.self_attn(tgt2, tgt2, tgt2, attn_mask=tgt_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        \n",
    "        tgt2 = self.norm2(tgt)\n",
    "        tgt2 = self.multihead_attn(tgt2, memory, memory, key_padding_mask=memory_key_padding_mask)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        \n",
    "        tgt2 = self.norm3(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(F.relu(self.linear1(tgt2))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        \n",
    "        return tgt\n",
    "\n",
    "class CustomTransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            CustomTransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory, tgt_mask=tgt_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return self.norm(tgt)\n",
    "        \n",
    "\n",
    "class CustomTransformerModel(nn.Module):\n",
    "    def __init__(self, input_vocab_size, target_vocab_size, d_model=512, nhead=8, num_layers=6, \n",
    "                 dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.pos_decoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        self.encoder = CustomTransformerEncoder(d_model, nhead, num_layers, dim_feedforward, dropout)\n",
    "        self.decoder = CustomTransformerDecoder(d_model, nhead, num_layers, dim_feedforward, dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        src_emb = self.pos_encoder(self.encoder_embedding(src))\n",
    "        tgt_emb = self.pos_decoder(self.decoder_embedding(tgt))\n",
    "        \n",
    "        memory = self.encoder(src_emb, src_mask)\n",
    "\n",
    "        tgt_seq_len = tgt.size(1)\n",
    "        causal_mask = self.generate_square_subsequent_mask(tgt_seq_len)\n",
    "\n",
    "        output = self.decoder(\n",
    "            tgt_emb,\n",
    "            memory,\n",
    "            tgt_mask=causal_mask,\n",
    "            tgt_key_padding_mask=tgt_mask,\n",
    "            memory_key_padding_mask=src_mask\n",
    "        )\n",
    "                \n",
    "        return self.fc_out(output)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, num_epochs=20, learning_rate=1e-5):\n",
    "    # device = torch.device(\"cpu\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=OUTPUT_PAD_TOKEN_ID)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            src = batch['input_seq'].to(device)\n",
    "            tgt = batch['target_seq'].to(device)\n",
    "\n",
    "            src_mask = batch['input_mask'].to(device)\n",
    "            tgt_mask = batch['target_mask'].to(device)\n",
    "\n",
    "            src_mask = src_mask[:, :src.shape[1]].to(device)\n",
    "            tgt_mask = tgt_mask[:, :tgt.shape[1]].to(device)\n",
    "\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "\n",
    "            tgt_input = tgt_input.to(device)\n",
    "            tgt_output = tgt_output.to(device)\n",
    "\n",
    "\n",
    "            tgt_mask = tgt_mask[:, :-1]\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "\n",
    "            loss = criterion(output, tgt_output)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "            if torch.isnan(loss):\n",
    "                print(f\"NaN loss detected in batch! Skipping batch.\")\n",
    "                continue\n",
    "\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_VOCAB_SIZE = 15\n",
    "TARGET_VOCAB_SIZE = sp.get_piece_size()\n",
    "\n",
    "D_MODEL = 512\n",
    "NHEAD = 8\n",
    "NUM_LAYERS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/AML/lib/python3.8/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/AML/lib/python3.8/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.5090139853954314\n",
      "Epoch 2/10, Loss: 0.5476888692378998\n",
      "Epoch 3/10, Loss: 0.2155199134349823\n",
      "Epoch 4/10, Loss: 0.09992211926728486\n",
      "Epoch 5/10, Loss: 0.04334178383927792\n",
      "Epoch 6/10, Loss: 0.027678875247947873\n",
      "Epoch 7/10, Loss: 0.03649301657918841\n",
      "Epoch 8/10, Loss: 0.01910744180902839\n",
      "Epoch 9/10, Loss: 0.010055963702034204\n",
      "Epoch 10/10, Loss: 0.012515408467734232\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(INPUT_VOCAB_SIZE, TARGET_VOCAB_SIZE, D_MODEL, NHEAD, NUM_LAYERS)\n",
    "train_model(model, dataloader, num_epochs=10, learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.1648234832286835\n",
      "Epoch 2/10, Loss: 0.8054987573623658\n",
      "Epoch 3/10, Loss: 0.6699686259031296\n",
      "Epoch 4/10, Loss: 0.562727137207985\n",
      "Epoch 5/10, Loss: 0.44500924587249757\n",
      "Epoch 6/10, Loss: 0.362404305934906\n",
      "Epoch 7/10, Loss: 0.35321529805660246\n",
      "Epoch 8/10, Loss: 0.3346623182296753\n",
      "Epoch 9/10, Loss: 0.31380678415298463\n",
      "Epoch 10/10, Loss: 0.2723615711927414\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(INPUT_VOCAB_SIZE, TARGET_VOCAB_SIZE, D_MODEL, NHEAD, NUM_LAYERS)\n",
    "train_model(model, dataloader, num_epochs=10, learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.5125640583038331\n",
      "Epoch 2/10, Loss: 0.6356963342428208\n",
      "Epoch 3/10, Loss: 0.3920679274201393\n",
      "Epoch 4/10, Loss: 0.20385737776756285\n",
      "Epoch 5/10, Loss: 0.12987626418471337\n",
      "Epoch 6/10, Loss: 0.06255884770303964\n",
      "Epoch 7/10, Loss: 0.04029437642544508\n",
      "Epoch 8/10, Loss: 0.036034593260847035\n",
      "Epoch 9/10, Loss: 0.02728276448789984\n",
      "Epoch 10/10, Loss: 0.021707666837610304\n"
     ]
    }
   ],
   "source": [
    "custom_model = CustomTransformerModel(INPUT_VOCAB_SIZE, TARGET_VOCAB_SIZE, D_MODEL, NHEAD, NUM_LAYERS)\n",
    "train_model(custom_model, dataloader, num_epochs=10, learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one hundred and twenty'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def greedy_decoding(model, number, tokenizer_text, max_length=50):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # tokenize the input number\n",
    "        src = tokenize_numeric(pd.Series([number]))\n",
    "        src_mask = ~torch.ones(src.shape, dtype=torch.bool).to(device)\n",
    "\n",
    "        # initialize the target sequence with the BOS token\n",
    "        tgt = torch.tensor([[OUTPUT_BOS_TOKEN_ID]], dtype=torch.long).to(device)\n",
    "        tgt_mask = ~torch.ones((1, 1), dtype=torch.bool).to(device)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            output = model(src, tgt, src_mask, tgt_mask)\n",
    "            next_token = output[:, -1].argmax(dim=-1).unsqueeze(1)\n",
    "\n",
    "            # check if the next token is the EOS token \n",
    "            # and stop decoding if so\n",
    "            if next_token.item() == OUTPUT_EOS_TOKEN_ID:\n",
    "                break\n",
    "\n",
    "            # append the next token to the target sequence\n",
    "            tgt = torch.cat([tgt, next_token], dim=1)\n",
    "            tgt_mask = ~torch.ones(tgt.shape, dtype=torch.bool)\n",
    "            \n",
    "        output_tokens = tgt[0][1:].tolist()\n",
    "\n",
    "        # if EOS token is present, remove it and everything after it\n",
    "        if OUTPUT_EOS_TOKEN_ID in output_tokens:\n",
    "            output_tokens = output_tokens[:output_tokens.index(OUTPUT_EOS_TOKEN_ID)]\n",
    "        return tokenizer_text.decode(output_tokens)\n",
    "\n",
    "greedy_decoding(model, 120, sp, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'two hundred and thirty-four'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def beam_search(model, number, tokenizer_text, beam_width=3, max_length=50):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        src = tokenize_numeric(pd.Series([number])).to(device)\n",
    "        src_mask = ~torch.ones(src.shape, dtype=torch.bool).to(device)\n",
    "\n",
    "        beams = [(torch.tensor([[OUTPUT_BOS_TOKEN_ID]], dtype=torch.long).to(device), 0)]\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            new_beams = []\n",
    "            for seq, score in beams:\n",
    "\n",
    "                # if sequence ends with EOS token, \n",
    "                # just add it directly to new beams\n",
    "                if(seq[0, -1].item() == OUTPUT_EOS_TOKEN_ID):\n",
    "                    new_beams.append((seq, score))\n",
    "                    continue\n",
    "\n",
    "                tgt_mask = ~torch.ones(seq.shape, dtype=torch.bool).to(device)\n",
    "                output = model(src, seq, src_mask, tgt_mask)\n",
    "                logits = output[:, -1, :]\n",
    "                top_k_probs, top_k_tokens = torch.topk(logits, beam_width, dim=-1)\n",
    "                \n",
    "                for i in range(beam_width):\n",
    "                    next_token = top_k_tokens[0, i].unsqueeze(0).unsqueeze(0)\n",
    "                    new_seq = torch.cat([seq, next_token], dim=1)\n",
    "                    new_score = score + torch.log(top_k_probs[0, i])\n",
    "                    new_beams.append((new_seq, new_score))\n",
    "\n",
    "            # sort the new beams by score a\n",
    "            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "            # early stopping if all beams end with EOS token\n",
    "            if all(beams[i][0][0, -1] == OUTPUT_EOS_TOKEN_ID for i in range(len(beams))):\n",
    "                break\n",
    "        \n",
    "        best_seq = beams[0][0][0].tolist()[1:]\n",
    "        if OUTPUT_EOS_TOKEN_ID in best_seq:\n",
    "            best_seq = best_seq[:best_seq.index(OUTPUT_EOS_TOKEN_ID)]\n",
    "\n",
    "        return tokenizer_text.decode(best_seq)\n",
    "\n",
    "beam_search(model, 234, sp, beam_width=3, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one hundred and twenty'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_k_sampling(model, number, tokenizer_text, k=10, temperature=0.2, max_length=50, p=None):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        src = tokenize_numeric(pd.Series([number])).to(device)\n",
    "        tgt = torch.tensor([[OUTPUT_BOS_TOKEN_ID]], dtype=torch.long).to(device)\n",
    "        src_mask = ~torch.ones(src.shape, dtype=torch.bool).to(device)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            tgt_mask = ~torch.ones(tgt.shape, dtype=torch.bool).to(device)\n",
    "            output = model(src, tgt, src_mask, tgt_mask)\n",
    "            logits = output[:, -1, :]\n",
    "            \n",
    "            # apply temperature scaling\n",
    "            scaled_logits = logits / temperature\n",
    "            \n",
    "            def top_k(scaled_logits, k):\n",
    "                # apply top-k filtering\n",
    "                top_k_probs, top_k_tokens = torch.topk(scaled_logits, k, dim=-1)\n",
    "                probabilities = torch.softmax(top_k_probs, dim=-1)\n",
    "                return probabilities, top_k_tokens\n",
    "\n",
    "\n",
    "            def top_p(scaled_logits, p):\n",
    "                # top p\n",
    "                probs = torch.softmax(scaled_logits, dim=-1)\n",
    "                sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "\n",
    "                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "                cutoff = cumulative_probs > p\n",
    "                cutoff[..., 1:] = cutoff[..., :-1].clone()\n",
    "                cutoff[..., 0] = False\n",
    "\n",
    "                sorted_probs = sorted_probs.masked_fill(cutoff, 0.0)\n",
    "                return sorted_probs, sorted_indices\n",
    "            \n",
    "            if p:\n",
    "                probabilities, items = top_p(scaled_logits, p)\n",
    "            elif k:\n",
    "                probabilities, items = top_k(scaled_logits, k)\n",
    "            else:\n",
    "                raise ValueError(\"Either k or p must be provided\")\n",
    "\n",
    "            next_token = items[0, torch.multinomial(probabilities[0], 1)].unsqueeze(0)\n",
    "\n",
    "            if next_token.item() == OUTPUT_EOS_TOKEN_ID:\n",
    "                break\n",
    "\n",
    "            tgt = torch.cat([tgt, next_token], dim=1)\n",
    "        \n",
    "        output_tokens = tgt[0][1:].tolist()\n",
    "        return tokenizer_text.decode(output_tokens)\n",
    "\n",
    "\n",
    "top_k_sampling(model, 120, sp, k=5, max_length=50)\n",
    "top_k_sampling(custom_model, 120, sp, p=0.5, temperature=0.5, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, test_df, tokenizer_num, tokenizer_text):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for _, row in test_df.iterrows():\n",
    "        number = row['num']\n",
    "        word = row['word']\n",
    "        # predicted_word = predict(model, number, tokenizer_num, tokenizer_text)\n",
    "        # predicted_word = top_k_sampling_predict(model, number, sp, k=5, max_length=50)\n",
    "        predicted_word = beam_search_predict(model, number, sp, beam_width=3)\n",
    "        print(f\"Number: {number}, Predicted: {predicted_word}, Actual: {word}\")\n",
    "        if predicted_word == word:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: 1, Predicted: zero, Actual: one\n",
      "Number: 4, Predicted: forty, Actual: four\n",
      "Number: 13, Predicted: thirteen, Actual: thirteen\n",
      "Number: 14, Predicted: fourteen, Actual: fourteen\n",
      "Number: 20, Predicted: twenty, Actual: twenty\n",
      "Number: 21, Predicted: twenty-one, Actual: twenty-one\n",
      "Number: 27, Predicted: twenty-seven, Actual: twenty-seven\n",
      "Number: 32, Predicted: thirty-two, Actual: thirty-two\n",
      "Number: 34, Predicted: thirty-four, Actual: thirty-four\n",
      "Number: 35, Predicted: thirty-five, Actual: thirty-five\n",
      "Number: 40, Predicted: forty, Actual: forty\n",
      "Number: 47, Predicted: forty-seven, Actual: forty-seven\n",
      "Number: 52, Predicted: fifty-two, Actual: fifty-two\n",
      "Number: 58, Predicted: fifty-eight, Actual: fifty-eight\n",
      "Number: 62, Predicted: sixty-two, Actual: sixty-two\n",
      "Number: 64, Predicted: sixty-four, Actual: sixty-four\n",
      "Number: 71, Predicted: seventy-one, Actual: seventy-one\n",
      "Number: 80, Predicted: eighty, Actual: eighty\n",
      "Number: 85, Predicted: eighty-five, Actual: eighty-five\n",
      "Number: 87, Predicted: eighty-seven, Actual: eighty-seven\n",
      "Number: 91, Predicted: ninety-one, Actual: ninety-one\n",
      "Number: 95, Predicted: ninety-five, Actual: ninety-five\n",
      "Number: 98, Predicted: ninety-eight, Actual: ninety-eight\n",
      "Number: 99, Predicted: ninety-nine, Actual: ninety-nine\n",
      "Number: 102, Predicted: one hundred and two, Actual: one hundred and two\n",
      "Number: 105, Predicted: one hundred and five, Actual: one hundred and five\n",
      "Number: 106, Predicted: one hundred and six, Actual: one hundred and six\n",
      "Number: 121, Predicted: one hundred and twenty-one, Actual: one hundred and twenty-one\n",
      "Number: 128, Predicted: one hundred and twenty-eight, Actual: one hundred and twenty-eight\n",
      "Number: 130, Predicted: one hundred and thirty, Actual: one hundred and thirty\n",
      "Number: 134, Predicted: one hundred and thirty-four, Actual: one hundred and thirty-four\n",
      "Number: 138, Predicted: one hundred and thirty-eight, Actual: one hundred and thirty-eight\n",
      "Number: 156, Predicted: one hundred and fifty-six, Actual: one hundred and fifty-six\n",
      "Number: 159, Predicted: one hundred and fifty-nine, Actual: one hundred and fifty-nine\n",
      "Number: 160, Predicted: one hundred and sixty, Actual: one hundred and sixty\n",
      "Number: 161, Predicted: one hundred and sixty-one, Actual: one hundred and sixty-one\n",
      "Number: 166, Predicted: one hundred and sixty-six, Actual: one hundred and sixty-six\n",
      "Number: 170, Predicted: one hundred and seventy, Actual: one hundred and seventy\n",
      "Number: 187, Predicted: one hundred and eighty-seven, Actual: one hundred and eighty-seven\n",
      "Number: 189, Predicted: one hundred and eighty-nine, Actual: one hundred and eighty-nine\n",
      "Number: 191, Predicted: one hundred and ninety-one, Actual: one hundred and ninety-one\n",
      "Number: 200, Predicted: two hundred, Actual: two hundred\n",
      "Number: 201, Predicted: two hundred and one, Actual: two hundred and one\n",
      "Number: 205, Predicted: two hundred and five, Actual: two hundred and five\n",
      "Number: 206, Predicted: two hundred and six, Actual: two hundred and six\n",
      "Number: 214, Predicted: two hundred and fourteen, Actual: two hundred and fourteen\n",
      "Number: 216, Predicted: two hundred and sixteen, Actual: two hundred and sixteen\n",
      "Number: 217, Predicted: two hundred and seventeen, Actual: two hundred and seventeen\n",
      "Number: 230, Predicted: two hundred and thirty, Actual: two hundred and thirty\n",
      "Number: 240, Predicted: two hundred and forty, Actual: two hundred and forty\n",
      "Number: 241, Predicted: two hundred and forty-one, Actual: two hundred and forty-one\n",
      "Number: 242, Predicted: two hundred and forty-two, Actual: two hundred and forty-two\n",
      "Number: 243, Predicted: two hundred and forty-three, Actual: two hundred and forty-three\n",
      "Number: 251, Predicted: two hundred and fifty-one, Actual: two hundred and fifty-one\n",
      "Number: 252, Predicted: two hundred and fifty-two, Actual: two hundred and fifty-two\n",
      "Number: 269, Predicted: two hundred and sixty-nine, Actual: two hundred and sixty-nine\n",
      "Number: 270, Predicted: two hundred and seventy, Actual: two hundred and seventy\n",
      "Number: 273, Predicted: two hundred and seventy-three, Actual: two hundred and seventy-three\n",
      "Number: 276, Predicted: two hundred and seventy-six, Actual: two hundred and seventy-six\n",
      "Number: 288, Predicted: two hundred and eighty-eight, Actual: two hundred and eighty-eight\n",
      "Number: 295, Predicted: two hundred and ninety-five, Actual: two hundred and ninety-five\n",
      "Number: 308, Predicted: three hundred and eight, Actual: three hundred and eight\n",
      "Number: 313, Predicted: three hundred and thirteen, Actual: three hundred and thirteen\n",
      "Number: 315, Predicted: three hundred and fifteen, Actual: three hundred and fifteen\n",
      "Number: 330, Predicted: three hundred and thirty, Actual: three hundred and thirty\n",
      "Number: 337, Predicted: three hundred and thirty-seven, Actual: three hundred and thirty-seven\n",
      "Number: 339, Predicted: three hundred and thirty-nine, Actual: three hundred and thirty-nine\n",
      "Number: 343, Predicted: three hundred and forty-three, Actual: three hundred and forty-three\n",
      "Number: 345, Predicted: three hundred and forty-five, Actual: three hundred and forty-five\n",
      "Number: 366, Predicted: three hundred and sixty-six, Actual: three hundred and sixty-six\n",
      "Number: 372, Predicted: three hundred and seventy-two, Actual: three hundred and seventy-two\n",
      "Number: 378, Predicted: three hundred and seventy-eight, Actual: three hundred and seventy-eight\n",
      "Number: 379, Predicted: three hundred and seventy-nine, Actual: three hundred and seventy-nine\n",
      "Number: 385, Predicted: three hundred and eighty-five, Actual: three hundred and eighty-five\n",
      "Number: 387, Predicted: three hundred and eighty-seven, Actual: three hundred and eighty-seven\n",
      "Number: 389, Predicted: three hundred and eighty-nine, Actual: three hundred and eighty-nine\n",
      "Number: 391, Predicted: three hundred and ninety-one, Actual: three hundred and ninety-one\n",
      "Number: 392, Predicted: three hundred and ninety-two, Actual: three hundred and ninety-two\n",
      "Number: 397, Predicted: three hundred and ninety-seven, Actual: three hundred and ninety-seven\n",
      "Number: 401, Predicted: four hundred and one, Actual: four hundred and one\n",
      "Number: 406, Predicted: four hundred and six, Actual: four hundred and six\n",
      "Number: 413, Predicted: four hundred and thirteen, Actual: four hundred and thirteen\n",
      "Number: 418, Predicted: four hundred and eighteen, Actual: four hundred and eighteen\n",
      "Number: 427, Predicted: four hundred and twenty-seven, Actual: four hundred and twenty-seven\n",
      "Number: 435, Predicted: four hundred and thirty-five, Actual: four hundred and thirty-five\n",
      "Number: 454, Predicted: four hundred and fifty-four, Actual: four hundred and fifty-four\n",
      "Number: 455, Predicted: four hundred and fifty-five, Actual: four hundred and fifty-five\n",
      "Number: 458, Predicted: four hundred and fifty-eight, Actual: four hundred and fifty-eight\n",
      "Number: 459, Predicted: four hundred and fifty-nine, Actual: four hundred and fifty-nine\n",
      "Number: 460, Predicted: four hundred and sixty, Actual: four hundred and sixty\n",
      "Number: 461, Predicted: four hundred and sixty-one, Actual: four hundred and sixty-one\n",
      "Number: 466, Predicted: four hundred and sixty-six, Actual: four hundred and sixty-six\n",
      "Number: 471, Predicted: four hundred and seventy-one, Actual: four hundred and seventy-one\n",
      "Number: 474, Predicted: four hundred and seventy-four, Actual: four hundred and seventy-four\n",
      "Number: 475, Predicted: four hundred and seventy-five, Actual: four hundred and seventy-five\n",
      "Number: 476, Predicted: four hundred and seventy-six, Actual: four hundred and seventy-six\n",
      "Number: 484, Predicted: four hundred and eighty-four, Actual: four hundred and eighty-four\n",
      "Number: 489, Predicted: four hundred and eighty-nine, Actual: four hundred and eighty-nine\n",
      "Number: 491, Predicted: four hundred and ninety-one, Actual: four hundred and ninety-one\n",
      "Number: 492, Predicted: four hundred and ninety-two, Actual: four hundred and ninety-two\n",
      "Number: 498, Predicted: four hundred and ninety-eight, Actual: four hundred and ninety-eight\n",
      "Number: 502, Predicted: five hundred and two, Actual: five hundred and two\n",
      "Number: 504, Predicted: five hundred and four, Actual: five hundred and four\n",
      "Number: 508, Predicted: five hundred and eight, Actual: five hundred and eight\n",
      "Number: 510, Predicted: five hundred and ten, Actual: five hundred and ten\n",
      "Number: 520, Predicted: five hundred and twenty, Actual: five hundred and twenty\n",
      "Number: 524, Predicted: five hundred and twenty-four, Actual: five hundred and twenty-four\n",
      "Number: 540, Predicted: five hundred and forty, Actual: five hundred and forty\n",
      "Number: 546, Predicted: five hundred and forty-six, Actual: five hundred and forty-six\n",
      "Number: 553, Predicted: five hundred and fifty-three, Actual: five hundred and fifty-three\n",
      "Number: 555, Predicted: five hundred and fifty-five, Actual: five hundred and fifty-five\n",
      "Number: 556, Predicted: five hundred and fifty-six, Actual: five hundred and fifty-six\n",
      "Number: 560, Predicted: five hundred and sixty, Actual: five hundred and sixty\n",
      "Number: 561, Predicted: five hundred and sixty-one, Actual: five hundred and sixty-one\n",
      "Number: 562, Predicted: five hundred and sixty-two, Actual: five hundred and sixty-two\n",
      "Number: 563, Predicted: five hundred and sixty-three, Actual: five hundred and sixty-three\n",
      "Number: 564, Predicted: five hundred and sixty-four, Actual: five hundred and sixty-four\n",
      "Number: 565, Predicted: five hundred and sixty-five, Actual: five hundred and sixty-five\n",
      "Number: 566, Predicted: five hundred and sixty-six, Actual: five hundred and sixty-six\n",
      "Number: 573, Predicted: five hundred and seventy-three, Actual: five hundred and seventy-three\n",
      "Number: 574, Predicted: five hundred and seventy-four, Actual: five hundred and seventy-four\n",
      "Number: 577, Predicted: five hundred and seventy-seven, Actual: five hundred and seventy-seven\n",
      "Number: 592, Predicted: five hundred and ninety-two, Actual: five hundred and ninety-two\n",
      "Number: 600, Predicted: six hundred, Actual: six hundred\n",
      "Number: 612, Predicted: six hundred and twelve, Actual: six hundred and twelve\n",
      "Number: 614, Predicted: six hundred and fourteen, Actual: six hundred and fourteen\n",
      "Number: 642, Predicted: six hundred and forty-two, Actual: six hundred and forty-two\n",
      "Number: 646, Predicted: six hundred and forty-six, Actual: six hundred and forty-six\n",
      "Number: 647, Predicted: six hundred and forty-seven, Actual: six hundred and forty-seven\n",
      "Number: 654, Predicted: six hundred and fifty-four, Actual: six hundred and fifty-four\n",
      "Number: 661, Predicted: six hundred and sixty-one, Actual: six hundred and sixty-one\n",
      "Number: 663, Predicted: six hundred and thirty-three, Actual: six hundred and sixty-three\n",
      "Number: 674, Predicted: six hundred and seventy-four, Actual: six hundred and seventy-four\n",
      "Number: 681, Predicted: six hundred and eighty-one, Actual: six hundred and eighty-one\n",
      "Number: 683, Predicted: six hundred and eighty-three, Actual: six hundred and eighty-three\n",
      "Number: 686, Predicted: six hundred and eighty-six, Actual: six hundred and eighty-six\n",
      "Number: 690, Predicted: six hundred and ninety, Actual: six hundred and ninety\n",
      "Number: 698, Predicted: six hundred and ninety-eight, Actual: six hundred and ninety-eight\n",
      "Number: 699, Predicted: six hundred and ninety-nine, Actual: six hundred and ninety-nine\n",
      "Number: 700, Predicted: seven hundred, Actual: seven hundred\n",
      "Number: 701, Predicted: seven hundred and one, Actual: seven hundred and one\n",
      "Number: 702, Predicted: seven hundred and two, Actual: seven hundred and two\n",
      "Number: 719, Predicted: seven hundred and nineteen, Actual: seven hundred and nineteen\n",
      "Number: 724, Predicted: seven hundred and twenty-four, Actual: seven hundred and twenty-four\n",
      "Number: 725, Predicted: seven hundred and twenty-five, Actual: seven hundred and twenty-five\n",
      "Number: 726, Predicted: seven hundred and twenty-six, Actual: seven hundred and twenty-six\n",
      "Number: 727, Predicted: seven hundred and twenty-seven, Actual: seven hundred and twenty-seven\n",
      "Number: 729, Predicted: seven hundred and twenty-nine, Actual: seven hundred and twenty-nine\n",
      "Number: 733, Predicted: seven hundred and thirty-three, Actual: seven hundred and thirty-three\n",
      "Number: 738, Predicted: seven hundred and thirty-eight, Actual: seven hundred and thirty-eight\n",
      "Number: 742, Predicted: seven hundred and forty-two, Actual: seven hundred and forty-two\n",
      "Number: 747, Predicted: seven hundred and forty-seven, Actual: seven hundred and forty-seven\n",
      "Number: 748, Predicted: seven hundred and forty-eight, Actual: seven hundred and forty-eight\n",
      "Number: 763, Predicted: seven hundred and sixty-three, Actual: seven hundred and sixty-three\n",
      "Number: 766, Predicted: seven hundred and sixty-six, Actual: seven hundred and sixty-six\n",
      "Number: 768, Predicted: seven hundred and sixty-eight, Actual: seven hundred and sixty-eight\n",
      "Number: 769, Predicted: seven hundred and sixty-nine, Actual: seven hundred and sixty-nine\n",
      "Number: 771, Predicted: seven hundred and seventy-one, Actual: seven hundred and seventy-one\n",
      "Number: 772, Predicted: seven hundred and seventy-two, Actual: seven hundred and seventy-two\n",
      "Number: 775, Predicted: seven hundred and seventy-five, Actual: seven hundred and seventy-five\n",
      "Number: 776, Predicted: seven hundred and seventy-six, Actual: seven hundred and seventy-six\n",
      "Number: 779, Predicted: seven hundred and seventy-nine, Actual: seven hundred and seventy-nine\n",
      "Number: 782, Predicted: seven hundred and eighty-two, Actual: seven hundred and eighty-two\n",
      "Number: 791, Predicted: seven hundred and ninety-one, Actual: seven hundred and ninety-one\n",
      "Number: 794, Predicted: seven hundred and ninety-four, Actual: seven hundred and ninety-four\n",
      "Number: 795, Predicted: seven hundred and ninety-five, Actual: seven hundred and ninety-five\n",
      "Number: 804, Predicted: eight hundred and four, Actual: eight hundred and four\n",
      "Number: 805, Predicted: eight hundred and five, Actual: eight hundred and five\n",
      "Number: 812, Predicted: eight hundred and twelve, Actual: eight hundred and twelve\n",
      "Number: 815, Predicted: eight hundred and fifteen, Actual: eight hundred and fifteen\n",
      "Number: 818, Predicted: eight hundred and eighteen, Actual: eight hundred and eighteen\n",
      "Number: 821, Predicted: eight hundred and twenty-one, Actual: eight hundred and twenty-one\n",
      "Number: 831, Predicted: eight hundred and thirty-one, Actual: eight hundred and thirty-one\n",
      "Number: 838, Predicted: eight hundred and thirty-eight, Actual: eight hundred and thirty-eight\n",
      "Number: 839, Predicted: eight hundred and thirty-nine, Actual: eight hundred and thirty-nine\n",
      "Number: 840, Predicted: eight hundred and forty, Actual: eight hundred and forty\n",
      "Number: 847, Predicted: eight hundred and forty-seven, Actual: eight hundred and forty-seven\n",
      "Number: 848, Predicted: eight hundred and forty-eight, Actual: eight hundred and forty-eight\n",
      "Number: 856, Predicted: eight hundred and fifty-six, Actual: eight hundred and fifty-six\n",
      "Number: 857, Predicted: eight hundred and fifty-seven, Actual: eight hundred and fifty-seven\n",
      "Number: 860, Predicted: eight hundred and sixty, Actual: eight hundred and sixty\n",
      "Number: 862, Predicted: eight hundred and sixty-two, Actual: eight hundred and sixty-two\n",
      "Number: 871, Predicted: eight hundred and seventy-one, Actual: eight hundred and seventy-one\n",
      "Number: 875, Predicted: eight hundred and seventy-five, Actual: eight hundred and seventy-five\n",
      "Number: 878, Predicted: eight hundred and seventy-eight, Actual: eight hundred and seventy-eight\n",
      "Number: 880, Predicted: eight hundred and eighty, Actual: eight hundred and eighty\n",
      "Number: 888, Predicted: eight hundred and eighty-eight, Actual: eight hundred and eighty-eight\n",
      "Number: 890, Predicted: eight hundred and ninety, Actual: eight hundred and ninety\n",
      "Number: 897, Predicted: eight hundred and ninety-seven, Actual: eight hundred and ninety-seven\n",
      "Number: 929, Predicted: nine hundred and twenty-nine, Actual: nine hundred and twenty-nine\n",
      "Number: 937, Predicted: nine hundred and thirty-seven, Actual: nine hundred and thirty-seven\n",
      "Number: 944, Predicted: nine hundred and forty-four, Actual: nine hundred and forty-four\n",
      "Number: 955, Predicted: nine hundred and fifty-five, Actual: nine hundred and fifty-five\n",
      "Number: 957, Predicted: nine hundred and fifty-seven, Actual: nine hundred and fifty-seven\n",
      "Number: 962, Predicted: nine hundred and sixty-two, Actual: nine hundred and sixty-two\n",
      "Number: 976, Predicted: nine hundred and seventy-six, Actual: nine hundred and seventy-six\n",
      "Number: 980, Predicted: nine hundred and eighty, Actual: nine hundred and eighty\n",
      "Number: 982, Predicted: nine hundred and eighty-two, Actual: nine hundred and eighty-two\n",
      "Number: 992, Predicted: nine hundred and ninety-two, Actual: nine hundred and ninety-two\n",
      "Number: 996, Predicted: nine hundred and ninety-six, Actual: nine hundred and ninety-six\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.985"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model, val_df, tokenize_numeric, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: 1, Predicted: eleven, Actual: one\n",
      "Number: 4, Predicted: forty-, Actual: four\n",
      "Number: 13, Predicted: one hundred and thirty-three, Actual: thirteen\n",
      "Number: 14, Predicted: one hundred and fourteen, Actual: fourteen\n",
      "Number: 20, Predicted: two hundred, Actual: twenty\n",
      "Number: 21, Predicted: twenty-two, Actual: twenty-one\n",
      "Number: 27, Predicted: twenty-seven, Actual: twenty-seven\n",
      "Number: 32, Predicted: three hundred and twenty-three, Actual: thirty-two\n",
      "Number: 34, Predicted: three hundred and forty-three, Actual: thirty-four\n",
      "Number: 35, Predicted: thirty-five, Actual: thirty-five\n",
      "Number: 40, Predicted: four hundred, Actual: forty\n",
      "Number: 47, Predicted: forty-seven, Actual: forty-seven\n",
      "Number: 52, Predicted: fifty-five, Actual: fifty-two\n",
      "Number: 58, Predicted: fifty-eight, Actual: fifty-eight\n",
      "Number: 62, Predicted: sixty-two, Actual: sixty-two\n",
      "Number: 64, Predicted: sixty-four, Actual: sixty-four\n",
      "Number: 71, Predicted: seven hundred and eleven, Actual: seventy-one\n",
      "Number: 80, Predicted: eighty, Actual: eighty\n",
      "Number: 85, Predicted: eighty-five, Actual: eighty-five\n",
      "Number: 87, Predicted: eighty-seven, Actual: eighty-seven\n",
      "Number: 91, Predicted: one hundred and ninety-nine, Actual: ninety-one\n",
      "Number: 95, Predicted: nine hundred and fifty-nine, Actual: ninety-five\n",
      "Number: 98, Predicted: eight hundred and ninety-nine, Actual: ninety-eight\n",
      "Number: 99, Predicted: ninety-nine, Actual: ninety-nine\n",
      "Number: 102, Predicted: one hundred and two, Actual: one hundred and two\n",
      "Number: 105, Predicted: one hundred and five, Actual: one hundred and five\n",
      "Number: 106, Predicted: one hundred and six, Actual: one hundred and six\n",
      "Number: 121, Predicted: one hundred and twenty-two, Actual: one hundred and twenty-one\n",
      "Number: 128, Predicted: one hundred and twenty-eight, Actual: one hundred and twenty-eight\n",
      "Number: 130, Predicted: one hundred and three, Actual: one hundred and thirty\n",
      "Number: 134, Predicted: one hundred and thirty-four, Actual: one hundred and thirty-four\n",
      "Number: 138, Predicted: one hundred and thirty-eight, Actual: one hundred and thirty-eight\n",
      "Number: 156, Predicted: one hundred and fifty-six, Actual: one hundred and fifty-six\n",
      "Number: 159, Predicted: one hundred and fifty-nine, Actual: one hundred and fifty-nine\n",
      "Number: 160, Predicted: one hundred and sixty, Actual: one hundred and sixty\n",
      "Number: 161, Predicted: one hundred and sixteen, Actual: one hundred and sixty-one\n",
      "Number: 166, Predicted: one hundred and sixty-six, Actual: one hundred and sixty-six\n",
      "Number: 170, Predicted: seven hundred and ten, Actual: one hundred and seventy\n",
      "Number: 187, Predicted: one hundred and eighty-seven, Actual: one hundred and eighty-seven\n",
      "Number: 189, Predicted: one hundred and eighty-nine, Actual: one hundred and eighty-nine\n",
      "Number: 191, Predicted: one hundred and ninety-nine, Actual: one hundred and ninety-one\n",
      "Number: 200, Predicted: two hundred, Actual: two hundred\n",
      "Number: 201, Predicted: two hundred, Actual: two hundred and one\n",
      "Number: 205, Predicted: two hundred and five, Actual: two hundred and five\n",
      "Number: 206, Predicted: two hundred and eight, Actual: two hundred and six\n",
      "Number: 214, Predicted: two hundred and twelve, Actual: two hundred and fourteen\n",
      "Number: 216, Predicted: two hundred and twelve, Actual: two hundred and sixteen\n",
      "Number: 217, Predicted: two hundred and seventeen, Actual: two hundred and seventeen\n",
      "Number: 230, Predicted: two hundred and thirty, Actual: two hundred and thirty\n",
      "Number: 240, Predicted: two hundred and twenty, Actual: two hundred and forty\n",
      "Number: 241, Predicted: two hundred and twenty-one, Actual: two hundred and forty-one\n",
      "Number: 242, Predicted: two hundred and twenty-four, Actual: two hundred and forty-two\n",
      "Number: 243, Predicted: two hundred and twenty-three, Actual: two hundred and forty-three\n",
      "Number: 251, Predicted: two hundred and fifty-one, Actual: two hundred and fifty-one\n",
      "Number: 252, Predicted: two hundred and fifty-five, Actual: two hundred and fifty-two\n",
      "Number: 269, Predicted: two hundred and sixty-nine, Actual: two hundred and sixty-nine\n",
      "Number: 270, Predicted: two hundred and seven, Actual: two hundred and seventy\n",
      "Number: 273, Predicted: two hundred and seventy-three, Actual: two hundred and seventy-three\n",
      "Number: 276, Predicted: seven hundred and twenty-two, Actual: two hundred and seventy-six\n",
      "Number: 288, Predicted: eight hundred and twenty-two, Actual: two hundred and eighty-eight\n",
      "Number: 295, Predicted: nine hundred and twenty-five, Actual: two hundred and ninety-five\n",
      "Number: 308, Predicted: three hundred and eight, Actual: three hundred and eight\n",
      "Number: 313, Predicted: three hundred and thirteen, Actual: three hundred and thirteen\n",
      "Number: 315, Predicted: three hundred and fifteen, Actual: three hundred and fifteen\n",
      "Number: 330, Predicted: three hundred and thirty, Actual: three hundred and thirty\n",
      "Number: 337, Predicted: three hundred and thirty-seven, Actual: three hundred and thirty-seven\n",
      "Number: 339, Predicted: three hundred and ninety-three, Actual: three hundred and thirty-nine\n",
      "Number: 343, Predicted: three hundred and thirty-four, Actual: three hundred and forty-three\n",
      "Number: 345, Predicted: three hundred and fifty-four, Actual: three hundred and forty-five\n",
      "Number: 366, Predicted: six hundred and thirty-three, Actual: three hundred and sixty-six\n",
      "Number: 372, Predicted: three hundred and seventy-three, Actual: three hundred and seventy-two\n",
      "Number: 378, Predicted: three hundred and seventy-three, Actual: three hundred and seventy-eight\n",
      "Number: 379, Predicted: three hundred and seventy-nine, Actual: three hundred and seventy-nine\n",
      "Number: 385, Predicted: three hundred and eighty-three, Actual: three hundred and eighty-five\n",
      "Number: 387, Predicted: three hundred and eighty-seven, Actual: three hundred and eighty-seven\n",
      "Number: 389, Predicted: three hundred and eighty-nine, Actual: three hundred and eighty-nine\n",
      "Number: 391, Predicted: three hundred and nineteen, Actual: three hundred and ninety-one\n",
      "Number: 392, Predicted: three hundred and ninety-two, Actual: three hundred and ninety-two\n",
      "Number: 397, Predicted: three hundred and ninety-seven, Actual: three hundred and ninety-seven\n",
      "Number: 401, Predicted: four hundred and one, Actual: four hundred and one\n",
      "Number: 406, Predicted: four hundred, Actual: four hundred and six\n",
      "Number: 413, Predicted: four hundred and thirty-one, Actual: four hundred and thirteen\n",
      "Number: 418, Predicted: four hundred and eighty-one, Actual: four hundred and eighteen\n",
      "Number: 427, Predicted: four hundred and twenty-eight, Actual: four hundred and twenty-seven\n",
      "Number: 435, Predicted: four hundred and thirty-five, Actual: four hundred and thirty-five\n",
      "Number: 454, Predicted: four hundred and fifty-four, Actual: four hundred and fifty-four\n",
      "Number: 455, Predicted: five hundred and forty-four, Actual: four hundred and fifty-five\n",
      "Number: 458, Predicted: five hundred and forty-eight, Actual: four hundred and fifty-eight\n",
      "Number: 459, Predicted: four hundred and fifty-nine, Actual: four hundred and fifty-nine\n",
      "Number: 460, Predicted: four hundred and sixty, Actual: four hundred and sixty\n",
      "Number: 461, Predicted: four hundred and sixteen, Actual: four hundred and sixty-one\n",
      "Number: 466, Predicted: four hundred and sixty-three, Actual: four hundred and sixty-six\n",
      "Number: 471, Predicted: four hundred and seventeen, Actual: four hundred and seventy-one\n",
      "Number: 474, Predicted: four hundred and forty-seven, Actual: four hundred and seventy-four\n",
      "Number: 475, Predicted: four hundred and fifty-seven, Actual: four hundred and seventy-five\n",
      "Number: 476, Predicted: four hundred and sixty-seven, Actual: four hundred and seventy-six\n",
      "Number: 484, Predicted: eight hundred and forty-four, Actual: four hundred and eighty-four\n",
      "Number: 489, Predicted: eight hundred and forty-nine, Actual: four hundred and eighty-nine\n",
      "Number: 491, Predicted: four hundred and ninety-one, Actual: four hundred and ninety-one\n",
      "Number: 492, Predicted: four hundred and twenty-nine, Actual: four hundred and ninety-two\n",
      "Number: 498, Predicted: four hundred and ninety-eight, Actual: four hundred and ninety-eight\n",
      "Number: 502, Predicted: five hundred, Actual: five hundred and two\n",
      "Number: 504, Predicted: five hundred and four, Actual: five hundred and four\n",
      "Number: 508, Predicted: eight hundred and five, Actual: five hundred and eight\n",
      "Number: 510, Predicted: five hundred and one, Actual: five hundred and ten\n",
      "Number: 520, Predicted: five hundred and twenty, Actual: five hundred and twenty\n",
      "Number: 524, Predicted: five hundred and twenty-four, Actual: five hundred and twenty-four\n",
      "Number: 540, Predicted: five hundred and forty, Actual: five hundred and forty\n",
      "Number: 546, Predicted: five hundred and forty-six, Actual: five hundred and forty-six\n",
      "Number: 553, Predicted: five hundred and thirty-nine, Actual: five hundred and fifty-three\n",
      "Number: 555, Predicted: fifty-five, Actual: five hundred and fifty-five\n",
      "Number: 556, Predicted: five hundred and sixty-nine, Actual: five hundred and fifty-six\n",
      "Number: 560, Predicted: five hundred and six, Actual: five hundred and sixty\n",
      "Number: 561, Predicted: five hundred and sixteen, Actual: five hundred and sixty-one\n",
      "Number: 562, Predicted: five hundred and sixty-three, Actual: five hundred and sixty-two\n",
      "Number: 563, Predicted: five hundred and thirty-six, Actual: five hundred and sixty-three\n",
      "Number: 564, Predicted: five hundred and sixty-four, Actual: five hundred and sixty-four\n",
      "Number: 565, Predicted: five hundred and sixty-nine, Actual: five hundred and sixty-five\n",
      "Number: 566, Predicted: six hundred and fifty-eight, Actual: five hundred and sixty-six\n",
      "Number: 573, Predicted: five hundred and seventy-three, Actual: five hundred and seventy-three\n",
      "Number: 574, Predicted: five hundred and forty-seven, Actual: five hundred and seventy-four\n",
      "Number: 577, Predicted: seven hundred and fifty-five, Actual: five hundred and seventy-seven\n",
      "Number: 592, Predicted: five hundred and ninety-two, Actual: five hundred and ninety-two\n",
      "Number: 600, Predicted: six hundred, Actual: six hundred\n",
      "Number: 612, Predicted: one hundred and sixty-two, Actual: six hundred and twelve\n",
      "Number: 614, Predicted: one hundred and sixty-four, Actual: six hundred and fourteen\n",
      "Number: 642, Predicted: six hundred and forty-four, Actual: six hundred and forty-two\n",
      "Number: 646, Predicted: six hundred and sixty-four, Actual: six hundred and forty-six\n",
      "Number: 647, Predicted: six hundred and seventy-four, Actual: six hundred and forty-seven\n",
      "Number: 654, Predicted: six hundred and fifty-four, Actual: six hundred and fifty-four\n",
      "Number: 661, Predicted: six hundred and sixteen, Actual: six hundred and sixty-one\n",
      "Number: 663, Predicted: six hundred and sixty-three, Actual: six hundred and sixty-three\n",
      "Number: 674, Predicted: seven hundred and sixty-four, Actual: six hundred and seventy-four\n",
      "Number: 681, Predicted: six hundred and eighty-one, Actual: six hundred and eighty-one\n",
      "Number: 683, Predicted: six hundred and eighty-three, Actual: six hundred and eighty-three\n",
      "Number: 686, Predicted: six hundred and eighty-eight, Actual: six hundred and eighty-six\n",
      "Number: 690, Predicted: six hundred and nine, Actual: six hundred and ninety\n",
      "Number: 698, Predicted: six hundred and ninety-eight, Actual: six hundred and ninety-eight\n",
      "Number: 699, Predicted: six hundred and ninety-nine, Actual: six hundred and ninety-nine\n",
      "Number: 700, Predicted: seven hundred, Actual: seven hundred\n",
      "Number: 701, Predicted: seven hundred, Actual: seven hundred and one\n",
      "Number: 702, Predicted: seven hundred, Actual: seven hundred and two\n",
      "Number: 719, Predicted: seven hundred and nineteen, Actual: seven hundred and nineteen\n",
      "Number: 724, Predicted: seven hundred and twenty-seven, Actual: seven hundred and twenty-four\n",
      "Number: 725, Predicted: seven hundred and fifty-two, Actual: seven hundred and twenty-five\n",
      "Number: 726, Predicted: seven hundred and twenty-six, Actual: seven hundred and twenty-six\n",
      "Number: 727, Predicted: seven hundred and twenty-seven, Actual: seven hundred and twenty-seven\n",
      "Number: 729, Predicted: seven hundred and twenty-nine, Actual: seven hundred and twenty-nine\n",
      "Number: 733, Predicted: seven hundred and thirty-three, Actual: seven hundred and thirty-three\n",
      "Number: 738, Predicted: seven hundred and thirty-eight, Actual: seven hundred and thirty-eight\n",
      "Number: 742, Predicted: seven hundred and twenty-four, Actual: seven hundred and forty-two\n",
      "Number: 747, Predicted: seven hundred and seventy-four, Actual: seven hundred and forty-seven\n",
      "Number: 748, Predicted: seven hundred and eighty-four, Actual: seven hundred and forty-eight\n",
      "Number: 763, Predicted: seven hundred and sixty-three, Actual: seven hundred and sixty-three\n",
      "Number: 766, Predicted: seven hundred and sixty-seven, Actual: seven hundred and sixty-six\n",
      "Number: 768, Predicted: seven hundred and eighty-six, Actual: seven hundred and sixty-eight\n",
      "Number: 769, Predicted: seven hundred and sixty-nine, Actual: seven hundred and sixty-nine\n",
      "Number: 771, Predicted: seven hundred and seventeen, Actual: seven hundred and seventy-one\n",
      "Number: 772, Predicted: seven hundred and twenty-seven, Actual: seven hundred and seventy-two\n",
      "Number: 775, Predicted: seven hundred and fifty-seven, Actual: seven hundred and seventy-five\n",
      "Number: 776, Predicted: seven hundred and sixty-seven, Actual: seven hundred and seventy-six\n",
      "Number: 779, Predicted: seven hundred and seventy-nine, Actual: seven hundred and seventy-nine\n",
      "Number: 782, Predicted: seven hundred and eighty-two, Actual: seven hundred and eighty-two\n",
      "Number: 791, Predicted: seven hundred and ninety-one, Actual: seven hundred and ninety-one\n",
      "Number: 794, Predicted: seven hundred and ninety-four, Actual: seven hundred and ninety-four\n",
      "Number: 795, Predicted: seven hundred and fifty-nine, Actual: seven hundred and ninety-five\n",
      "Number: 804, Predicted: eight hundred and four, Actual: eight hundred and four\n",
      "Number: 805, Predicted: eight hundred and eight, Actual: eight hundred and five\n",
      "Number: 812, Predicted: eight hundred and twelve, Actual: eight hundred and twelve\n",
      "Number: 815, Predicted: eight hundred and eighteen, Actual: eight hundred and fifteen\n",
      "Number: 818, Predicted: eighty-one, Actual: eight hundred and eighteen\n",
      "Number: 821, Predicted: eight hundred and twenty-eight, Actual: eight hundred and twenty-one\n",
      "Number: 831, Predicted: eight hundred and thirty-one, Actual: eight hundred and thirty-one\n",
      "Number: 838, Predicted: eight hundred and eighty-three, Actual: eight hundred and thirty-eight\n",
      "Number: 839, Predicted: eight hundred and thirty-nine, Actual: eight hundred and thirty-nine\n",
      "Number: 840, Predicted: eight hundred and forty, Actual: eight hundred and forty\n",
      "Number: 847, Predicted: eight hundred and eighty-four, Actual: eight hundred and forty-seven\n",
      "Number: 848, Predicted: eight hundred and eighty-four, Actual: eight hundred and forty-eight\n",
      "Number: 856, Predicted: eight hundred and sixty-five, Actual: eight hundred and fifty-six\n",
      "Number: 857, Predicted: eight hundred and fifty-seven, Actual: eight hundred and fifty-seven\n",
      "Number: 860, Predicted: eight hundred and sixty, Actual: eight hundred and sixty\n",
      "Number: 862, Predicted: eight hundred and twenty-six, Actual: eight hundred and sixty-two\n",
      "Number: 871, Predicted: eight hundred and seventeen, Actual: eight hundred and seventy-one\n",
      "Number: 875, Predicted: eight hundred and seventy-five, Actual: eight hundred and seventy-five\n",
      "Number: 878, Predicted: eight hundred and eighty-seven, Actual: eight hundred and seventy-eight\n",
      "Number: 880, Predicted: eight hundred and eighty, Actual: eight hundred and eighty\n",
      "Number: 888, Predicted: eighty-eight, Actual: eight hundred and eighty-eight\n",
      "Number: 890, Predicted: eight hundred and ninety, Actual: eight hundred and ninety\n",
      "Number: 897, Predicted: eight hundred and seventy-nine, Actual: eight hundred and ninety-seven\n",
      "Number: 929, Predicted: nine hundred and twenty-nine, Actual: nine hundred and twenty-nine\n",
      "Number: 937, Predicted: nine hundred and thirty-seven, Actual: nine hundred and thirty-seven\n",
      "Number: 944, Predicted: four hundred and ninety-nine, Actual: nine hundred and forty-four\n",
      "Number: 955, Predicted: nine hundred and fifty-nine, Actual: nine hundred and fifty-five\n",
      "Number: 957, Predicted: nine hundred and fifty-seven, Actual: nine hundred and fifty-seven\n",
      "Number: 962, Predicted: nine hundred and sixty-nine, Actual: nine hundred and sixty-two\n",
      "Number: 976, Predicted: nine hundred and sixty-seven, Actual: nine hundred and seventy-six\n",
      "Number: 980, Predicted: eight hundred and nine, Actual: nine hundred and eighty\n",
      "Number: 982, Predicted: nine hundred and twenty-eight, Actual: nine hundred and eighty-two\n",
      "Number: 992, Predicted: nine hundred and twenty-nine, Actual: nine hundred and ninety-two\n",
      "Number: 996, Predicted: nine hundred and sixty-nine, Actual: nine hundred and ninety-six\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.435"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model, val_df, tokenize_numeric, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: 1, Predicted: eleven, Actual: one\n",
      "Number: 4, Predicted: forty-four, Actual: four\n",
      "Number: 13, Predicted: thirty-three, Actual: thirteen\n",
      "Number: 14, Predicted: fourteen, Actual: fourteen\n",
      "Number: 20, Predicted: twenty, Actual: twenty\n",
      "Number: 21, Predicted: twenty-one, Actual: twenty-one\n",
      "Number: 27, Predicted: twenty-seven, Actual: twenty-seven\n",
      "Number: 32, Predicted: thirty-two, Actual: thirty-two\n",
      "Number: 34, Predicted: thirty-four, Actual: thirty-four\n",
      "Number: 35, Predicted: thirty-five, Actual: thirty-five\n",
      "Number: 40, Predicted: forty, Actual: forty\n",
      "Number: 47, Predicted: forty-seven, Actual: forty-seven\n",
      "Number: 52, Predicted: fifty-two, Actual: fifty-two\n",
      "Number: 58, Predicted: fifty-eight, Actual: fifty-eight\n",
      "Number: 62, Predicted: sixty-two, Actual: sixty-two\n",
      "Number: 64, Predicted: sixty-four, Actual: sixty-four\n",
      "Number: 71, Predicted: seventy-one, Actual: seventy-one\n",
      "Number: 80, Predicted: eighty, Actual: eighty\n",
      "Number: 85, Predicted: eighty-five, Actual: eighty-five\n",
      "Number: 87, Predicted: eighty-seven, Actual: eighty-seven\n",
      "Number: 91, Predicted: ninety-one, Actual: ninety-one\n",
      "Number: 95, Predicted: ninety-five, Actual: ninety-five\n",
      "Number: 98, Predicted: ninety-eight, Actual: ninety-eight\n",
      "Number: 99, Predicted: ninety-nine, Actual: ninety-nine\n",
      "Number: 102, Predicted: one hundred and two, Actual: one hundred and two\n",
      "Number: 105, Predicted: one hundred and five, Actual: one hundred and five\n",
      "Number: 106, Predicted: one hundred and six, Actual: one hundred and six\n",
      "Number: 121, Predicted: one hundred and twenty-one, Actual: one hundred and twenty-one\n",
      "Number: 128, Predicted: one hundred and twenty-eight, Actual: one hundred and twenty-eight\n",
      "Number: 130, Predicted: one hundred and thirty, Actual: one hundred and thirty\n",
      "Number: 134, Predicted: one hundred and thirty-four, Actual: one hundred and thirty-four\n",
      "Number: 138, Predicted: one hundred and thirty-eight, Actual: one hundred and thirty-eight\n",
      "Number: 156, Predicted: one hundred and fifty-six, Actual: one hundred and fifty-six\n",
      "Number: 159, Predicted: one hundred and fifty-nine, Actual: one hundred and fifty-nine\n",
      "Number: 160, Predicted: one hundred and sixty, Actual: one hundred and sixty\n",
      "Number: 161, Predicted: one hundred and sixty-one, Actual: one hundred and sixty-one\n",
      "Number: 166, Predicted: one hundred and sixty-six, Actual: one hundred and sixty-six\n",
      "Number: 170, Predicted: one hundred and seventy, Actual: one hundred and seventy\n",
      "Number: 187, Predicted: one hundred and eighty-seven, Actual: one hundred and eighty-seven\n",
      "Number: 189, Predicted: one hundred and eighty-nine, Actual: one hundred and eighty-nine\n",
      "Number: 191, Predicted: one hundred and ninety-one, Actual: one hundred and ninety-one\n",
      "Number: 200, Predicted: two hundred, Actual: two hundred\n",
      "Number: 201, Predicted: two hundred and one, Actual: two hundred and one\n",
      "Number: 205, Predicted: two hundred and five, Actual: two hundred and five\n",
      "Number: 206, Predicted: two hundred and six, Actual: two hundred and six\n",
      "Number: 214, Predicted: two hundred and fourteen, Actual: two hundred and fourteen\n",
      "Number: 216, Predicted: two hundred and sixteen, Actual: two hundred and sixteen\n",
      "Number: 217, Predicted: two hundred and seventeen, Actual: two hundred and seventeen\n",
      "Number: 230, Predicted: two hundred and thirty, Actual: two hundred and thirty\n",
      "Number: 240, Predicted: two hundred and forty, Actual: two hundred and forty\n",
      "Number: 241, Predicted: two hundred and forty-one, Actual: two hundred and forty-one\n",
      "Number: 242, Predicted: two hundred and forty-two, Actual: two hundred and forty-two\n",
      "Number: 243, Predicted: two hundred and forty-three, Actual: two hundred and forty-three\n",
      "Number: 251, Predicted: two hundred and fifty-one, Actual: two hundred and fifty-one\n",
      "Number: 252, Predicted: two hundred and fifty-two, Actual: two hundred and fifty-two\n",
      "Number: 269, Predicted: two hundred and sixty-nine, Actual: two hundred and sixty-nine\n",
      "Number: 270, Predicted: two hundred and seventy, Actual: two hundred and seventy\n",
      "Number: 273, Predicted: two hundred and seventy-three, Actual: two hundred and seventy-three\n",
      "Number: 276, Predicted: two hundred and seventy-six, Actual: two hundred and seventy-six\n",
      "Number: 288, Predicted: two hundred and eighty-eight, Actual: two hundred and eighty-eight\n",
      "Number: 295, Predicted: two hundred and ninety-five, Actual: two hundred and ninety-five\n",
      "Number: 308, Predicted: three hundred and eight, Actual: three hundred and eight\n",
      "Number: 313, Predicted: three hundred and thirteen, Actual: three hundred and thirteen\n",
      "Number: 315, Predicted: three hundred and fifteen, Actual: three hundred and fifteen\n",
      "Number: 330, Predicted: three hundred and thirty, Actual: three hundred and thirty\n",
      "Number: 337, Predicted: three hundred and thirty-seven, Actual: three hundred and thirty-seven\n",
      "Number: 339, Predicted: three hundred and thirty-nine, Actual: three hundred and thirty-nine\n",
      "Number: 343, Predicted: three hundred and forty-three, Actual: three hundred and forty-three\n",
      "Number: 345, Predicted: three hundred and forty-five, Actual: three hundred and forty-five\n",
      "Number: 366, Predicted: three hundred and sixty-six, Actual: three hundred and sixty-six\n",
      "Number: 372, Predicted: three hundred and seventy-two, Actual: three hundred and seventy-two\n",
      "Number: 378, Predicted: three hundred and seventy-eight, Actual: three hundred and seventy-eight\n",
      "Number: 379, Predicted: three hundred and seventy-nine, Actual: three hundred and seventy-nine\n",
      "Number: 385, Predicted: three hundred and eighty-five, Actual: three hundred and eighty-five\n",
      "Number: 387, Predicted: three hundred and eighty-seven, Actual: three hundred and eighty-seven\n",
      "Number: 389, Predicted: three hundred and eighty-nine, Actual: three hundred and eighty-nine\n",
      "Number: 391, Predicted: three hundred and ninety-one, Actual: three hundred and ninety-one\n",
      "Number: 392, Predicted: three hundred and ninety-two, Actual: three hundred and ninety-two\n",
      "Number: 397, Predicted: three hundred and ninety-seven, Actual: three hundred and ninety-seven\n",
      "Number: 401, Predicted: four hundred and one, Actual: four hundred and one\n",
      "Number: 406, Predicted: four hundred and six, Actual: four hundred and six\n",
      "Number: 413, Predicted: four hundred and thirteen, Actual: four hundred and thirteen\n",
      "Number: 418, Predicted: four hundred and eighteen, Actual: four hundred and eighteen\n",
      "Number: 427, Predicted: four hundred and twenty-seven, Actual: four hundred and twenty-seven\n",
      "Number: 435, Predicted: four hundred and thirty-five, Actual: four hundred and thirty-five\n",
      "Number: 454, Predicted: four hundred and fifty-four, Actual: four hundred and fifty-four\n",
      "Number: 455, Predicted: four hundred and fifty-five, Actual: four hundred and fifty-five\n",
      "Number: 458, Predicted: four hundred and fifty-eight, Actual: four hundred and fifty-eight\n",
      "Number: 459, Predicted: four hundred and fifty-nine, Actual: four hundred and fifty-nine\n",
      "Number: 460, Predicted: four hundred and sixty, Actual: four hundred and sixty\n",
      "Number: 461, Predicted: four hundred and sixty-one, Actual: four hundred and sixty-one\n",
      "Number: 466, Predicted: four hundred and sixty-six, Actual: four hundred and sixty-six\n",
      "Number: 471, Predicted: four hundred and seventy-one, Actual: four hundred and seventy-one\n",
      "Number: 474, Predicted: four hundred and seventy-four, Actual: four hundred and seventy-four\n",
      "Number: 475, Predicted: four hundred and seventy-five, Actual: four hundred and seventy-five\n",
      "Number: 476, Predicted: four hundred and seventy-six, Actual: four hundred and seventy-six\n",
      "Number: 484, Predicted: four hundred and eighty-four, Actual: four hundred and eighty-four\n",
      "Number: 489, Predicted: four hundred and eighty-nine, Actual: four hundred and eighty-nine\n",
      "Number: 491, Predicted: four hundred and ninety-one, Actual: four hundred and ninety-one\n",
      "Number: 492, Predicted: four hundred and ninety-two, Actual: four hundred and ninety-two\n",
      "Number: 498, Predicted: four hundred and ninety-eight, Actual: four hundred and ninety-eight\n",
      "Number: 502, Predicted: five hundred and two, Actual: five hundred and two\n",
      "Number: 504, Predicted: five hundred and four, Actual: five hundred and four\n",
      "Number: 508, Predicted: five hundred and eight, Actual: five hundred and eight\n",
      "Number: 510, Predicted: five hundred and ten, Actual: five hundred and ten\n",
      "Number: 520, Predicted: five hundred and twenty, Actual: five hundred and twenty\n",
      "Number: 524, Predicted: five hundred and twenty-four, Actual: five hundred and twenty-four\n",
      "Number: 540, Predicted: five hundred and forty, Actual: five hundred and forty\n",
      "Number: 546, Predicted: five hundred and forty-six, Actual: five hundred and forty-six\n",
      "Number: 553, Predicted: five hundred and fifty-three, Actual: five hundred and fifty-three\n",
      "Number: 555, Predicted: five hundred and fifty-five, Actual: five hundred and fifty-five\n",
      "Number: 556, Predicted: five hundred and fifty-six, Actual: five hundred and fifty-six\n",
      "Number: 560, Predicted: five hundred and sixty, Actual: five hundred and sixty\n",
      "Number: 561, Predicted: five hundred and sixty-one, Actual: five hundred and sixty-one\n",
      "Number: 562, Predicted: five hundred and sixty-two, Actual: five hundred and sixty-two\n",
      "Number: 563, Predicted: five hundred and sixty-three, Actual: five hundred and sixty-three\n",
      "Number: 564, Predicted: five hundred and sixty-four, Actual: five hundred and sixty-four\n",
      "Number: 565, Predicted: five hundred and sixty-five, Actual: five hundred and sixty-five\n",
      "Number: 566, Predicted: five hundred and sixty-six, Actual: five hundred and sixty-six\n",
      "Number: 573, Predicted: five hundred and seventy-three, Actual: five hundred and seventy-three\n",
      "Number: 574, Predicted: five hundred and seventy-four, Actual: five hundred and seventy-four\n",
      "Number: 577, Predicted: five hundred and seventy-seven, Actual: five hundred and seventy-seven\n",
      "Number: 592, Predicted: five hundred and ninety-two, Actual: five hundred and ninety-two\n",
      "Number: 600, Predicted: six hundred, Actual: six hundred\n",
      "Number: 612, Predicted: six hundred and twelve, Actual: six hundred and twelve\n",
      "Number: 614, Predicted: six hundred and fourteen, Actual: six hundred and fourteen\n",
      "Number: 642, Predicted: six hundred and forty-two, Actual: six hundred and forty-two\n",
      "Number: 646, Predicted: six hundred and forty-six, Actual: six hundred and forty-six\n",
      "Number: 647, Predicted: six hundred and forty-seven, Actual: six hundred and forty-seven\n",
      "Number: 654, Predicted: six hundred and fifty-four, Actual: six hundred and fifty-four\n",
      "Number: 661, Predicted: six hundred and sixty-one, Actual: six hundred and sixty-one\n",
      "Number: 663, Predicted: six hundred and sixty-three, Actual: six hundred and sixty-three\n",
      "Number: 674, Predicted: six hundred and seventy-four, Actual: six hundred and seventy-four\n",
      "Number: 681, Predicted: six hundred and eighty-one, Actual: six hundred and eighty-one\n",
      "Number: 683, Predicted: six hundred and eighty-three, Actual: six hundred and eighty-three\n",
      "Number: 686, Predicted: six hundred and eighty-six, Actual: six hundred and eighty-six\n",
      "Number: 690, Predicted: six hundred and ninety, Actual: six hundred and ninety\n",
      "Number: 698, Predicted: six hundred and ninety-eight, Actual: six hundred and ninety-eight\n",
      "Number: 699, Predicted: six hundred and ninety-nine, Actual: six hundred and ninety-nine\n",
      "Number: 700, Predicted: seven hundred, Actual: seven hundred\n",
      "Number: 701, Predicted: seven hundred and one, Actual: seven hundred and one\n",
      "Number: 702, Predicted: seven hundred and two, Actual: seven hundred and two\n",
      "Number: 719, Predicted: seven hundred and nineteen, Actual: seven hundred and nineteen\n",
      "Number: 724, Predicted: seven hundred and twenty-four, Actual: seven hundred and twenty-four\n",
      "Number: 725, Predicted: seven hundred and twenty-five, Actual: seven hundred and twenty-five\n",
      "Number: 726, Predicted: seven hundred and twenty-six, Actual: seven hundred and twenty-six\n",
      "Number: 727, Predicted: seven hundred and twenty-seven, Actual: seven hundred and twenty-seven\n",
      "Number: 729, Predicted: seven hundred and twenty-nine, Actual: seven hundred and twenty-nine\n",
      "Number: 733, Predicted: seven hundred and thirty-three, Actual: seven hundred and thirty-three\n",
      "Number: 738, Predicted: seven hundred and thirty-eight, Actual: seven hundred and thirty-eight\n",
      "Number: 742, Predicted: seven hundred and forty-two, Actual: seven hundred and forty-two\n",
      "Number: 747, Predicted: seven hundred and forty-seven, Actual: seven hundred and forty-seven\n",
      "Number: 748, Predicted: seven hundred and forty-eight, Actual: seven hundred and forty-eight\n",
      "Number: 763, Predicted: seven hundred and sixty-three, Actual: seven hundred and sixty-three\n",
      "Number: 766, Predicted: seven hundred and sixty-six, Actual: seven hundred and sixty-six\n",
      "Number: 768, Predicted: seven hundred and sixty-eight, Actual: seven hundred and sixty-eight\n",
      "Number: 769, Predicted: seven hundred and sixty-nine, Actual: seven hundred and sixty-nine\n",
      "Number: 771, Predicted: seven hundred and seventy-one, Actual: seven hundred and seventy-one\n",
      "Number: 772, Predicted: seven hundred and seventy-two, Actual: seven hundred and seventy-two\n",
      "Number: 775, Predicted: seven hundred and seventy-five, Actual: seven hundred and seventy-five\n",
      "Number: 776, Predicted: seven hundred and seventy-six, Actual: seven hundred and seventy-six\n",
      "Number: 779, Predicted: seven hundred and seventy-nine, Actual: seven hundred and seventy-nine\n",
      "Number: 782, Predicted: seven hundred and eighty-two, Actual: seven hundred and eighty-two\n",
      "Number: 791, Predicted: seven hundred and ninety-one, Actual: seven hundred and ninety-one\n",
      "Number: 794, Predicted: seven hundred and ninety-four, Actual: seven hundred and ninety-four\n",
      "Number: 795, Predicted: seven hundred and ninety-five, Actual: seven hundred and ninety-five\n",
      "Number: 804, Predicted: eight hundred and four, Actual: eight hundred and four\n",
      "Number: 805, Predicted: eight hundred and five, Actual: eight hundred and five\n",
      "Number: 812, Predicted: eight hundred and twelve, Actual: eight hundred and twelve\n",
      "Number: 815, Predicted: eight hundred and fifteen, Actual: eight hundred and fifteen\n",
      "Number: 818, Predicted: eight hundred and eighteen, Actual: eight hundred and eighteen\n",
      "Number: 821, Predicted: eight hundred and twenty-one, Actual: eight hundred and twenty-one\n",
      "Number: 831, Predicted: eight hundred and thirty-one, Actual: eight hundred and thirty-one\n",
      "Number: 838, Predicted: eight hundred and thirty-eight, Actual: eight hundred and thirty-eight\n",
      "Number: 839, Predicted: eight hundred and thirty-nine, Actual: eight hundred and thirty-nine\n",
      "Number: 840, Predicted: eight hundred and forty, Actual: eight hundred and forty\n",
      "Number: 847, Predicted: eight hundred and forty-seven, Actual: eight hundred and forty-seven\n",
      "Number: 848, Predicted: eight hundred and forty-eight, Actual: eight hundred and forty-eight\n",
      "Number: 856, Predicted: eight hundred and fifty-six, Actual: eight hundred and fifty-six\n",
      "Number: 857, Predicted: eight hundred and fifty-seven, Actual: eight hundred and fifty-seven\n",
      "Number: 860, Predicted: eight hundred and sixty, Actual: eight hundred and sixty\n",
      "Number: 862, Predicted: eight hundred and sixty-two, Actual: eight hundred and sixty-two\n",
      "Number: 871, Predicted: eight hundred and seventy-one, Actual: eight hundred and seventy-one\n",
      "Number: 875, Predicted: eight hundred and seventy-five, Actual: eight hundred and seventy-five\n",
      "Number: 878, Predicted: eight hundred and seventy-eight, Actual: eight hundred and seventy-eight\n",
      "Number: 880, Predicted: eight hundred and eighty, Actual: eight hundred and eighty\n",
      "Number: 888, Predicted: eight hundred and eighty-eight, Actual: eight hundred and eighty-eight\n",
      "Number: 890, Predicted: eight hundred and ninety, Actual: eight hundred and ninety\n",
      "Number: 897, Predicted: eight hundred and ninety-seven, Actual: eight hundred and ninety-seven\n",
      "Number: 929, Predicted: nine hundred and twenty-nine, Actual: nine hundred and twenty-nine\n",
      "Number: 937, Predicted: nine hundred and thirty-seven, Actual: nine hundred and thirty-seven\n",
      "Number: 944, Predicted: nine hundred and forty-four, Actual: nine hundred and forty-four\n",
      "Number: 955, Predicted: nine hundred and fifty-five, Actual: nine hundred and fifty-five\n",
      "Number: 957, Predicted: nine hundred and fifty-seven, Actual: nine hundred and fifty-seven\n",
      "Number: 962, Predicted: nine hundred and sixty-two, Actual: nine hundred and sixty-two\n",
      "Number: 976, Predicted: nine hundred and seventy-six, Actual: nine hundred and seventy-six\n",
      "Number: 980, Predicted: nine hundred and eighty, Actual: nine hundred and eighty\n",
      "Number: 982, Predicted: nine hundred and eighty-two, Actual: nine hundred and eighty-two\n",
      "Number: 992, Predicted: nine hundred and ninety-two, Actual: nine hundred and ninety-two\n",
      "Number: 996, Predicted: nine hundred and ninety-six, Actual: nine hundred and ninety-six\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.985"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(custom_model, val_df, tokenize_numeric, sp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
