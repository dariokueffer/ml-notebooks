{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook implements the Transformer architecture as described in the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762). The model is trained to convert numbers from their numerical representation to their textual representation. The purpose of this notebook is educational, and it covers the following concepts and topics:\n",
    "- Training a tokenizer\n",
    "- Original Transformer architecture\n",
    "- Positional encoding\n",
    "- Attention mechanism\n",
    "- Different decoding methods: greedy, beam search, top-k, top-p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation\n",
    "\n",
    "The following method creates a data set of numbers with their numerical and textual representation. It creates a training and a test split. It creates the data for all numbers between start and end, the splits are generated randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from -r ../requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: num2words in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from -r ../requirements.txt (line 2)) (0.5.14)\n",
      "Requirement already satisfied: sentencepiece in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from -r ../requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from -r ../requirements.txt (line 4)) (2.6.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from pandas->-r ../requirements.txt (line 1)) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from pandas->-r ../requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from pandas->-r ../requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from pandas->-r ../requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from num2words->-r ../requirements.txt (line 2)) (0.6.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from torch->-r ../requirements.txt (line 4)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from torch->-r ../requirements.txt (line 4)) (4.13.1)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from torch->-r ../requirements.txt (line 4)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from torch->-r ../requirements.txt (line 4)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from torch->-r ../requirements.txt (line 4)) (2025.3.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from torch->-r ../requirements.txt (line 4)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from sympy==1.13.1->torch->-r ../requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->-r ../requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages (from jinja2->torch->-r ../requirements.txt (line 4)) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from num2words import num2words\n",
    "\n",
    "def create_dataset(start=0, end=1000, train_size=0.8):\n",
    "   numbers = list(range(start, end))\n",
    "   data = [(str(n), num2words(n)) for n in numbers]\n",
    "   df = pd.DataFrame(data, columns=['num', 'word'])\n",
    "   \n",
    "   train_df = df.sample(frac=train_size, random_state=42)\n",
    "   test_df = df.drop(train_df.index)\n",
    "   \n",
    "   return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in val_df:  200\n",
      "Items in train_df:  800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>521</td>\n",
       "      <td>five hundred and twenty-one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>737</td>\n",
       "      <td>seven hundred and thirty-seven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>740</td>\n",
       "      <td>seven hundred and forty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>660</td>\n",
       "      <td>six hundred and sixty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>411</td>\n",
       "      <td>four hundred and eleven</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     num                            word\n",
       "521  521     five hundred and twenty-one\n",
       "737  737  seven hundred and thirty-seven\n",
       "740  740         seven hundred and forty\n",
       "660  660           six hundred and sixty\n",
       "411  411         four hundred and eleven"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = create_dataset(0, 1000, 0.8)\n",
    "print('Items in val_df: ', len(val_df))\n",
    "print('Items in train_df: ', len(train_df))\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Tokenizer\n",
    "We use different tokenizer for input and output, which is different from the original Vaswani paper. But since the input consists only of digits, we can use the digits directly as tokens and just add a special token for BOS, EOS and PAD respectively. For the output we train a SentencePiece tokenizer using Byte Pair Encoding on the whole input vocabulary. \n",
    "\n",
    "The special tokens are assigned as follows:\n",
    "\n",
    "- INPUT_BOS_TOKEN_ID = 10\n",
    "- INPUT_EOS_TOKEN_ID = 11\n",
    "- INPUT_PAD_TOKEN_ID = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_BOS_TOKEN_ID = 10\n",
    "INPUT_EOS_TOKEN_ID = 11\n",
    "INPUT_PAD_TOKEN_ID = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: tokenizer/num_to_words\n",
      "  model_type: BPE\n",
      "  vocab_size: 200\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: <PAD>\n",
      "  user_defined_symbols: <BOS>\n",
      "  user_defined_symbols: <EOS>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 800 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <PAD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <BOS>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <EOS>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=20504\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=21\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 800 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 800\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 102\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1442 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=238 size=20 all=108 active=87 piece=ei\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=90 size=40 all=130 active=109 piece=▁fif\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=60 all=118 active=97 piece=eight\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=80 all=114 active=93 piece=▁thirteen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=100 all=98 active=77 piece=le\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=0 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=120 all=78 active=57 piece=and\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=140 all=58 active=37 piece=▁ei\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=160 all=38 active=17 piece=ninet\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: tokenizer/num_to_words.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: tokenizer/num_to_words.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import io\n",
    "\n",
    "text_data = \"\\n\".join(train_df['word'])\n",
    "text_stream = io.StringIO(text_data)\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    sentence_iterator=text_stream,\n",
    "    model_prefix='tokenizer/num_to_words',\n",
    "    vocab_size=200,\n",
    "    model_type='bpe',\n",
    "    character_coverage=1.0,\n",
    "    user_defined_symbols=['<PAD>', '<BOS>', '<EOS>']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: one hundred and twenty-five\n",
      "Tokens: ['▁one', '▁hundred', '▁and', '▁twenty', '-', 'five']\n",
      "IDs: [47, 12, 14, 51, 190, 64]\n",
      "Decoded: one hundred and twenty-five\n"
     ]
    }
   ],
   "source": [
    "# Sample usage of the tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('tokenizer/num_to_words.model')\n",
    "\n",
    "text = \"one hundred and twenty-five\"\n",
    "ids = sp.encode(text, out_type=int)\n",
    "tokens = sp.encode(text, out_type=str)\n",
    "decoded = sp.decode(ids)\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"IDs:\", ids)\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT_BOS_TOKEN_ID: 4\n",
      "OUTPUT_EOS_TOKEN_ID: 5\n",
      "OUTPUT_PAD_TOKEN_ID: 3\n"
     ]
    }
   ],
   "source": [
    "# get ids of special tokens\n",
    "OUTPUT_BOS_TOKEN_ID = sp.piece_to_id('<BOS>')\n",
    "OUTPUT_EOS_TOKEN_ID = sp.piece_to_id('<EOS>')\n",
    "OUTPUT_PAD_TOKEN_ID = sp.piece_to_id('<PAD>')\n",
    "print('OUTPUT_BOS_TOKEN_ID:', OUTPUT_BOS_TOKEN_ID)\n",
    "print('OUTPUT_EOS_TOKEN_ID:', OUTPUT_EOS_TOKEN_ID)\n",
    "print('OUTPUT_PAD_TOKEN_ID:', OUTPUT_PAD_TOKEN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def tokenize_numeric_representation(df_column):\n",
    "    tokenized_sequences = [\n",
    "        [INPUT_BOS_TOKEN_ID] + [int(d) for d in str(num)] + [INPUT_EOS_TOKEN_ID] for num in df_column\n",
    "    ]\n",
    "\n",
    "    # Ensure consistent padding length across input and target\n",
    "    max_length = max(len(seq) for seq in tokenized_sequences)\n",
    "    \n",
    "    padded_sequences = [\n",
    "        seq + [INPUT_PAD_TOKEN_ID] * (max_length - len(seq)) for seq in tokenized_sequences\n",
    "    ]\n",
    "\n",
    "    tensor_sequences = torch.tensor(padded_sequences, dtype=torch.long)\n",
    "    return tensor_sequences\n",
    "\n",
    "def decode_numeric_representation(sequence):\n",
    "   seq_list = sequence.tolist()\n",
    "   tokens = [\n",
    "       '<BOS>' if x == INPUT_BOS_TOKEN_ID else\n",
    "       '<EOS>' if x == INPUT_EOS_TOKEN_ID else\n",
    "       '<PAD>' if x == INPUT_PAD_TOKEN_ID else\n",
    "       str(x) \n",
    "       for x in seq_list\n",
    "   ]\n",
    "   return \"\".join(tokens)\n",
    "\n",
    "def tokenize_textual_representation(df_column):\n",
    "    sp = spm.SentencePieceProcessor(model_file='tokenizer/num_to_words.model')\n",
    "\n",
    "    tokenized_words = df_column.apply(lambda x: [OUTPUT_BOS_TOKEN_ID] + sp.encode(x, out_type=int) + [OUTPUT_EOS_TOKEN_ID])\n",
    "    max_length = tokenized_words.apply(len).max()\n",
    "\n",
    "    padded_tokens = tokenized_words.apply(lambda x: x + [OUTPUT_PAD_TOKEN_ID] * (max_length - len(x)))\n",
    "\n",
    "    tensor_data = torch.tensor(padded_tokens.tolist(), dtype=torch.long)\n",
    "    return tensor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NumberWordDataset(Dataset):\n",
    "    def __init__(self, input_tensor, target_tensor, pad_token_id=3):\n",
    "        self.input_tensor = input_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.input_tensor[idx]\n",
    "        target_seq = self.target_tensor[idx]\n",
    "\n",
    "        # Create attention masks,\n",
    "        # True for padding tokens, \n",
    "        # False for actual tokens\n",
    "        input_mask = (input_seq == INPUT_PAD_TOKEN_ID).long()\n",
    "        target_mask = (target_seq == self.pad_token_id).long()\n",
    "        \n",
    "        return {\n",
    "            'input_seq': input_seq,\n",
    "            'target_seq': target_seq,\n",
    "            'input_mask': input_mask.bool(),\n",
    "            'target_mask': target_mask.bool() \n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the train_df data\n",
    "tokenized_textual = tokenize_textual_representation(train_df['word'])\n",
    "tokenized_numeric = tokenize_numeric_representation(train_df['num'])\n",
    "\n",
    "# Create the dataset from tokenized data\n",
    "dataset = NumberWordDataset(tokenized_numeric, tokenized_textual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item: 1\n",
      "tensor([10,  9,  7, 11, 12])\n",
      "Decoded Input: <BOS>97<EOS><PAD>\n",
      "Input: tensor([10,  9,  7, 11, 12])\n",
      "Input Mask: tensor([False, False, False, False,  True])\n",
      "Decoded Target: <BOS> ninety-seven<EOS><PAD><PAD><PAD>\n",
      "Target: tensor([  4,  56, 190,  67,   5,   3,   3,   3])\n",
      "Target Mask: tensor([False, False, False, False, False,  True,  True,  True])\n",
      "\n",
      "Item: 2\n",
      "tensor([10,  4,  2,  1, 11])\n",
      "Decoded Input: <BOS>421<EOS>\n",
      "Input: tensor([10,  4,  2,  1, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> four hundred and twenty-one<EOS>\n",
      "Target: tensor([  4,  43,  12,  14,  51, 190,  41,   5])\n",
      "Target Mask: tensor([False, False, False, False, False, False, False, False])\n",
      "\n",
      "Item: 3\n",
      "tensor([10,  2,  0,  4, 11])\n",
      "Decoded Input: <BOS>204<EOS>\n",
      "Input: tensor([10,  2,  0,  4, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> two hundred and four<EOS><PAD><PAD>\n",
      "Target: tensor([ 4, 48, 12, 14, 43,  5,  3,  3])\n",
      "Target Mask: tensor([False, False, False, False, False, False,  True,  True])\n",
      "\n",
      "Item: 4\n",
      "tensor([10,  6,  9,  4, 11])\n",
      "Decoded Input: <BOS>694<EOS>\n",
      "Input: tensor([10,  6,  9,  4, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> six hundred and ninety-four<EOS>\n",
      "Target: tensor([  4,  33,  12,  14,  56, 190,  62,   5])\n",
      "Target Mask: tensor([False, False, False, False, False, False, False, False])\n",
      "\n",
      "Item: 5\n",
      "tensor([10,  4,  1, 11, 12])\n",
      "Decoded Input: <BOS>41<EOS><PAD>\n",
      "Input: tensor([10,  4,  1, 11, 12])\n",
      "Input Mask: tensor([False, False, False, False,  True])\n",
      "Decoded Target: <BOS> forty-one<EOS><PAD><PAD><PAD>\n",
      "Target: tensor([  4,  55, 190,  41,   5,   3,   3,   3])\n",
      "Target Mask: tensor([False, False, False, False, False,  True,  True,  True])\n",
      "\n",
      "Item: 6\n",
      "tensor([10,  1,  7,  7, 11])\n",
      "Decoded Input: <BOS>177<EOS>\n",
      "Input: tensor([10,  1,  7,  7, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> one hundred and seventy-seven<EOS>\n",
      "Target: tensor([  4,  47,  12,  14,  57, 190,  67,   5])\n",
      "Target Mask: tensor([False, False, False, False, False, False, False, False])\n",
      "\n",
      "Item: 7\n",
      "tensor([10,  2,  1,  0, 11])\n",
      "Decoded Input: <BOS>210<EOS>\n",
      "Input: tensor([10,  2,  1,  0, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> two hundred and ten<EOS><PAD><PAD>\n",
      "Target: tensor([ 4, 48, 12, 14, 75,  5,  3,  3])\n",
      "Target Mask: tensor([False, False, False, False, False, False,  True,  True])\n",
      "\n",
      "Item: 8\n",
      "tensor([10,  6,  6,  6, 11])\n",
      "Decoded Input: <BOS>666<EOS>\n",
      "Input: tensor([10,  6,  6,  6, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> six hundred and sixty-six<EOS>\n",
      "Target: tensor([  4,  33,  12,  14,  58, 190,  66,   5])\n",
      "Target Mask: tensor([False, False, False, False, False, False, False, False])\n",
      "\n",
      "Item: 9\n",
      "tensor([10,  6,  0, 11, 12])\n",
      "Decoded Input: <BOS>60<EOS><PAD>\n",
      "Input: tensor([10,  6,  0, 11, 12])\n",
      "Input Mask: tensor([False, False, False, False,  True])\n",
      "Decoded Target: <BOS> sixty<EOS><PAD><PAD><PAD><PAD><PAD>\n",
      "Target: tensor([ 4, 58,  5,  3,  3,  3,  3,  3])\n",
      "Target Mask: tensor([False, False, False,  True,  True,  True,  True,  True])\n",
      "\n",
      "Item: 10\n",
      "tensor([10,  3,  4,  6, 11])\n",
      "Decoded Input: <BOS>346<EOS>\n",
      "Input: tensor([10,  3,  4,  6, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> three hundred and forty-six<EOS>\n",
      "Target: tensor([  4,  44,  12,  14,  55, 190,  66,   5])\n",
      "Target Mask: tensor([False, False, False, False, False, False, False, False])\n",
      "\n",
      "Item: 11\n",
      "tensor([10,  2,  6,  0, 11])\n",
      "Decoded Input: <BOS>260<EOS>\n",
      "Input: tensor([10,  2,  6,  0, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> two hundred and sixty<EOS><PAD><PAD>\n",
      "Target: tensor([ 4, 48, 12, 14, 58,  5,  3,  3])\n",
      "Target Mask: tensor([False, False, False, False, False, False,  True,  True])\n",
      "\n",
      "Item: 12\n",
      "tensor([10,  8,  8,  5, 11])\n",
      "Decoded Input: <BOS>885<EOS>\n",
      "Input: tensor([10,  8,  8,  5, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> eight hundred and eighty-five<EOS>\n",
      "Target: tensor([  4,  42,  12,  14,  54, 190,  64,   5])\n",
      "Target Mask: tensor([False, False, False, False, False, False, False, False])\n",
      "\n",
      "Item: 13\n",
      "tensor([10,  9,  4,  8, 11])\n",
      "Decoded Input: <BOS>948<EOS>\n",
      "Input: tensor([10,  9,  4,  8, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> nine hundred and forty-eight<EOS>\n",
      "Target: tensor([  4,  29,  12,  14,  55, 190,  65,   5])\n",
      "Target Mask: tensor([False, False, False, False, False, False, False, False])\n",
      "\n",
      "Item: 14\n",
      "tensor([10,  8,  5,  4, 11])\n",
      "Decoded Input: <BOS>854<EOS>\n",
      "Input: tensor([10,  8,  5,  4, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> eight hundred and fifty-four<EOS>\n",
      "Target: tensor([  4,  42,  12,  14,  53, 190,  62,   5])\n",
      "Target Mask: tensor([False, False, False, False, False, False, False, False])\n",
      "\n",
      "Item: 15\n",
      "tensor([10,  6,  6,  9, 11])\n",
      "Decoded Input: <BOS>669<EOS>\n",
      "Input: tensor([10,  6,  6,  9, 11])\n",
      "Input Mask: tensor([False, False, False, False, False])\n",
      "Decoded Target: <BOS> six hundred and sixty-nine<EOS>\n",
      "Target: tensor([  4,  33,  12,  14,  58, 190,  22,   5])\n",
      "Target Mask: tensor([False, False, False, False, False, False, False, False])\n",
      "\n",
      "Item: 16\n",
      "tensor([10,  8,  1, 11, 12])\n",
      "Decoded Input: <BOS>81<EOS><PAD>\n",
      "Input: tensor([10,  8,  1, 11, 12])\n",
      "Input Mask: tensor([False, False, False, False,  True])\n",
      "Decoded Target: <BOS> eighty-one<EOS><PAD><PAD><PAD>\n",
      "Target: tensor([  4,  54, 190,  41,   5,   3,   3,   3])\n",
      "Target Mask: tensor([False, False, False, False, False,  True,  True,  True])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample usage of the dataset and dataloader exploration\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "sp = spm.SentencePieceProcessor(model_file='tokenizer/num_to_words.model')\n",
    "\n",
    "for batch in dataloader:\n",
    "    input_seq = batch['input_seq']\n",
    "    target_seq = batch['target_seq']\n",
    "    input_mask = batch['input_mask']\n",
    "    target_mask = batch['target_mask']\n",
    "\n",
    "    for i in range(input_seq.size(0)):\n",
    "        decoded_input = decode_numeric_representation(input_seq[i])\n",
    "        print('Item:', i + 1)\n",
    "        print(input_seq[i])\n",
    "        print(\"Decoded Input:\", decoded_input)\n",
    "        print(\"Input:\", input_seq[i])\n",
    "        print(\"Input Mask:\", input_mask[i])\n",
    "        decoded_target = sp.decode_ids(target_seq[i].tolist())\n",
    "        print(\"Decoded Target:\", decoded_target)\n",
    "        print(\"Target:\", target_seq[i])\n",
    "        print(\"Target Mask:\", target_mask[i])\n",
    "        print()\n",
    "\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Models\n",
    "\n",
    "In the following two Transformer models are created, one using the PyTorch modules and the other using a custom implementation. Both models use a custom positional encoding that encodes the position using trigonometric functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_vocab_size, target_vocab_size, d_model=512, nhead=8, num_layers=6,\n",
    "                 d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.pos_decoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Use Pre-LN\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, target_vocab_size)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        src_emb = self.pos_encoder(self.encoder_embedding(src))\n",
    "        tgt_emb = self.pos_decoder(self.decoder_embedding(tgt))\n",
    "\n",
    "        causal_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(src.device)\n",
    "\n",
    "        output = self.transformer(\n",
    "            src=src_emb,\n",
    "            tgt=tgt_emb,\n",
    "            tgt_mask=causal_mask,\n",
    "            src_key_padding_mask=src_mask,\n",
    "            tgt_key_padding_mask=tgt_mask,\n",
    "            memory_key_padding_mask=src_mask\n",
    "        )\n",
    "        return self.fc_out(output)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoderLayer\n",
    "import math\n",
    "\n",
    "class CustomMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
    "        batch_size, query_len, _ = query.shape\n",
    "        _, key_len, _ = key.shape\n",
    "\n",
    "        Q = self.q_linear(query).view(batch_size, query_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.k_linear(key).view(batch_size, key_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.v_linear(value).view(batch_size, key_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        scores = scores.clamp(min=-10, max=10)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dim() == 2:\n",
    "                attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)\n",
    "            elif attn_mask.dim() == 3:\n",
    "                attn_mask = attn_mask.unsqueeze(1)\n",
    "            \n",
    "            scores = scores + attn_mask.to(scores.device)\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(key_padding_mask, -1e9)\n",
    "\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, query_len, self.d_model)\n",
    "\n",
    "        return self.out_linear(attn_output)\n",
    "\n",
    "class CustomTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = CustomMultiHeadAttention(d_model, nhead, dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask=None):\n",
    "        src2 = self.norm1(src)\n",
    "        src2 = self.self_attn(src2, src2, src2, key_padding_mask=src_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        \n",
    "        src2 = self.norm2(src)\n",
    "        src2 = self.linear2(self.dropout(F.relu(self.linear1(src2))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        \n",
    "        return src\n",
    "\n",
    "class CustomTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            CustomTransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask=mask)\n",
    "        return self.norm(src)\n",
    "\n",
    "class CustomTransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = CustomMultiHeadAttention(d_model, nhead, dropout)\n",
    "        self.multihead_attn = CustomMultiHeadAttention(d_model, nhead, dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        tgt2 = self.norm1(tgt)\n",
    "        tgt2 = self.self_attn(tgt2, tgt2, tgt2, attn_mask=tgt_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        \n",
    "        tgt2 = self.norm2(tgt)\n",
    "        tgt2 = self.multihead_attn(tgt2, memory, memory, key_padding_mask=memory_key_padding_mask)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        \n",
    "        tgt2 = self.norm3(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(F.relu(self.linear1(tgt2))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        \n",
    "        return tgt\n",
    "\n",
    "class CustomTransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            CustomTransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory, tgt_mask=tgt_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return self.norm(tgt)\n",
    "        \n",
    "\n",
    "class CustomTransformerModel(nn.Module):\n",
    "    def __init__(self, input_vocab_size, target_vocab_size, d_model=512, nhead=8, num_layers=6, \n",
    "                 dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.pos_decoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        self.encoder = CustomTransformerEncoder(d_model, nhead, num_layers, dim_feedforward, dropout)\n",
    "        self.decoder = CustomTransformerDecoder(d_model, nhead, num_layers, dim_feedforward, dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        src_emb = self.pos_encoder(self.encoder_embedding(src))\n",
    "        tgt_emb = self.pos_decoder(self.decoder_embedding(tgt))\n",
    "        \n",
    "        memory = self.encoder(src_emb, src_mask)\n",
    "\n",
    "        tgt_seq_len = tgt.size(1)\n",
    "        causal_mask = self.generate_square_subsequent_mask(tgt_seq_len)\n",
    "\n",
    "        output = self.decoder(\n",
    "            tgt_emb,\n",
    "            memory,\n",
    "            tgt_mask=causal_mask,\n",
    "            tgt_key_padding_mask=tgt_mask,\n",
    "            memory_key_padding_mask=src_mask\n",
    "        )\n",
    "                \n",
    "        return self.fc_out(output)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, num_epochs=20, learning_rate=1e-5):\n",
    "    # device = torch.device(\"cpu\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=OUTPUT_PAD_TOKEN_ID)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            src = batch['input_seq'].to(device)\n",
    "            tgt = batch['target_seq'].to(device)\n",
    "\n",
    "            src_mask = batch['input_mask'].to(device)\n",
    "            tgt_mask = batch['target_mask'].to(device)\n",
    "\n",
    "            src_mask = src_mask[:, :src.shape[1]].to(device)\n",
    "            tgt_mask = tgt_mask[:, :tgt.shape[1]].to(device)\n",
    "\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "\n",
    "            tgt_input = tgt_input.to(device)\n",
    "            tgt_output = tgt_output.to(device)\n",
    "\n",
    "\n",
    "            tgt_mask = tgt_mask[:, :-1]\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "\n",
    "            loss = criterion(output, tgt_output)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "            if torch.isnan(loss):\n",
    "                print(f\"NaN loss detected in batch! Skipping batch.\")\n",
    "                continue\n",
    "\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_VOCAB_SIZE = 15\n",
    "TARGET_VOCAB_SIZE = sp.get_piece_size()\n",
    "\n",
    "D_MODEL = 512\n",
    "NHEAD = 8\n",
    "NUM_LAYERS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.5416268479824067\n",
      "Epoch 2/10, Loss: 0.6178751677274704\n",
      "Epoch 3/10, Loss: 0.23993728026747704\n",
      "Epoch 4/10, Loss: 0.1136762547492981\n",
      "Epoch 5/10, Loss: 0.0500045863725245\n",
      "Epoch 6/10, Loss: 0.025248954109847545\n",
      "Epoch 7/10, Loss: 0.023065023603849114\n",
      "Epoch 8/10, Loss: 0.028501707909163086\n",
      "Epoch 9/10, Loss: 0.019775470080785455\n",
      "Epoch 10/10, Loss: 0.009861704148352145\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(INPUT_VOCAB_SIZE, TARGET_VOCAB_SIZE, D_MODEL, NHEAD, NUM_LAYERS)\n",
    "train_model(model, dataloader, num_epochs=10, learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.4889960479736328\n",
      "Epoch 2/10, Loss: 0.6352079486846924\n",
      "Epoch 3/10, Loss: 0.40897645592689513\n",
      "Epoch 4/10, Loss: 0.2331121051311493\n",
      "Epoch 5/10, Loss: 0.12604800805449487\n",
      "Epoch 6/10, Loss: 0.06859868505969643\n",
      "Epoch 7/10, Loss: 0.0452171132247895\n",
      "Epoch 8/10, Loss: 0.03937608480453491\n",
      "Epoch 9/10, Loss: 0.02648100570309907\n",
      "Epoch 10/10, Loss: 0.022407926116138698\n"
     ]
    }
   ],
   "source": [
    "custom_model = CustomTransformerModel(INPUT_VOCAB_SIZE, TARGET_VOCAB_SIZE, D_MODEL, NHEAD, NUM_LAYERS)\n",
    "train_model(custom_model, dataloader, num_epochs=10, learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Sequence Generation\n",
    "\n",
    "The following methods are defined to predict the output sequence: \n",
    "- greedy\n",
    "- beam search\n",
    "- top-k\n",
    "- top-p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'eighteen'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def greedy_decoding(model, number, tokenizer_text, max_length=50):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # tokenize the input number\n",
    "        src = tokenize_numeric_representation(pd.Series([number]))\n",
    "        src_mask = ~torch.ones(src.shape, dtype=torch.bool).to(device)\n",
    "\n",
    "        # initialize the target sequence with the BOS token\n",
    "        tgt = torch.tensor([[OUTPUT_BOS_TOKEN_ID]], dtype=torch.long).to(device)\n",
    "        tgt_mask = ~torch.ones((1, 1), dtype=torch.bool).to(device)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            output = model(src, tgt, src_mask, tgt_mask)\n",
    "            next_token = output[:, -1].argmax(dim=-1).unsqueeze(1)\n",
    "\n",
    "            # check if the next token is the EOS token \n",
    "            # and stop decoding if so\n",
    "            if next_token.item() == OUTPUT_EOS_TOKEN_ID:\n",
    "                break\n",
    "\n",
    "            # append the next token to the target sequence\n",
    "            tgt = torch.cat([tgt, next_token], dim=1)\n",
    "            tgt_mask = ~torch.ones(tgt.shape, dtype=torch.bool)\n",
    "            \n",
    "        output_tokens = tgt[0][1:].tolist()\n",
    "\n",
    "        # if EOS token is present, remove it and everything after it\n",
    "        if OUTPUT_EOS_TOKEN_ID in output_tokens:\n",
    "            output_tokens = output_tokens[:output_tokens.index(OUTPUT_EOS_TOKEN_ID)]\n",
    "        return tokenizer_text.decode(output_tokens)\n",
    "\n",
    "greedy_decoding(model, 18, sp, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'twenty'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def beam_search_decoding(model, number, tokenizer_text, beam_width=3, max_length=50):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        src = tokenize_numeric_representation(pd.Series([number]))\n",
    "        src_mask = ~torch.ones(src.shape, dtype=torch.bool).to(device)\n",
    "\n",
    "        beams = [(torch.tensor([[OUTPUT_BOS_TOKEN_ID]], dtype=torch.long).to(device), 0)]  # (sequence, score)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            new_beams = []\n",
    "\n",
    "            for seq, score in beams:\n",
    "                # check if the sequence has ended with EOS token\n",
    "                if seq[0, -1].item() == OUTPUT_EOS_TOKEN_ID:\n",
    "                    new_beams.append((seq, score))\n",
    "                    continue\n",
    "\n",
    "                output = model(src, seq, src_mask=None, tgt_mask=None)\n",
    "\n",
    "                logits = output[:, -1, :] # take the last token's logits\n",
    "                probs = torch.softmax(logits, dim=-1) # turn logits to probabilities\n",
    "                topk_probs, topk_tokens = torch.topk(probs, beam_width, dim=-1) # get top b_width tokens and their probabilities\n",
    " \n",
    "                for i in range(beam_width):\n",
    "                    next_token = topk_tokens[0, i].unsqueeze(0).unsqueeze(0)\n",
    "                    new_seq = torch.cat([seq, next_token], dim=1)\n",
    "                    new_score = score + torch.log(topk_probs[0, i]).item()  # Convert to scalar\n",
    "                    new_beams.append((new_seq, new_score))\n",
    "\n",
    "            finished_beams = []\n",
    "            unfinished_beams = []\n",
    "\n",
    "            for seq, score in new_beams:\n",
    "                if seq[0, -1].item() == OUTPUT_EOS_TOKEN_ID:\n",
    "                    finished_beams.append((seq, score))\n",
    "                else:\n",
    "                    unfinished_beams.append((seq, score))\n",
    "\n",
    "            unfinished_beams = sorted(unfinished_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "            finished_beams = sorted(finished_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "            beams = unfinished_beams + finished_beams\n",
    "\n",
    "            if not unfinished_beams:\n",
    "                break\n",
    "\n",
    "        if finished_beams:\n",
    "            best_seq = finished_beams[0][0][0].tolist()[1:]\n",
    "        else:\n",
    "            best_seq = beams[0][0][0].tolist()[1:]\n",
    "\n",
    "\n",
    "        if OUTPUT_EOS_TOKEN_ID in best_seq:\n",
    "            best_seq = best_seq[:best_seq.index(OUTPUT_EOS_TOKEN_ID)]\n",
    "        return tokenizer_text.decode(best_seq)\n",
    "\n",
    "\n",
    "beam_search_decoding(custom_model, 20, sp, beam_width=3, max_length=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decoding(model, number, tokenizer_text, beam_width=3, max_length=50):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        src = tokenize_numeric_representation(pd.Series([number]))\n",
    "        src_mask = ~torch.ones(src.shape, dtype=torch.bool).to(device)\n",
    "\n",
    "        beams = [(torch.tensor([[OUTPUT_BOS_TOKEN_ID]], dtype=torch.long).to(device), 0)]  # (sequence, score)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            new_beams = []\n",
    "\n",
    "            all_seq = torch.cat([seq for seq, _ in beams], dim=0) # (beam_width, seq_len)\n",
    "            output = model(src, all_seq, src_mask=None, tgt_mask=None)\n",
    "            for seq, score in beams:\n",
    "                # check if the sequence has ended with EOS token\n",
    "                if seq[0, -1].item() == OUTPUT_EOS_TOKEN_ID:\n",
    "                    new_beams.append((seq, score))\n",
    "                    continue\n",
    "\n",
    "                output = model(src, seq, src_mask=None, tgt_mask=None)\n",
    "\n",
    "                logits = output[:, -1, :] # take the last token's logits\n",
    "                probs = torch.softmax(logits, dim=-1) # turn logits to probabilities\n",
    "                topk_probs, topk_tokens = torch.topk(probs, beam_width, dim=-1) # get top b_width tokens and their probabilities\n",
    " \n",
    "                for i in range(beam_width):\n",
    "                    next_token = topk_tokens[0, i].unsqueeze(0).unsqueeze(0)\n",
    "                    new_seq = torch.cat([seq, next_token], dim=1)\n",
    "                    new_score = score + torch.log(topk_probs[0, i]).item()  # Convert to scalar\n",
    "                    new_beams.append((new_seq, new_score))\n",
    "\n",
    "            finished_beams = []\n",
    "            unfinished_beams = []\n",
    "\n",
    "            for seq, score in new_beams:\n",
    "                if seq[0, -1].item() == OUTPUT_EOS_TOKEN_ID:\n",
    "                    finished_beams.append((seq, score))\n",
    "                else:\n",
    "                    unfinished_beams.append((seq, score))\n",
    "\n",
    "            unfinished_beams = sorted(unfinished_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "            finished_beams = sorted(finished_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "            beams = unfinished_beams + finished_beams\n",
    "\n",
    "            if not unfinished_beams:\n",
    "                break\n",
    "\n",
    "        if finished_beams:\n",
    "            best_seq = finished_beams[0][0][0].tolist()[1:]\n",
    "        else:\n",
    "            best_seq = beams[0][0][0].tolist()[1:]\n",
    "\n",
    "\n",
    "        if OUTPUT_EOS_TOKEN_ID in best_seq:\n",
    "            best_seq = best_seq[:best_seq.index(OUTPUT_EOS_TOKEN_ID)]\n",
    "        return tokenizer_text.decode(best_seq)\n",
    "\n",
    "\n",
    "beam_search_decoding(custom_model, 20, sp, beam_width=3, max_length=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twenty\n",
      "twenty\n"
     ]
    }
   ],
   "source": [
    "def top_k_sampling(model, number, tokenizer_text, k=10, temperature=0.2, max_length=50, p=None):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        src = tokenize_numeric_representation(pd.Series([number])).to(device)\n",
    "        tgt = torch.tensor([[OUTPUT_BOS_TOKEN_ID]], dtype=torch.long).to(device)\n",
    "        src_mask = ~torch.ones(src.shape, dtype=torch.bool).to(device)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            tgt_mask = ~torch.ones(tgt.shape, dtype=torch.bool).to(device)\n",
    "            output = model(src, tgt, src_mask, tgt_mask)\n",
    "            logits = output[:, -1, :]\n",
    "            \n",
    "            # apply temperature scaling\n",
    "            scaled_logits = logits / temperature\n",
    "            \n",
    "            def top_k(scaled_logits, k):\n",
    "                # apply top-k filtering\n",
    "                top_k_probs, top_k_tokens = torch.topk(scaled_logits, k, dim=-1)\n",
    "                probabilities = torch.softmax(top_k_probs, dim=-1)\n",
    "                return probabilities, top_k_tokens\n",
    "\n",
    "\n",
    "            def top_p(scaled_logits, p):\n",
    "                # top p\n",
    "                probs = torch.softmax(scaled_logits, dim=-1)\n",
    "                sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "\n",
    "                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "                cutoff = cumulative_probs > p\n",
    "                cutoff[..., 1:] = cutoff[..., :-1].clone()\n",
    "                cutoff[..., 0] = False\n",
    "\n",
    "                sorted_probs = sorted_probs.masked_fill(cutoff, 0.0)\n",
    "                return sorted_probs, sorted_indices\n",
    "            \n",
    "            if p:\n",
    "                probabilities, items = top_p(scaled_logits, p)\n",
    "            elif k:\n",
    "                probabilities, items = top_k(scaled_logits, k)\n",
    "            else:\n",
    "                raise ValueError(\"Either k or p must be provided\")\n",
    "\n",
    "            next_token = items[0, torch.multinomial(probabilities[0], 1)].unsqueeze(0)\n",
    "\n",
    "            if next_token.item() == OUTPUT_EOS_TOKEN_ID:\n",
    "                break\n",
    "\n",
    "            tgt = torch.cat([tgt, next_token], dim=1)\n",
    "        \n",
    "        output_tokens = tgt[0][1:].tolist()\n",
    "        return tokenizer_text.decode(output_tokens)\n",
    "\n",
    "\n",
    "print(top_k_sampling(custom_model, 20, sp, k=5, max_length=50))\n",
    "print(top_k_sampling(custom_model, 20, sp, p=0.5, temperature=0.5, max_length=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, test_df, tokenizer_num, tokenizer_text):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for _, row in test_df.iterrows():\n",
    "        number = row['num']\n",
    "        word = row['word']\n",
    "        \n",
    "        # predicted_word = greedy_decoding(model, number, sp)\n",
    "        # predicted_word = beam_search_decoding(model, number, sp, beam_width=3, max_length=10)\n",
    "        predicted_word = top_k_sampling(model, number, sp, k=5, max_length=50)\n",
    "        print(f\"Number: {number}, Predicted: {predicted_word}, Actual: {word}\")\n",
    "        if predicted_word == word:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: 1, Predicted: zero, Actual: one\n",
      "Number: 4, Predicted: forty-four, Actual: four\n",
      "Number: 13, Predicted: thirty-three, Actual: thirteen\n",
      "Number: 14, Predicted: forty-four, Actual: fourteen\n",
      "Number: 20, Predicted: twenty, Actual: twenty\n",
      "Number: 21, Predicted: twenty-one, Actual: twenty-one\n",
      "Number: 27, Predicted: twenty-seven, Actual: twenty-seven\n",
      "Number: 32, Predicted: thirty-two, Actual: thirty-two\n",
      "Number: 34, Predicted: thirty-four, Actual: thirty-four\n",
      "Number: 35, Predicted: thirty-five, Actual: thirty-five\n",
      "Number: 40, Predicted: forty, Actual: forty\n",
      "Number: 47, Predicted: forty-seven, Actual: forty-seven\n",
      "Number: 52, Predicted: fifty-two, Actual: fifty-two\n",
      "Number: 58, Predicted: fifty-eight, Actual: fifty-eight\n",
      "Number: 62, Predicted: sixty-two, Actual: sixty-two\n",
      "Number: 64, Predicted: sixty-four, Actual: sixty-four\n",
      "Number: 71, Predicted: seventy-one, Actual: seventy-one\n",
      "Number: 80, Predicted: eighty, Actual: eighty\n",
      "Number: 85, Predicted: eighty-five, Actual: eighty-five\n",
      "Number: 87, Predicted: eighty-seven, Actual: eighty-seven\n",
      "Number: 91, Predicted: nine, Actual: ninety-one\n",
      "Number: 95, Predicted: ninety-five, Actual: ninety-five\n",
      "Number: 98, Predicted: ninety-eight, Actual: ninety-eight\n",
      "Number: 99, Predicted: nine, Actual: ninety-nine\n",
      "Number: 102, Predicted: one hundred and two, Actual: one hundred and two\n",
      "Number: 105, Predicted: one hundred and five, Actual: one hundred and five\n",
      "Number: 106, Predicted: one hundred and six, Actual: one hundred and six\n",
      "Number: 121, Predicted: one hundred and twenty-one, Actual: one hundred and twenty-one\n",
      "Number: 128, Predicted: one hundred and twenty-eight, Actual: one hundred and twenty-eight\n",
      "Number: 130, Predicted: one hundred and thirty, Actual: one hundred and thirty\n",
      "Number: 134, Predicted: one hundred and thirty-four, Actual: one hundred and thirty-four\n",
      "Number: 138, Predicted: one hundred and thirty-eight, Actual: one hundred and thirty-eight\n",
      "Number: 156, Predicted: one hundred and fifty-six, Actual: one hundred and fifty-six\n",
      "Number: 159, Predicted: one hundred and fifty-nine, Actual: one hundred and fifty-nine\n",
      "Number: 160, Predicted: one hundred and sixty, Actual: one hundred and sixty\n",
      "Number: 161, Predicted: one hundred and sixty-one, Actual: one hundred and sixty-one\n",
      "Number: 166, Predicted: one hundred and sixty-six, Actual: one hundred and sixty-six\n",
      "Number: 170, Predicted: one hundred and seventy, Actual: one hundred and seventy\n",
      "Number: 187, Predicted: one hundred and eighty-seven, Actual: one hundred and eighty-seven\n",
      "Number: 189, Predicted: one hundred and eighty-nine, Actual: one hundred and eighty-nine\n",
      "Number: 191, Predicted: one hundred and ninety-one, Actual: one hundred and ninety-one\n",
      "Number: 200, Predicted: two hundred and, Actual: two hundred\n",
      "Number: 201, Predicted: two hundred and one, Actual: two hundred and one\n",
      "Number: 205, Predicted: two hundred and five, Actual: two hundred and five\n",
      "Number: 206, Predicted: two hundred and six, Actual: two hundred and six\n",
      "Number: 214, Predicted: two hundred and fourteen, Actual: two hundred and fourteen\n",
      "Number: 216, Predicted: two hundred and sixteen, Actual: two hundred and sixteen\n",
      "Number: 217, Predicted: two hundred and seventeen, Actual: two hundred and seventeen\n",
      "Number: 230, Predicted: two hundred and thirty, Actual: two hundred and thirty\n",
      "Number: 240, Predicted: two hundred and forty, Actual: two hundred and forty\n",
      "Number: 241, Predicted: two hundred and forty-one, Actual: two hundred and forty-one\n",
      "Number: 242, Predicted: two hundred and forty-two, Actual: two hundred and forty-two\n",
      "Number: 243, Predicted: two hundred and forty-three, Actual: two hundred and forty-three\n",
      "Number: 251, Predicted: two hundred and fifty-one, Actual: two hundred and fifty-one\n",
      "Number: 252, Predicted: two hundred and fifty-two, Actual: two hundred and fifty-two\n",
      "Number: 269, Predicted: two hundred and sixty-nine, Actual: two hundred and sixty-nine\n",
      "Number: 270, Predicted: two hundred and seventy, Actual: two hundred and seventy\n",
      "Number: 273, Predicted: two hundred and seventy-three, Actual: two hundred and seventy-three\n",
      "Number: 276, Predicted: two hundred and seventy-six, Actual: two hundred and seventy-six\n",
      "Number: 288, Predicted: two hundred and eighty-eight, Actual: two hundred and eighty-eight\n",
      "Number: 295, Predicted: two hundred and ninety-five, Actual: two hundred and ninety-five\n",
      "Number: 308, Predicted: three hundred and eight, Actual: three hundred and eight\n",
      "Number: 313, Predicted: three hundred and thirteen, Actual: three hundred and thirteen\n",
      "Number: 315, Predicted: three hundred and fifteen, Actual: three hundred and fifteen\n",
      "Number: 330, Predicted: three hundred and thirty, Actual: three hundred and thirty\n",
      "Number: 337, Predicted: three hundred and thirty-seven, Actual: three hundred and thirty-seven\n",
      "Number: 339, Predicted: three hundred and thirty-nine, Actual: three hundred and thirty-nine\n",
      "Number: 343, Predicted: three hundred and forty-three, Actual: three hundred and forty-three\n",
      "Number: 345, Predicted: three hundred and forty-five, Actual: three hundred and forty-five\n",
      "Number: 366, Predicted: three hundred and sixty-six, Actual: three hundred and sixty-six\n",
      "Number: 372, Predicted: three hundred and seventy-two, Actual: three hundred and seventy-two\n",
      "Number: 378, Predicted: three hundred and seventy-eight, Actual: three hundred and seventy-eight\n",
      "Number: 379, Predicted: three hundred and seventy-nine, Actual: three hundred and seventy-nine\n",
      "Number: 385, Predicted: three hundred and eighty-five, Actual: three hundred and eighty-five\n",
      "Number: 387, Predicted: three hundred and eighty-seven, Actual: three hundred and eighty-seven\n",
      "Number: 389, Predicted: three hundred and eighty-nine, Actual: three hundred and eighty-nine\n",
      "Number: 391, Predicted: three hundred and ninety-one, Actual: three hundred and ninety-one\n",
      "Number: 392, Predicted: three hundred and ninety-two, Actual: three hundred and ninety-two\n",
      "Number: 397, Predicted: three hundred and ninety-seven, Actual: three hundred and ninety-seven\n",
      "Number: 401, Predicted: four hundred and one, Actual: four hundred and one\n",
      "Number: 406, Predicted: four hundred and six, Actual: four hundred and six\n",
      "Number: 413, Predicted: four hundred and thirteen, Actual: four hundred and thirteen\n",
      "Number: 418, Predicted: four hundred and eighteen, Actual: four hundred and eighteen\n",
      "Number: 427, Predicted: four hundred and twenty-seven, Actual: four hundred and twenty-seven\n",
      "Number: 435, Predicted: four hundred and thirty-five, Actual: four hundred and thirty-five\n",
      "Number: 454, Predicted: four hundred and fifty-four, Actual: four hundred and fifty-four\n",
      "Number: 455, Predicted: four hundred and fifty-five, Actual: four hundred and fifty-five\n",
      "Number: 458, Predicted: four hundred and fifty-eight, Actual: four hundred and fifty-eight\n",
      "Number: 459, Predicted: four hundred and fifty-nine, Actual: four hundred and fifty-nine\n",
      "Number: 460, Predicted: four hundred and sixty, Actual: four hundred and sixty\n",
      "Number: 461, Predicted: four hundred and sixty-one, Actual: four hundred and sixty-one\n",
      "Number: 466, Predicted: four hundred and sixty-six, Actual: four hundred and sixty-six\n",
      "Number: 471, Predicted: four hundred and seventy-one, Actual: four hundred and seventy-one\n",
      "Number: 474, Predicted: four hundred and seventy-four, Actual: four hundred and seventy-four\n",
      "Number: 475, Predicted: four hundred and seventy-five, Actual: four hundred and seventy-five\n",
      "Number: 476, Predicted: four hundred and seventy-six, Actual: four hundred and seventy-six\n",
      "Number: 484, Predicted: four hundred and eighty-four, Actual: four hundred and eighty-four\n",
      "Number: 489, Predicted: four hundred and eighty-nine, Actual: four hundred and eighty-nine\n",
      "Number: 491, Predicted: four hundred and ninety-one, Actual: four hundred and ninety-one\n",
      "Number: 492, Predicted: four hundred and ninety-two, Actual: four hundred and ninety-two\n",
      "Number: 498, Predicted: four hundred and ninety-eight, Actual: four hundred and ninety-eight\n",
      "Number: 502, Predicted: five hundred and two, Actual: five hundred and two\n",
      "Number: 504, Predicted: five hundred and four, Actual: five hundred and four\n",
      "Number: 508, Predicted: five hundred and eight, Actual: five hundred and eight\n",
      "Number: 510, Predicted: five hundred and ten, Actual: five hundred and ten\n",
      "Number: 520, Predicted: five hundred and twenty, Actual: five hundred and twenty\n",
      "Number: 524, Predicted: five hundred and twenty-four, Actual: five hundred and twenty-four\n",
      "Number: 540, Predicted: five hundred and forty, Actual: five hundred and forty\n",
      "Number: 546, Predicted: five hundred and forty-six, Actual: five hundred and forty-six\n",
      "Number: 553, Predicted: five hundred and fifty-three, Actual: five hundred and fifty-three\n",
      "Number: 555, Predicted: five hundred and fifty-five, Actual: five hundred and fifty-five\n",
      "Number: 556, Predicted: five hundred and fifty-six, Actual: five hundred and fifty-six\n",
      "Number: 560, Predicted: five hundred and sixty, Actual: five hundred and sixty\n",
      "Number: 561, Predicted: five hundred and sixty-one, Actual: five hundred and sixty-one\n",
      "Number: 562, Predicted: five hundred and sixty-two, Actual: five hundred and sixty-two\n",
      "Number: 563, Predicted: five hundred and sixty-three, Actual: five hundred and sixty-three\n",
      "Number: 564, Predicted: five hundred and sixty-four, Actual: five hundred and sixty-four\n",
      "Number: 565, Predicted: five hundred and sixty-five, Actual: five hundred and sixty-five\n",
      "Number: 566, Predicted: five hundred and sixty-six, Actual: five hundred and sixty-six\n",
      "Number: 573, Predicted: five hundred and seventy-three, Actual: five hundred and seventy-three\n",
      "Number: 574, Predicted: five hundred and seventy-four, Actual: five hundred and seventy-four\n",
      "Number: 577, Predicted: five hundred and seventy-seven, Actual: five hundred and seventy-seven\n",
      "Number: 592, Predicted: five hundred and ninety-two, Actual: five hundred and ninety-two\n",
      "Number: 600, Predicted: six hundred and, Actual: six hundred\n",
      "Number: 612, Predicted: six hundred and twelve, Actual: six hundred and twelve\n",
      "Number: 614, Predicted: six hundred and fourteen, Actual: six hundred and fourteen\n",
      "Number: 642, Predicted: six hundred and forty-two, Actual: six hundred and forty-two\n",
      "Number: 646, Predicted: six hundred and forty-six, Actual: six hundred and forty-six\n",
      "Number: 647, Predicted: six hundred and forty-seven, Actual: six hundred and forty-seven\n",
      "Number: 654, Predicted: six hundred and fifty-four, Actual: six hundred and fifty-four\n",
      "Number: 661, Predicted: six hundred and sixty-one, Actual: six hundred and sixty-one\n",
      "Number: 663, Predicted: six hundred and sixty-three, Actual: six hundred and sixty-three\n",
      "Number: 674, Predicted: six hundred and seventy-four, Actual: six hundred and seventy-four\n",
      "Number: 681, Predicted: six hundred and eighty-one, Actual: six hundred and eighty-one\n",
      "Number: 683, Predicted: six hundred and eighty-three, Actual: six hundred and eighty-three\n",
      "Number: 686, Predicted: six hundred and eighty-six, Actual: six hundred and eighty-six\n",
      "Number: 690, Predicted: six hundred and ninety, Actual: six hundred and ninety\n",
      "Number: 698, Predicted: six hundred and ninety-eight, Actual: six hundred and ninety-eight\n",
      "Number: 699, Predicted: six hundred and ninety-nine, Actual: six hundred and ninety-nine\n",
      "Number: 700, Predicted: seven hundred and ten, Actual: seven hundred\n",
      "Number: 701, Predicted: seven hundred and one, Actual: seven hundred and one\n",
      "Number: 702, Predicted: seven hundred and two, Actual: seven hundred and two\n",
      "Number: 719, Predicted: seven hundred and nineteen, Actual: seven hundred and nineteen\n",
      "Number: 724, Predicted: seven hundred and twenty-four, Actual: seven hundred and twenty-four\n",
      "Number: 725, Predicted: seven hundred and twenty-five, Actual: seven hundred and twenty-five\n",
      "Number: 726, Predicted: seven hundred and twenty-six, Actual: seven hundred and twenty-six\n",
      "Number: 727, Predicted: seven hundred and twenty-seven, Actual: seven hundred and twenty-seven\n",
      "Number: 729, Predicted: seven hundred and twenty-nine, Actual: seven hundred and twenty-nine\n",
      "Number: 733, Predicted: seven hundred and thirty-three, Actual: seven hundred and thirty-three\n",
      "Number: 738, Predicted: seven hundred and thirty-eight, Actual: seven hundred and thirty-eight\n",
      "Number: 742, Predicted: seven hundred and forty-two, Actual: seven hundred and forty-two\n",
      "Number: 747, Predicted: seven hundred and forty-seven, Actual: seven hundred and forty-seven\n",
      "Number: 748, Predicted: seven hundred and forty-eight, Actual: seven hundred and forty-eight\n",
      "Number: 763, Predicted: seven hundred and sixty-three, Actual: seven hundred and sixty-three\n",
      "Number: 766, Predicted: seven hundred and sixty-six, Actual: seven hundred and sixty-six\n",
      "Number: 768, Predicted: seven hundred and sixty-eight, Actual: seven hundred and sixty-eight\n",
      "Number: 769, Predicted: seven hundred and sixty-nine, Actual: seven hundred and sixty-nine\n",
      "Number: 771, Predicted: seven hundred and seventy-one, Actual: seven hundred and seventy-one\n",
      "Number: 772, Predicted: seven hundred and seventy-two, Actual: seven hundred and seventy-two\n",
      "Number: 775, Predicted: seven hundred and seventy-five, Actual: seven hundred and seventy-five\n",
      "Number: 776, Predicted: seven hundred and seventy-six, Actual: seven hundred and seventy-six\n",
      "Number: 779, Predicted: seven hundred and seventy-nine, Actual: seven hundred and seventy-nine\n",
      "Number: 782, Predicted: seven hundred and eighty-two, Actual: seven hundred and eighty-two\n",
      "Number: 791, Predicted: seven hundred and ninety-one, Actual: seven hundred and ninety-one\n",
      "Number: 794, Predicted: seven hundred and ninety-four, Actual: seven hundred and ninety-four\n",
      "Number: 795, Predicted: seven hundred and ninety-five, Actual: seven hundred and ninety-five\n",
      "Number: 804, Predicted: eight hundred and four, Actual: eight hundred and four\n",
      "Number: 805, Predicted: eight hundred and five, Actual: eight hundred and five\n",
      "Number: 812, Predicted: eight hundred and twelve, Actual: eight hundred and twelve\n",
      "Number: 815, Predicted: eight hundred and fifteen, Actual: eight hundred and fifteen\n",
      "Number: 818, Predicted: eight hundred and eighteen, Actual: eight hundred and eighteen\n",
      "Number: 821, Predicted: eight hundred and twenty-one, Actual: eight hundred and twenty-one\n",
      "Number: 831, Predicted: eight hundred and thirty-one, Actual: eight hundred and thirty-one\n",
      "Number: 838, Predicted: eight hundred and thirty-eight, Actual: eight hundred and thirty-eight\n",
      "Number: 839, Predicted: eight hundred and thirty-nine, Actual: eight hundred and thirty-nine\n",
      "Number: 840, Predicted: eight hundred and forty, Actual: eight hundred and forty\n",
      "Number: 847, Predicted: eight hundred and forty-seven, Actual: eight hundred and forty-seven\n",
      "Number: 848, Predicted: eight hundred and forty-eight, Actual: eight hundred and forty-eight\n",
      "Number: 856, Predicted: eight hundred and fifty-six, Actual: eight hundred and fifty-six\n",
      "Number: 857, Predicted: eight hundred and fifty-seven, Actual: eight hundred and fifty-seven\n",
      "Number: 860, Predicted: eight hundred and sixty, Actual: eight hundred and sixty\n",
      "Number: 862, Predicted: eight hundred and sixty-two, Actual: eight hundred and sixty-two\n",
      "Number: 871, Predicted: eight hundred and seventy-one, Actual: eight hundred and seventy-one\n",
      "Number: 875, Predicted: eight hundred and seventy-five, Actual: eight hundred and seventy-five\n",
      "Number: 878, Predicted: eight hundred and seventy-eight, Actual: eight hundred and seventy-eight\n",
      "Number: 880, Predicted: eight hundred and eighty, Actual: eight hundred and eighty\n",
      "Number: 888, Predicted: eight hundred and eighty-eight, Actual: eight hundred and eighty-eight\n",
      "Number: 890, Predicted: eight hundred and ninety, Actual: eight hundred and ninety\n",
      "Number: 897, Predicted: eight hundred and ninety-seven, Actual: eight hundred and ninety-seven\n",
      "Number: 929, Predicted: nine hundred and twenty-nine, Actual: nine hundred and twenty-nine\n",
      "Number: 937, Predicted: nine hundred and thirty-seven, Actual: nine hundred and thirty-seven\n",
      "Number: 944, Predicted: nine hundred and forty-four, Actual: nine hundred and forty-four\n",
      "Number: 955, Predicted: nine hundred and fifty-five, Actual: nine hundred and fifty-five\n",
      "Number: 957, Predicted: nine hundred and fifty-seven, Actual: nine hundred and fifty-seven\n",
      "Number: 962, Predicted: nine hundred and sixty-two, Actual: nine hundred and sixty-two\n",
      "Number: 976, Predicted: nine hundred and seventy-six, Actual: nine hundred and seventy-six\n",
      "Number: 980, Predicted: nine hundred and eighty, Actual: nine hundred and eighty\n",
      "Number: 982, Predicted: nine hundred and eighty-two, Actual: nine hundred and eighty-two\n",
      "Number: 992, Predicted: nine hundred and ninety-two, Actual: nine hundred and ninety-two\n",
      "Number: 996, Predicted: nine hundred and ninety-six, Actual: nine hundred and ninety-six\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.955"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(custom_model, val_df, tokenize_numeric_representation, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-notebooks/lib/python3.11/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: 1, Predicted: zero, Actual: one\n",
      "Number: 4, Predicted: forty-one, Actual: four\n",
      "Number: 13, Predicted: thirteen, Actual: thirteen\n",
      "Number: 14, Predicted: fourteen, Actual: fourteen\n",
      "Number: 20, Predicted: twenty, Actual: twenty\n",
      "Number: 21, Predicted: twenty-one, Actual: twenty-one\n",
      "Number: 27, Predicted: twenty-seven, Actual: twenty-seven\n",
      "Number: 32, Predicted: thirty-two, Actual: thirty-two\n",
      "Number: 34, Predicted: thirty-four, Actual: thirty-four\n",
      "Number: 35, Predicted: thirty-five, Actual: thirty-five\n",
      "Number: 40, Predicted: forty, Actual: forty\n",
      "Number: 47, Predicted: forty-seven, Actual: forty-seven\n",
      "Number: 52, Predicted: fifty-two, Actual: fifty-two\n",
      "Number: 58, Predicted: fifty-eight, Actual: fifty-eight\n",
      "Number: 62, Predicted: sixty-two, Actual: sixty-two\n",
      "Number: 64, Predicted: sixty-four, Actual: sixty-four\n",
      "Number: 71, Predicted: seventy-one, Actual: seventy-one\n",
      "Number: 80, Predicted: eighty, Actual: eighty\n",
      "Number: 85, Predicted: eighty-five, Actual: eighty-five\n",
      "Number: 87, Predicted: eighty-seven, Actual: eighty-seven\n",
      "Number: 91, Predicted: ninety-one, Actual: ninety-one\n",
      "Number: 95, Predicted: ninety-five, Actual: ninety-five\n",
      "Number: 98, Predicted: ninety-eight, Actual: ninety-eight\n",
      "Number: 99, Predicted: ninety-nine, Actual: ninety-nine\n",
      "Number: 102, Predicted: one hundred and two, Actual: one hundred and two\n",
      "Number: 105, Predicted: one hundred and five, Actual: one hundred and five\n",
      "Number: 106, Predicted: one hundred and six, Actual: one hundred and six\n",
      "Number: 121, Predicted: one hundred and twenty-one, Actual: one hundred and twenty-one\n",
      "Number: 128, Predicted: one hundred and twenty-eight, Actual: one hundred and twenty-eight\n",
      "Number: 130, Predicted: one hundred and thirty, Actual: one hundred and thirty\n",
      "Number: 134, Predicted: one hundred and thirty-four, Actual: one hundred and thirty-four\n",
      "Number: 138, Predicted: one hundred and thirty-eight, Actual: one hundred and thirty-eight\n",
      "Number: 156, Predicted: one hundred and fifty-six, Actual: one hundred and fifty-six\n",
      "Number: 159, Predicted: one hundred and fifty-nine, Actual: one hundred and fifty-nine\n",
      "Number: 160, Predicted: one hundred and sixty, Actual: one hundred and sixty\n",
      "Number: 161, Predicted: one hundred and sixty-one, Actual: one hundred and sixty-one\n",
      "Number: 166, Predicted: one hundred and sixty-six, Actual: one hundred and sixty-six\n",
      "Number: 170, Predicted: one hundred and seventy, Actual: one hundred and seventy\n",
      "Number: 187, Predicted: one hundred and eighty-seven, Actual: one hundred and eighty-seven\n",
      "Number: 189, Predicted: one hundred and eighty-nine, Actual: one hundred and eighty-nine\n",
      "Number: 191, Predicted: one hundred and ninety-one, Actual: one hundred and ninety-one\n",
      "Number: 200, Predicted: two hundred and, Actual: two hundred\n",
      "Number: 201, Predicted: two hundred and one, Actual: two hundred and one\n",
      "Number: 205, Predicted: two hundred and five, Actual: two hundred and five\n",
      "Number: 206, Predicted: two hundred and six, Actual: two hundred and six\n",
      "Number: 214, Predicted: two hundred and fourteen, Actual: two hundred and fourteen\n",
      "Number: 216, Predicted: two hundred and sixteen, Actual: two hundred and sixteen\n",
      "Number: 217, Predicted: two hundred and seventeen, Actual: two hundred and seventeen\n",
      "Number: 230, Predicted: two hundred and thirty, Actual: two hundred and thirty\n",
      "Number: 240, Predicted: two hundred and forty, Actual: two hundred and forty\n",
      "Number: 241, Predicted: two hundred and forty-one, Actual: two hundred and forty-one\n",
      "Number: 242, Predicted: two hundred and forty-two, Actual: two hundred and forty-two\n",
      "Number: 243, Predicted: two hundred and forty-three, Actual: two hundred and forty-three\n",
      "Number: 251, Predicted: two hundred and fifty-one, Actual: two hundred and fifty-one\n",
      "Number: 252, Predicted: two hundred and fifty-two, Actual: two hundred and fifty-two\n",
      "Number: 269, Predicted: two hundred and sixty-nine, Actual: two hundred and sixty-nine\n",
      "Number: 270, Predicted: two hundred and seventy, Actual: two hundred and seventy\n",
      "Number: 273, Predicted: two hundred and seventy-three, Actual: two hundred and seventy-three\n",
      "Number: 276, Predicted: two hundred and seventy-six, Actual: two hundred and seventy-six\n",
      "Number: 288, Predicted: two hundred and eighty-eight, Actual: two hundred and eighty-eight\n",
      "Number: 295, Predicted: two hundred and ninety-five, Actual: two hundred and ninety-five\n",
      "Number: 308, Predicted: three hundred and eight, Actual: three hundred and eight\n",
      "Number: 313, Predicted: three hundred and thirteen, Actual: three hundred and thirteen\n",
      "Number: 315, Predicted: three hundred and fifteen, Actual: three hundred and fifteen\n",
      "Number: 330, Predicted: three hundred and thirty, Actual: three hundred and thirty\n",
      "Number: 337, Predicted: three hundred and thirty-seven, Actual: three hundred and thirty-seven\n",
      "Number: 339, Predicted: three hundred and thirty-nine, Actual: three hundred and thirty-nine\n",
      "Number: 343, Predicted: three hundred and forty-three, Actual: three hundred and forty-three\n",
      "Number: 345, Predicted: three hundred and forty-five, Actual: three hundred and forty-five\n",
      "Number: 366, Predicted: three hundred and sixty-six, Actual: three hundred and sixty-six\n",
      "Number: 372, Predicted: three hundred and seventy-two, Actual: three hundred and seventy-two\n",
      "Number: 378, Predicted: three hundred and seventy-eight, Actual: three hundred and seventy-eight\n",
      "Number: 379, Predicted: three hundred and seventy-nine, Actual: three hundred and seventy-nine\n",
      "Number: 385, Predicted: three hundred and eighty-five, Actual: three hundred and eighty-five\n",
      "Number: 387, Predicted: three hundred and eighty-seven, Actual: three hundred and eighty-seven\n",
      "Number: 389, Predicted: three hundred and eighty-nine, Actual: three hundred and eighty-nine\n",
      "Number: 391, Predicted: three hundred and ninety-one, Actual: three hundred and ninety-one\n",
      "Number: 392, Predicted: three hundred and ninety-two, Actual: three hundred and ninety-two\n",
      "Number: 397, Predicted: three hundred and ninety-seven, Actual: three hundred and ninety-seven\n",
      "Number: 401, Predicted: four hundred and one, Actual: four hundred and one\n",
      "Number: 406, Predicted: four hundred and six, Actual: four hundred and six\n",
      "Number: 413, Predicted: four hundred and thirteen, Actual: four hundred and thirteen\n",
      "Number: 418, Predicted: four hundred and eighteen, Actual: four hundred and eighteen\n",
      "Number: 427, Predicted: four hundred and twenty-seven, Actual: four hundred and twenty-seven\n",
      "Number: 435, Predicted: four hundred and thirty-five, Actual: four hundred and thirty-five\n",
      "Number: 454, Predicted: four hundred and fifty-four, Actual: four hundred and fifty-four\n",
      "Number: 455, Predicted: four hundred and fifty-five, Actual: four hundred and fifty-five\n",
      "Number: 458, Predicted: four hundred and fifty-eight, Actual: four hundred and fifty-eight\n",
      "Number: 459, Predicted: four hundred and fifty-nine, Actual: four hundred and fifty-nine\n",
      "Number: 460, Predicted: four hundred and sixty, Actual: four hundred and sixty\n",
      "Number: 461, Predicted: four hundred and sixty-one, Actual: four hundred and sixty-one\n",
      "Number: 466, Predicted: four hundred and sixty-six, Actual: four hundred and sixty-six\n",
      "Number: 471, Predicted: four hundred and seventy-one, Actual: four hundred and seventy-one\n",
      "Number: 474, Predicted: four hundred and seventy-four, Actual: four hundred and seventy-four\n",
      "Number: 475, Predicted: four hundred and seventy-five, Actual: four hundred and seventy-five\n",
      "Number: 476, Predicted: four hundred and seventy-six, Actual: four hundred and seventy-six\n",
      "Number: 484, Predicted: four hundred and eighty-four, Actual: four hundred and eighty-four\n",
      "Number: 489, Predicted: four hundred and eighty-nine, Actual: four hundred and eighty-nine\n",
      "Number: 491, Predicted: four hundred and ninety-one, Actual: four hundred and ninety-one\n",
      "Number: 492, Predicted: four hundred and ninety-two, Actual: four hundred and ninety-two\n",
      "Number: 498, Predicted: four hundred and ninety-eight, Actual: four hundred and ninety-eight\n",
      "Number: 502, Predicted: five hundred and two, Actual: five hundred and two\n",
      "Number: 504, Predicted: five hundred and four, Actual: five hundred and four\n",
      "Number: 508, Predicted: five hundred and eight, Actual: five hundred and eight\n",
      "Number: 510, Predicted: five hundred and ten, Actual: five hundred and ten\n",
      "Number: 520, Predicted: five hundred and twenty, Actual: five hundred and twenty\n",
      "Number: 524, Predicted: five hundred and twenty-four, Actual: five hundred and twenty-four\n",
      "Number: 540, Predicted: five hundred and forty, Actual: five hundred and forty\n",
      "Number: 546, Predicted: five hundred and forty-six, Actual: five hundred and forty-six\n",
      "Number: 553, Predicted: five hundred and fifty-three, Actual: five hundred and fifty-three\n",
      "Number: 555, Predicted: five hundred and fifty-five, Actual: five hundred and fifty-five\n",
      "Number: 556, Predicted: five hundred and fifty-six, Actual: five hundred and fifty-six\n",
      "Number: 560, Predicted: five hundred and sixty, Actual: five hundred and sixty\n",
      "Number: 561, Predicted: five hundred and sixty-one, Actual: five hundred and sixty-one\n",
      "Number: 562, Predicted: five hundred and sixty-two, Actual: five hundred and sixty-two\n",
      "Number: 563, Predicted: five hundred and sixty-three, Actual: five hundred and sixty-three\n",
      "Number: 564, Predicted: five hundred and sixty-four, Actual: five hundred and sixty-four\n",
      "Number: 565, Predicted: five hundred and sixty-five, Actual: five hundred and sixty-five\n",
      "Number: 566, Predicted: five hundred and sixty-six, Actual: five hundred and sixty-six\n",
      "Number: 573, Predicted: five hundred and seventy-three, Actual: five hundred and seventy-three\n",
      "Number: 574, Predicted: five hundred and seventy-four, Actual: five hundred and seventy-four\n",
      "Number: 577, Predicted: five hundred and seventy-seven, Actual: five hundred and seventy-seven\n",
      "Number: 592, Predicted: five hundred and ninety-two, Actual: five hundred and ninety-two\n",
      "Number: 600, Predicted: six hundred and, Actual: six hundred\n",
      "Number: 612, Predicted: six hundred and twelve, Actual: six hundred and twelve\n",
      "Number: 614, Predicted: six hundred and fourteen, Actual: six hundred and fourteen\n",
      "Number: 642, Predicted: six hundred and forty-two, Actual: six hundred and forty-two\n",
      "Number: 646, Predicted: six hundred and forty-six, Actual: six hundred and forty-six\n",
      "Number: 647, Predicted: six hundred and forty-seven, Actual: six hundred and forty-seven\n",
      "Number: 654, Predicted: six hundred and fifty-four, Actual: six hundred and fifty-four\n",
      "Number: 661, Predicted: six hundred and sixty-one, Actual: six hundred and sixty-one\n",
      "Number: 663, Predicted: six hundred and sixty-three, Actual: six hundred and sixty-three\n",
      "Number: 674, Predicted: six hundred and seventy-four, Actual: six hundred and seventy-four\n",
      "Number: 681, Predicted: six hundred and eighty-one, Actual: six hundred and eighty-one\n",
      "Number: 683, Predicted: six hundred and eighty-three, Actual: six hundred and eighty-three\n",
      "Number: 686, Predicted: six hundred and eighty-six, Actual: six hundred and eighty-six\n",
      "Number: 690, Predicted: six hundred and ninety, Actual: six hundred and ninety\n",
      "Number: 698, Predicted: six hundred and ninety-eight, Actual: six hundred and ninety-eight\n",
      "Number: 699, Predicted: six hundred and ninety-nine, Actual: six hundred and ninety-nine\n",
      "Number: 700, Predicted: seven hundred and four, Actual: seven hundred\n",
      "Number: 701, Predicted: seven hundred and one, Actual: seven hundred and one\n",
      "Number: 702, Predicted: seven hundred and two, Actual: seven hundred and two\n",
      "Number: 719, Predicted: seven hundred and nineteen, Actual: seven hundred and nineteen\n",
      "Number: 724, Predicted: seven hundred and twenty-four, Actual: seven hundred and twenty-four\n",
      "Number: 725, Predicted: seven hundred and twenty-five, Actual: seven hundred and twenty-five\n",
      "Number: 726, Predicted: seven hundred and twenty-six, Actual: seven hundred and twenty-six\n",
      "Number: 727, Predicted: seven hundred and twenty-seven, Actual: seven hundred and twenty-seven\n",
      "Number: 729, Predicted: seven hundred and twenty-nine, Actual: seven hundred and twenty-nine\n",
      "Number: 733, Predicted: seven hundred and thirty-three, Actual: seven hundred and thirty-three\n",
      "Number: 738, Predicted: seven hundred and thirty-eight, Actual: seven hundred and thirty-eight\n",
      "Number: 742, Predicted: seven hundred and forty-two, Actual: seven hundred and forty-two\n",
      "Number: 747, Predicted: seven hundred and forty-seven, Actual: seven hundred and forty-seven\n",
      "Number: 748, Predicted: seven hundred and forty-eight, Actual: seven hundred and forty-eight\n",
      "Number: 763, Predicted: seven hundred and sixty-three, Actual: seven hundred and sixty-three\n",
      "Number: 766, Predicted: seven hundred and sixty-six, Actual: seven hundred and sixty-six\n",
      "Number: 768, Predicted: seven hundred and sixty-eight, Actual: seven hundred and sixty-eight\n",
      "Number: 769, Predicted: seven hundred and sixty-nine, Actual: seven hundred and sixty-nine\n",
      "Number: 771, Predicted: seven hundred and seventy-one, Actual: seven hundred and seventy-one\n",
      "Number: 772, Predicted: seven hundred and seventy-two, Actual: seven hundred and seventy-two\n",
      "Number: 775, Predicted: seven hundred and seventy-five, Actual: seven hundred and seventy-five\n",
      "Number: 776, Predicted: seven hundred and seventy-six, Actual: seven hundred and seventy-six\n",
      "Number: 779, Predicted: seven hundred and seventy-nine, Actual: seven hundred and seventy-nine\n",
      "Number: 782, Predicted: seven hundred and eighty-two, Actual: seven hundred and eighty-two\n",
      "Number: 791, Predicted: seven hundred and ninety-one, Actual: seven hundred and ninety-one\n",
      "Number: 794, Predicted: seven hundred and ninety-four, Actual: seven hundred and ninety-four\n",
      "Number: 795, Predicted: seven hundred and ninety-five, Actual: seven hundred and ninety-five\n",
      "Number: 804, Predicted: eight hundred and four, Actual: eight hundred and four\n",
      "Number: 805, Predicted: eight hundred and five, Actual: eight hundred and five\n",
      "Number: 812, Predicted: eight hundred and twelve, Actual: eight hundred and twelve\n",
      "Number: 815, Predicted: eight hundred and fifteen, Actual: eight hundred and fifteen\n",
      "Number: 818, Predicted: eight hundred and eighteen, Actual: eight hundred and eighteen\n",
      "Number: 821, Predicted: eight hundred and twenty-one, Actual: eight hundred and twenty-one\n",
      "Number: 831, Predicted: eight hundred and thirty-one, Actual: eight hundred and thirty-one\n",
      "Number: 838, Predicted: eight hundred and thirty-eight, Actual: eight hundred and thirty-eight\n",
      "Number: 839, Predicted: eight hundred and thirty-nine, Actual: eight hundred and thirty-nine\n",
      "Number: 840, Predicted: eight hundred and forty, Actual: eight hundred and forty\n",
      "Number: 847, Predicted: eight hundred and forty-seven, Actual: eight hundred and forty-seven\n",
      "Number: 848, Predicted: eight hundred and forty-eight, Actual: eight hundred and forty-eight\n",
      "Number: 856, Predicted: eight hundred and fifty-six, Actual: eight hundred and fifty-six\n",
      "Number: 857, Predicted: eight hundred and fifty-seven, Actual: eight hundred and fifty-seven\n",
      "Number: 860, Predicted: eight hundred and sixty, Actual: eight hundred and sixty\n",
      "Number: 862, Predicted: eight hundred and sixty-two, Actual: eight hundred and sixty-two\n",
      "Number: 871, Predicted: eight hundred and seventy-one, Actual: eight hundred and seventy-one\n",
      "Number: 875, Predicted: eight hundred and seventy-five, Actual: eight hundred and seventy-five\n",
      "Number: 878, Predicted: eight hundred and seventy-eight, Actual: eight hundred and seventy-eight\n",
      "Number: 880, Predicted: eight hundred and eighty, Actual: eight hundred and eighty\n",
      "Number: 888, Predicted: eight hundred and eighty-eight, Actual: eight hundred and eighty-eight\n",
      "Number: 890, Predicted: eight hundred and ninety, Actual: eight hundred and ninety\n",
      "Number: 897, Predicted: eight hundred and ninety-seven, Actual: eight hundred and ninety-seven\n",
      "Number: 929, Predicted: nine hundred and twenty-nine, Actual: nine hundred and twenty-nine\n",
      "Number: 937, Predicted: nine hundred and thirty-seven, Actual: nine hundred and thirty-seven\n",
      "Number: 944, Predicted: nine hundred and forty-four, Actual: nine hundred and forty-four\n",
      "Number: 955, Predicted: nine hundred and fifty-five, Actual: nine hundred and fifty-five\n",
      "Number: 957, Predicted: nine hundred and fifty-seven, Actual: nine hundred and fifty-seven\n",
      "Number: 962, Predicted: nine hundred and sixty-two, Actual: nine hundred and sixty-two\n",
      "Number: 976, Predicted: nine hundred and seventy-six, Actual: nine hundred and seventy-six\n",
      "Number: 980, Predicted: nine hundred and eighty, Actual: nine hundred and eighty\n",
      "Number: 982, Predicted: nine hundred and eighty-two, Actual: nine hundred and eighty-two\n",
      "Number: 992, Predicted: nine hundred and ninety-two, Actual: nine hundred and ninety-two\n",
      "Number: 996, Predicted: nine hundred and ninety-six, Actual: nine hundred and ninety-six\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model, val_df, tokenize_numeric_representation, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: 1, Predicted: zero, Actual: one\n",
      "Number: 4, Predicted: forty-four, Actual: four\n",
      "Number: 13, Predicted: thirty-three, Actual: thirteen\n",
      "Number: 14, Predicted: forty-four, Actual: fourteen\n",
      "Number: 20, Predicted: twenty, Actual: twenty\n",
      "Number: 21, Predicted: twenty-one, Actual: twenty-one\n",
      "Number: 27, Predicted: twenty-seven, Actual: twenty-seven\n",
      "Number: 32, Predicted: thirty-two, Actual: thirty-two\n",
      "Number: 34, Predicted: thirty-four, Actual: thirty-four\n",
      "Number: 35, Predicted: thirty-five, Actual: thirty-five\n",
      "Number: 40, Predicted: forty, Actual: forty\n",
      "Number: 47, Predicted: forty-seven, Actual: forty-seven\n",
      "Number: 52, Predicted: fifty-two, Actual: fifty-two\n",
      "Number: 58, Predicted: fifty-eight, Actual: fifty-eight\n",
      "Number: 62, Predicted: sixty-two, Actual: sixty-two\n",
      "Number: 64, Predicted: sixty-four, Actual: sixty-four\n",
      "Number: 71, Predicted: seventy-one, Actual: seventy-one\n",
      "Number: 80, Predicted: eighty, Actual: eighty\n",
      "Number: 85, Predicted: eighty-five, Actual: eighty-five\n",
      "Number: 87, Predicted: eighty-seven, Actual: eighty-seven\n",
      "Number: 91, Predicted: ninety-one, Actual: ninety-one\n",
      "Number: 95, Predicted: ninety-five, Actual: ninety-five\n",
      "Number: 98, Predicted: ninety-eight, Actual: ninety-eight\n",
      "Number: 99, Predicted: nine, Actual: ninety-nine\n",
      "Number: 102, Predicted: one hundred and two, Actual: one hundred and two\n",
      "Number: 105, Predicted: one hundred and five, Actual: one hundred and five\n",
      "Number: 106, Predicted: one hundred and six, Actual: one hundred and six\n",
      "Number: 121, Predicted: one hundred and twenty-one, Actual: one hundred and twenty-one\n",
      "Number: 128, Predicted: one hundred and twenty-eight, Actual: one hundred and twenty-eight\n",
      "Number: 130, Predicted: one hundred and thirty, Actual: one hundred and thirty\n",
      "Number: 134, Predicted: one hundred and thirty-four, Actual: one hundred and thirty-four\n",
      "Number: 138, Predicted: one hundred and thirty-eight, Actual: one hundred and thirty-eight\n",
      "Number: 156, Predicted: one hundred and fifty-six, Actual: one hundred and fifty-six\n",
      "Number: 159, Predicted: one hundred and fifty-nine, Actual: one hundred and fifty-nine\n",
      "Number: 160, Predicted: one hundred and sixty, Actual: one hundred and sixty\n",
      "Number: 161, Predicted: one hundred and sixty-one, Actual: one hundred and sixty-one\n",
      "Number: 166, Predicted: one hundred and sixty-six, Actual: one hundred and sixty-six\n",
      "Number: 170, Predicted: one hundred and seventy, Actual: one hundred and seventy\n",
      "Number: 187, Predicted: one hundred and eighty-seven, Actual: one hundred and eighty-seven\n",
      "Number: 189, Predicted: one hundred and eighty-nine, Actual: one hundred and eighty-nine\n",
      "Number: 191, Predicted: one hundred and ninety-one, Actual: one hundred and ninety-one\n",
      "Number: 200, Predicted: two hundred and, Actual: two hundred\n",
      "Number: 201, Predicted: two hundred and one, Actual: two hundred and one\n",
      "Number: 205, Predicted: two hundred and five, Actual: two hundred and five\n",
      "Number: 206, Predicted: two hundred and six, Actual: two hundred and six\n",
      "Number: 214, Predicted: two hundred and fourteen, Actual: two hundred and fourteen\n",
      "Number: 216, Predicted: two hundred and sixteen, Actual: two hundred and sixteen\n",
      "Number: 217, Predicted: two hundred and seventeen, Actual: two hundred and seventeen\n",
      "Number: 230, Predicted: two hundred and thirty, Actual: two hundred and thirty\n",
      "Number: 240, Predicted: two hundred and forty, Actual: two hundred and forty\n",
      "Number: 241, Predicted: two hundred and forty-one, Actual: two hundred and forty-one\n",
      "Number: 242, Predicted: two hundred and forty-two, Actual: two hundred and forty-two\n",
      "Number: 243, Predicted: two hundred and forty-three, Actual: two hundred and forty-three\n",
      "Number: 251, Predicted: two hundred and fifty-one, Actual: two hundred and fifty-one\n",
      "Number: 252, Predicted: two hundred and fifty-two, Actual: two hundred and fifty-two\n",
      "Number: 269, Predicted: two hundred and sixty-nine, Actual: two hundred and sixty-nine\n",
      "Number: 270, Predicted: two hundred and seventy, Actual: two hundred and seventy\n",
      "Number: 273, Predicted: two hundred and seventy-three, Actual: two hundred and seventy-three\n",
      "Number: 276, Predicted: two hundred and seventy-six, Actual: two hundred and seventy-six\n",
      "Number: 288, Predicted: two hundred and eighty-eight, Actual: two hundred and eighty-eight\n",
      "Number: 295, Predicted: two hundred and ninety-five, Actual: two hundred and ninety-five\n",
      "Number: 308, Predicted: three hundred and eight, Actual: three hundred and eight\n",
      "Number: 313, Predicted: three hundred and thirteen, Actual: three hundred and thirteen\n",
      "Number: 315, Predicted: three hundred and fifteen, Actual: three hundred and fifteen\n",
      "Number: 330, Predicted: three hundred and thirty, Actual: three hundred and thirty\n",
      "Number: 337, Predicted: three hundred and thirty-seven, Actual: three hundred and thirty-seven\n",
      "Number: 339, Predicted: three hundred and thirty-nine, Actual: three hundred and thirty-nine\n",
      "Number: 343, Predicted: three hundred and forty-three, Actual: three hundred and forty-three\n",
      "Number: 345, Predicted: three hundred and forty-five, Actual: three hundred and forty-five\n",
      "Number: 366, Predicted: three hundred and sixty-six, Actual: three hundred and sixty-six\n",
      "Number: 372, Predicted: three hundred and seventy-two, Actual: three hundred and seventy-two\n",
      "Number: 378, Predicted: three hundred and seventy-eight, Actual: three hundred and seventy-eight\n",
      "Number: 379, Predicted: three hundred and seventy-nine, Actual: three hundred and seventy-nine\n",
      "Number: 385, Predicted: three hundred and eighty-five, Actual: three hundred and eighty-five\n",
      "Number: 387, Predicted: three hundred and eighty-seven, Actual: three hundred and eighty-seven\n",
      "Number: 389, Predicted: three hundred and eighty-nine, Actual: three hundred and eighty-nine\n",
      "Number: 391, Predicted: three hundred and ninety-one, Actual: three hundred and ninety-one\n",
      "Number: 392, Predicted: three hundred and ninety-two, Actual: three hundred and ninety-two\n",
      "Number: 397, Predicted: three hundred and ninety-seven, Actual: three hundred and ninety-seven\n",
      "Number: 401, Predicted: four hundred and one, Actual: four hundred and one\n",
      "Number: 406, Predicted: four hundred and six, Actual: four hundred and six\n",
      "Number: 413, Predicted: four hundred and thirteen, Actual: four hundred and thirteen\n",
      "Number: 418, Predicted: four hundred and eighteen, Actual: four hundred and eighteen\n",
      "Number: 427, Predicted: four hundred and twenty-seven, Actual: four hundred and twenty-seven\n",
      "Number: 435, Predicted: four hundred and thirty-five, Actual: four hundred and thirty-five\n",
      "Number: 454, Predicted: four hundred and fifty-four, Actual: four hundred and fifty-four\n",
      "Number: 455, Predicted: four hundred and fifty-five, Actual: four hundred and fifty-five\n",
      "Number: 458, Predicted: four hundred and fifty-eight, Actual: four hundred and fifty-eight\n",
      "Number: 459, Predicted: four hundred and fifty-nine, Actual: four hundred and fifty-nine\n",
      "Number: 460, Predicted: four hundred and sixty, Actual: four hundred and sixty\n",
      "Number: 461, Predicted: four hundred and sixty-one, Actual: four hundred and sixty-one\n",
      "Number: 466, Predicted: four hundred and sixty-six, Actual: four hundred and sixty-six\n",
      "Number: 471, Predicted: four hundred and seventy-one, Actual: four hundred and seventy-one\n",
      "Number: 474, Predicted: four hundred and seventy-four, Actual: four hundred and seventy-four\n",
      "Number: 475, Predicted: four hundred and seventy-five, Actual: four hundred and seventy-five\n",
      "Number: 476, Predicted: four hundred and seventy-six, Actual: four hundred and seventy-six\n",
      "Number: 484, Predicted: four hundred and eighty-four, Actual: four hundred and eighty-four\n",
      "Number: 489, Predicted: four hundred and eighty-nine, Actual: four hundred and eighty-nine\n",
      "Number: 491, Predicted: four hundred and ninety-one, Actual: four hundred and ninety-one\n",
      "Number: 492, Predicted: four hundred and ninety-two, Actual: four hundred and ninety-two\n",
      "Number: 498, Predicted: four hundred and ninety-eight, Actual: four hundred and ninety-eight\n",
      "Number: 502, Predicted: five hundred and two, Actual: five hundred and two\n",
      "Number: 504, Predicted: five hundred and four, Actual: five hundred and four\n",
      "Number: 508, Predicted: five hundred and eight, Actual: five hundred and eight\n",
      "Number: 510, Predicted: five hundred and ten, Actual: five hundred and ten\n",
      "Number: 520, Predicted: five hundred and twenty, Actual: five hundred and twenty\n",
      "Number: 524, Predicted: five hundred and twenty-four, Actual: five hundred and twenty-four\n",
      "Number: 540, Predicted: five hundred and forty, Actual: five hundred and forty\n",
      "Number: 546, Predicted: five hundred and forty-six, Actual: five hundred and forty-six\n",
      "Number: 553, Predicted: five hundred and fifty-three, Actual: five hundred and fifty-three\n",
      "Number: 555, Predicted: five hundred and fifty-five, Actual: five hundred and fifty-five\n",
      "Number: 556, Predicted: five hundred and fifty-six, Actual: five hundred and fifty-six\n",
      "Number: 560, Predicted: five hundred and sixty, Actual: five hundred and sixty\n",
      "Number: 561, Predicted: five hundred and sixty-one, Actual: five hundred and sixty-one\n",
      "Number: 562, Predicted: five hundred and sixty-two, Actual: five hundred and sixty-two\n",
      "Number: 563, Predicted: five hundred and sixty-three, Actual: five hundred and sixty-three\n",
      "Number: 564, Predicted: five hundred and sixty-four, Actual: five hundred and sixty-four\n",
      "Number: 565, Predicted: five hundred and sixty-five, Actual: five hundred and sixty-five\n",
      "Number: 566, Predicted: five hundred and sixty-six, Actual: five hundred and sixty-six\n",
      "Number: 573, Predicted: five hundred and seventy-three, Actual: five hundred and seventy-three\n",
      "Number: 574, Predicted: five hundred and seventy-four, Actual: five hundred and seventy-four\n",
      "Number: 577, Predicted: five hundred and seventy-seven, Actual: five hundred and seventy-seven\n",
      "Number: 592, Predicted: five hundred and ninety-two, Actual: five hundred and ninety-two\n",
      "Number: 600, Predicted: six hundred, Actual: six hundred\n",
      "Number: 612, Predicted: six hundred and twelve, Actual: six hundred and twelve\n",
      "Number: 614, Predicted: six hundred and fourteen, Actual: six hundred and fourteen\n",
      "Number: 642, Predicted: six hundred and forty-two, Actual: six hundred and forty-two\n",
      "Number: 646, Predicted: six hundred and forty-six, Actual: six hundred and forty-six\n",
      "Number: 647, Predicted: six hundred and forty-seven, Actual: six hundred and forty-seven\n",
      "Number: 654, Predicted: six hundred and fifty-four, Actual: six hundred and fifty-four\n",
      "Number: 661, Predicted: six hundred and sixty-one, Actual: six hundred and sixty-one\n",
      "Number: 663, Predicted: six hundred and sixty-three, Actual: six hundred and sixty-three\n",
      "Number: 674, Predicted: six hundred and seventy-four, Actual: six hundred and seventy-four\n",
      "Number: 681, Predicted: six hundred and eighty-one, Actual: six hundred and eighty-one\n",
      "Number: 683, Predicted: six hundred and eighty-three, Actual: six hundred and eighty-three\n",
      "Number: 686, Predicted: six hundred and eighty-six, Actual: six hundred and eighty-six\n",
      "Number: 690, Predicted: six hundred and ninety, Actual: six hundred and ninety\n",
      "Number: 698, Predicted: six hundred and ninety-eight, Actual: six hundred and ninety-eight\n",
      "Number: 699, Predicted: six hundred and ninety-nine, Actual: six hundred and ninety-nine\n",
      "Number: 700, Predicted: seven hundred and ten, Actual: seven hundred\n",
      "Number: 701, Predicted: seven hundred and one, Actual: seven hundred and one\n",
      "Number: 702, Predicted: seven hundred and two, Actual: seven hundred and two\n",
      "Number: 719, Predicted: seven hundred and nineteen, Actual: seven hundred and nineteen\n",
      "Number: 724, Predicted: seven hundred and twenty-four, Actual: seven hundred and twenty-four\n",
      "Number: 725, Predicted: seven hundred and twenty-five, Actual: seven hundred and twenty-five\n",
      "Number: 726, Predicted: seven hundred and twenty-six, Actual: seven hundred and twenty-six\n",
      "Number: 727, Predicted: seven hundred and twenty-seven, Actual: seven hundred and twenty-seven\n",
      "Number: 729, Predicted: seven hundred and twenty-nine, Actual: seven hundred and twenty-nine\n",
      "Number: 733, Predicted: seven hundred and thirty-three, Actual: seven hundred and thirty-three\n",
      "Number: 738, Predicted: seven hundred and thirty-eight, Actual: seven hundred and thirty-eight\n",
      "Number: 742, Predicted: seven hundred and forty-two, Actual: seven hundred and forty-two\n",
      "Number: 747, Predicted: seven hundred and forty-seven, Actual: seven hundred and forty-seven\n",
      "Number: 748, Predicted: seven hundred and forty-eight, Actual: seven hundred and forty-eight\n",
      "Number: 763, Predicted: seven hundred and sixty-three, Actual: seven hundred and sixty-three\n",
      "Number: 766, Predicted: seven hundred and sixty-six, Actual: seven hundred and sixty-six\n",
      "Number: 768, Predicted: seven hundred and sixty-eight, Actual: seven hundred and sixty-eight\n",
      "Number: 769, Predicted: seven hundred and sixty-nine, Actual: seven hundred and sixty-nine\n",
      "Number: 771, Predicted: seven hundred and seventy-one, Actual: seven hundred and seventy-one\n",
      "Number: 772, Predicted: seven hundred and seventy-two, Actual: seven hundred and seventy-two\n",
      "Number: 775, Predicted: seven hundred and seventy-five, Actual: seven hundred and seventy-five\n",
      "Number: 776, Predicted: seven hundred and seventy-six, Actual: seven hundred and seventy-six\n",
      "Number: 779, Predicted: seven hundred and seventy-nine, Actual: seven hundred and seventy-nine\n",
      "Number: 782, Predicted: seven hundred and eighty-two, Actual: seven hundred and eighty-two\n",
      "Number: 791, Predicted: seven hundred and ninety-one, Actual: seven hundred and ninety-one\n",
      "Number: 794, Predicted: seven hundred and ninety-four, Actual: seven hundred and ninety-four\n",
      "Number: 795, Predicted: seven hundred and ninety-five, Actual: seven hundred and ninety-five\n",
      "Number: 804, Predicted: eight hundred and four, Actual: eight hundred and four\n",
      "Number: 805, Predicted: eight hundred and five, Actual: eight hundred and five\n",
      "Number: 812, Predicted: eight hundred and twelve, Actual: eight hundred and twelve\n",
      "Number: 815, Predicted: eight hundred and fifteen, Actual: eight hundred and fifteen\n",
      "Number: 818, Predicted: eight hundred and eighteen, Actual: eight hundred and eighteen\n",
      "Number: 821, Predicted: eight hundred and twenty-one, Actual: eight hundred and twenty-one\n",
      "Number: 831, Predicted: eight hundred and thirty-one, Actual: eight hundred and thirty-one\n",
      "Number: 838, Predicted: eight hundred and thirty-eight, Actual: eight hundred and thirty-eight\n",
      "Number: 839, Predicted: eight hundred and thirty-nine, Actual: eight hundred and thirty-nine\n",
      "Number: 840, Predicted: eight hundred and forty, Actual: eight hundred and forty\n",
      "Number: 847, Predicted: eight hundred and forty-seven, Actual: eight hundred and forty-seven\n",
      "Number: 848, Predicted: eight hundred and forty-eight, Actual: eight hundred and forty-eight\n",
      "Number: 856, Predicted: eight hundred and fifty-six, Actual: eight hundred and fifty-six\n",
      "Number: 857, Predicted: eight hundred and fifty-seven, Actual: eight hundred and fifty-seven\n",
      "Number: 860, Predicted: eight hundred and sixty, Actual: eight hundred and sixty\n",
      "Number: 862, Predicted: eight hundred and sixty-two, Actual: eight hundred and sixty-two\n",
      "Number: 871, Predicted: eight hundred and seventy-one, Actual: eight hundred and seventy-one\n",
      "Number: 875, Predicted: eight hundred and seventy-five, Actual: eight hundred and seventy-five\n",
      "Number: 878, Predicted: eight hundred and seventy-eight, Actual: eight hundred and seventy-eight\n",
      "Number: 880, Predicted: eight hundred and eighty, Actual: eight hundred and eighty\n",
      "Number: 888, Predicted: eight hundred and eighty-eight, Actual: eight hundred and eighty-eight\n",
      "Number: 890, Predicted: eight hundred and ninety, Actual: eight hundred and ninety\n",
      "Number: 897, Predicted: eight hundred and ninety-seven, Actual: eight hundred and ninety-seven\n",
      "Number: 929, Predicted: nine hundred and twenty-nine, Actual: nine hundred and twenty-nine\n",
      "Number: 937, Predicted: nine hundred and thirty-seven, Actual: nine hundred and thirty-seven\n",
      "Number: 944, Predicted: nine hundred and forty-four, Actual: nine hundred and forty-four\n",
      "Number: 955, Predicted: nine hundred and fifty-five, Actual: nine hundred and fifty-five\n",
      "Number: 957, Predicted: nine hundred and fifty-seven, Actual: nine hundred and fifty-seven\n",
      "Number: 962, Predicted: nine hundred and sixty-two, Actual: nine hundred and sixty-two\n",
      "Number: 976, Predicted: nine hundred and seventy-six, Actual: nine hundred and seventy-six\n",
      "Number: 980, Predicted: nine hundred and eighty, Actual: nine hundred and eighty\n",
      "Number: 982, Predicted: nine hundred and eighty-two, Actual: nine hundred and eighty-two\n",
      "Number: 992, Predicted: nine hundred and ninety-two, Actual: nine hundred and ninety-two\n",
      "Number: 996, Predicted: nine hundred and ninety-six, Actual: nine hundred and ninety-six\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.965"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(custom_model, val_df, tokenize_numeric_representation, sp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
